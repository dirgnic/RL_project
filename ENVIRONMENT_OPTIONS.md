# Gymnasium Environments - Complete Guide & Recommendations

## ğŸ“Š Environment Categories Overview

### 1. **Classic Control** (Simple, Good for Learning)
### 2. **Box2D** (Physics-based, Medium Complexity)
### 3. **Toy Text** (Discrete, Tabular RL)
### 4. **MuJoCo** (Advanced Robotics - Requires License)
### 5. **Atari** (Deep RL, Image-based)

---

## ğŸ¯ RECOMMENDED FOR YOUR PROJECT

### â­â­â­ **TOP 3 CHOICES** (Best for Academic Project)

#### 1. **CartPole-v1** âœ… CURRENTLY IMPLEMENTED
- **Type**: Classic Control
- **State Space**: Continuous (4D)
  - Cart position: [-4.8, 4.8]
  - Cart velocity: [-âˆ, âˆ]
  - Pole angle: [-24Â°, 24Â°]
  - Pole angular velocity: [-âˆ, âˆ]
- **Action Space**: Discrete (2) - Push left or right
- **Episode Length**: Max 500 steps
- **Success Criteria**: Balance pole for 500 steps (reward â‰¥ 475)
- **Difficulty**: â­ Easy
- **Installation**: `pip install gymnasium` âœ… Already working!

**Why Recommend:**
- âœ… Perfect for Q-Learning with discretization
- âœ… Fast training (converges in 500-1000 episodes)
- âœ… Easy to understand and visualize
- âœ… Good for demonstrating reward shaping
- âœ… No special dependencies

**Reward Shaping Ideas:**
- âœ… Already implemented: `bonus_center`, `penalize_oscillations`, `progressive_penalty`
- Bonus for keeping pole vertical
- Penalty for cart moving to edges
- Bonus for low velocity

---

#### 2. **MountainCar-v0** âœ… CURRENTLY IMPLEMENTED
- **Type**: Classic Control
- **State Space**: Continuous (2D)
  - Position: [-1.2, 0.6]
  - Velocity: [-0.07, 0.07]
- **Action Space**: Discrete (3) - Push left, no push, push right
- **Episode Length**: Max 200 steps
- **Success Criteria**: Reach flag at position â‰¥ 0.5
- **Difficulty**: â­â­ Medium (sparse rewards!)
- **Installation**: `pip install gymnasium` âœ… Already working!

**Why Recommend:**
- âœ… Classic RL benchmark problem
- âœ… Demonstrates importance of reward shaping (default reward is -1 per step)
- âœ… Shows exploration challenges
- âœ… Great for comparing algorithms
- âœ… Small state space (easy discretization)

**Reward Shaping Ideas:**
- âœ… Already implemented: `height_based`, `momentum_based`, `potential_based`
- Reward for gaining height
- Reward for building momentum
- Potential-based shaping (most effective!)

---

#### 3. **LunarLander-v2** âš ï¸ REQUIRES BOX2D
- **Type**: Box2D (Physics simulation)
- **State Space**: Continuous (8D)
  - Position (x, y)
  - Velocity (x, y)
  - Angle, angular velocity
  - Left leg contact, right leg contact
- **Action Space**: Discrete (4) - Do nothing, fire left, fire main, fire right
- **Episode Length**: Max 1000 steps
- **Success Criteria**: Land safely (reward â‰¥ 200)
- **Difficulty**: â­â­â­ Hard
- **Installation**: 
  ```bash
  brew install swig  # macOS
  pip install 'gymnasium[box2d]'
  ```

**Why Recommend:**
- âœ… Visually impressive (good for presentations!)
- âœ… Real-world physics
- âœ… Good for DQN and PPO
- âœ… Well-documented benchmark
- âš ï¸ Harder to train with Q-Learning
- âš ï¸ Requires Box2D installation

**Reward Shaping Ideas:**
- âœ… Already implemented: `fuel_efficiency`, `safety_first`
- Penalty for fuel consumption
- Bonus for soft landings
- Penalty for crashing
- Reward for staying centered

---

## ğŸ® ALL CLASSIC CONTROL ENVIRONMENTS

### **Acrobot-v1**
- **State**: 6D (sin/cos of 2 joint angles + velocities)
- **Actions**: 3 (apply +1, 0, -1 torque)
- **Goal**: Swing tip above a line
- **Difficulty**: â­â­ Medium
- **Good for**: Continuous state Q-Learning, DQN
- **Reward Shaping**: Height-based rewards, energy penalties

### **CartPole-v1** âœ… IMPLEMENTED
(See above)

### **MountainCar-v0** âœ… IMPLEMENTED
(See above)

### **MountainCarContinuous-v0**
- **State**: 2D (position, velocity)
- **Actions**: Continuous [-1, 1] (force)
- **Goal**: Reach flag
- **Difficulty**: â­â­â­ Hard
- **Good for**: PPO, DDPG, SAC
- **Recommendation**: Use discrete version (easier)

### **Pendulum-v1**
- **State**: 3D (cos(Î¸), sin(Î¸), angular velocity)
- **Actions**: Continuous [-2, 2] (torque)
- **Goal**: Keep pendulum upright
- **Difficulty**: â­â­ Medium
- **Good for**: PPO, DDPG (continuous control)
- **Reward Shaping**: Angle and velocity penalties

---

## ğŸ“¦ BOX2D ENVIRONMENTS (Requires Box2D)

### **LunarLander-v2** âœ… CONFIGS READY
(See above - Installation required)

### **LunarLanderContinuous-v2**
- Same as LunarLander but continuous actions
- **Difficulty**: â­â­â­â­ Very Hard
- **Good for**: PPO, DDPG, TD3
- **Recommendation**: Use discrete version

### **BipedalWalker-v3**
- **State**: 24D (hull angle, velocities, joints, ground contact, etc.)
- **Actions**: 4 continuous (hip/knee motors)
- **Goal**: Walk forward without falling
- **Difficulty**: â­â­â­â­â­ Very Hard
- **Good for**: Advanced PPO, SAC
- **Recommendation**: Too complex for your project

### **BipedalWalkerHardcore-v3**
- Even harder version with obstacles
- **Recommendation**: Skip

### **CarRacing-v2**
- **State**: 96Ã—96 RGB image
- **Actions**: 3 continuous (steering, gas, brake)
- **Difficulty**: â­â­â­â­â­ Very Hard
- **Good for**: Deep RL with CNNs
- **Recommendation**: Too complex

---

## ğŸ“ TOY TEXT ENVIRONMENTS (Perfect for Tabular Q-Learning!)

### **FrozenLake-v1** â­ EXCELLENT FOR Q-LEARNING
- **State**: 16 discrete positions (4Ã—4 grid)
- **Actions**: 4 (up, down, left, right)
- **Goal**: Reach goal without falling in holes
- **Difficulty**: â­ Easy (but slippery!)
- **Installation**: Built-in
- **Why Good**: Perfect for pure Q-Learning (no discretization needed!)

```python
import gymnasium as gym
env = gym.make('FrozenLake-v1', is_slippery=True)
```

**Variants:**
- `FrozenLake-v1` (4Ã—4)
- `FrozenLake8x8-v1` (8Ã—8, harder)

### **Taxi-v3** â­ GREAT FOR Q-LEARNING
- **State**: 500 discrete states (taxi location, passenger location, destination)
- **Actions**: 6 (move 4 directions, pickup, dropoff)
- **Goal**: Pick up passenger and drop at destination
- **Difficulty**: â­â­ Medium
- **Why Good**: Perfect discrete environment, more complex than FrozenLake

### **CliffWalking-v0** â­ EXCELLENT FOR SARSA VS Q-LEARNING
- **State**: 48 discrete positions (4Ã—12 grid)
- **Actions**: 4 (up, down, left, right)
- **Goal**: Navigate from start to goal avoiding cliff
- **Difficulty**: â­â­ Medium
- **Why Good**: Classic for comparing SARSA and Q-Learning!

### **Blackjack-v1**
- **State**: 704 discrete states (player sum, dealer card, usable ace)
- **Actions**: 2 (hit, stand)
- **Difficulty**: â­â­ Medium
- **Why Good**: Card game, different domain

---

## ğŸ® ATARI ENVIRONMENTS (Requires ROM files)

**Installation**: `pip install 'gymnasium[atari]'` + `pip install 'gymnasium[accept-rom-license]'`

Popular Games:
- **Pong-v5**: Simple, good for DQN
- **Breakout-v5**: Classic benchmark
- **SpaceInvaders-v5**: More complex
- **MsPacman-v5**: Very complex

**Difficulty**: â­â­â­â­ Very Hard  
**Good for**: Deep Q-Networks (DQN), CNN-based RL  
**Recommendation**: Too complex for your project (requires image processing)

---

## ğŸ¤– MUJOCO ENVIRONMENTS (Requires License)

**Installation**: Requires MuJoCo license (free for students)

Popular Environments:
- Ant-v4, HalfCheetah-v4, Hopper-v4, Humanoid-v4, Walker2d-v4

**Difficulty**: â­â­â­â­â­ Very Hard  
**Good for**: Advanced continuous control research  
**Recommendation**: Skip (license required, very complex)

---

## ğŸ¯ MY RECOMMENDATIONS FOR YOUR PROJECT

### **Option A: Simple & Fast** â­â­â­â­â­ BEST CHOICE
**Environments**: CartPole-v1 + MountainCar-v0  
**Why**: 
- âœ… Already implemented and working!
- âœ… Fast training (results in minutes)
- âœ… Clear demonstration of reward shaping
- âœ… Works perfectly with Q-Learning, DQN, PPO
- âœ… No installation issues
- âœ… Great for presentations

**Algorithms**:
1. Q-Learning (Ingrid) âœ…
2. DQN (Matei)
3. PPO (Iustin)

### **Option B: Add Visual Impact** â­â­â­â­
**Environments**: CartPole-v1 + MountainCar-v0 + LunarLander-v2  
**Why**:
- âœ… LunarLander looks impressive in presentations
- âœ… More complex physics
- âš ï¸ Requires Box2D installation
- âš ï¸ Slower training
- âš ï¸ Q-Learning might struggle (need more episodes)

### **Option C: Pure Tabular RL** â­â­â­â­
**Environments**: FrozenLake-v1 + Taxi-v3 + CliffWalking-v0  
**Why**:
- âœ… Perfect for pure Q-Learning (no discretization!)
- âœ… Fast training
- âœ… Clear state-action tables
- âœ… CliffWalking great for SARSA comparison
- âš ï¸ Less visually impressive
- âš ï¸ DQN might be overkill

### **Option D: Mixed Approach** â­â­â­
**Environments**: CartPole-v1 (continuous) + FrozenLake-v1 (discrete)  
**Why**:
- âœ… Shows both discretization and pure tabular
- âœ… Fast training on both
- âœ… Different types of problems

---

## ğŸ“Š COMPARISON TABLE

| Environment | State Space | Action Space | Difficulty | Training Time | Visual Appeal | Box2D? | Recommend |
|------------|-------------|--------------|------------|---------------|---------------|---------|-----------|
| **CartPole-v1** | 4D Continuous | 2 Discrete | â­ | ~5 min | â­â­â­ | No | â­â­â­â­â­ |
| **MountainCar-v0** | 2D Continuous | 3 Discrete | â­â­ | ~10 min | â­â­ | No | â­â­â­â­â­ |
| **LunarLander-v2** | 8D Continuous | 4 Discrete | â­â­â­ | ~30 min | â­â­â­â­â­ | Yes | â­â­â­â­ |
| **FrozenLake-v1** | 16 Discrete | 4 Discrete | â­ | ~2 min | â­ | No | â­â­â­â­ |
| **Taxi-v3** | 500 Discrete | 6 Discrete | â­â­ | ~5 min | â­â­ | No | â­â­â­ |
| **Acrobot-v1** | 6D Continuous | 3 Discrete | â­â­ | ~15 min | â­â­â­ | No | â­â­â­ |
| **Pendulum-v1** | 3D Continuous | 1 Continuous | â­â­ | ~10 min | â­â­â­ | No | â­â­ |
| **Atari Games** | 210Ã—160 Image | Varies | â­â­â­â­ | Hours | â­â­â­â­â­ | No | â­ |

---

## ğŸ’¡ FINAL RECOMMENDATION FOR YOUR PROJECT

### âœ… **STICK WITH CURRENT CHOICE**: CartPole-v1 + MountainCar-v0

**Reasons**:
1. âœ… **Already implemented** - you have working code!
2. âœ… **All tests passing** - proven to work
3. âœ… **Fast training** - quick iteration and experiments
4. âœ… **Great for all 3 algorithms**:
   - Q-Learning: Works perfectly with discretization âœ…
   - DQN: Good continuous state approximation
   - PPO: Good for policy gradients
5. âœ… **Clear reward shaping demonstration**
6. âœ… **No dependency issues** (no Box2D needed)
7. âœ… **Professor will approve** - classic benchmarks

### Optional: Add ONE More Environment

**If you want 3 environments**, best choices:

1. **FrozenLake-v1** (Easiest to add)
   - No new dependencies
   - Perfect for pure Q-Learning demo
   - 5 minutes to implement
   - Shows tabular vs discretization comparison

2. **LunarLander-v2** (Most impressive)
   - Requires Box2D installation
   - Looks great in presentations
   - More challenging but doable
   - You already have configs ready!

3. **Acrobot-v1** (Good middle ground)
   - No new dependencies
   - Different mechanics than CartPole
   - Medium complexity

---

## ğŸš€ CODE EXAMPLES

### Quick Test Any Environment

```python
import gymnasium as gym

# Test any environment
env_name = "FrozenLake-v1"  # Change this
env = gym.make(env_name)

print(f"Environment: {env_name}")
print(f"Observation space: {env.observation_space}")
print(f"Action space: {env.action_space}")

# Random agent test
state, info = env.reset(seed=42)
for _ in range(100):
    action = env.action_space.sample()
    state, reward, terminated, truncated, info = env.step(action)
    if terminated or truncated:
        break

env.close()
```

### Add FrozenLake to Your Project (5 minutes)

```python
# In env/reward_wrapper.py, add to CustomRewardWrapper:

def _frozenlake_reward(self, state, reward, terminated, truncated, info):
    """Reward shaping for FrozenLake."""
    if self.reward_type == "default":
        return reward
    
    elif self.reward_type == "distance_based":
        # Reward based on distance to goal
        goal_pos = 15  # Bottom-right corner in 4x4
        row, col = state // 4, state % 4
        goal_row, goal_col = goal_pos // 4, goal_pos % 4
        distance = abs(row - goal_row) + abs(col - goal_col)
        distance_reward = -0.01 * distance
        
        if terminated and reward > 0:  # Reached goal
            return reward + 1.0
        elif terminated:  # Fell in hole
            return reward - 1.0
        return distance_reward
    
    return reward
```

---

## ğŸ“ MEETING #1 TALKING POINTS

### What to Present:

1. **Current Environments**: CartPole-v1 + MountainCar-v0
   - Working implementations âœ…
   - Multiple reward types âœ…
   - Fast training âœ…

2. **Proposal**: Keep these two OR add one more
   - **Option A**: Add FrozenLake (easiest)
   - **Option B**: Add LunarLander (most impressive)
   - **Option C**: Stay with current two (safest)

3. **Ask Professor**:
   - Is 2 environments sufficient?
   - Preference: discrete (FrozenLake) vs physics (LunarLander)?
   - Any specific benchmarks required?

---

## ğŸ“š Resources

- **Gymnasium Docs**: https://gymnasium.farama.org/
- **Environment List**: https://gymnasium.farama.org/environments/
- **Your Lab Materials**: `Laboratoare/lab2_envs_mdp/`

**Good luck with your project! ğŸš€**
