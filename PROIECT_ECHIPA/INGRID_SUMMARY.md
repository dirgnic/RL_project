# ðŸš€ PROJECT REFOCUSED: TAXI-V3 + DQN

**Date:** November 30, 2025  
**Status:** âœ… Successfully pushed to GitHub!

---

## âœ… WHAT WE JUST DID

### 1. **Fixed Git Issues** 
- âŒ Old problem: `.venv` stuck in git history (200+ MB)
- âœ… Solution: Created fresh branch, removed all history
- âœ… Result: Clean 26.74 MiB repository on GitHub

### 2. **Refocused Project**
- âŒ Old: CartPole + MountainCar with Q-Learning only
- âœ… New: **Taxi-v3 (Extended)** with 3 algorithms:
  - **Person B (Irina):** Q-Learning & SARSA (tabular)
  - **Person C (Ingrid):** DQN (deep value-based) â† **YOU!**
  - **Person D (Matei):** REINFORCE (policy-based)
  - **Person A (Iustin):** Environment designer (extensions)

### 3. **Created DQN Implementation** 
- âœ… `agents/dqn/network.py` - Neural network (basic + dueling)
- âœ… `agents/dqn/replay_buffer.py` - Experience replay (basic + prioritized)
- âœ… `agents/dqn/agent.py` - Full DQN agent with target network
- âœ… `agents/dqn/train.py` - Training & evaluation script
- âœ… `PROIECT_ECHIPA/INGRID_DQN_TASKS.md` - Your detailed task list

---

## ðŸŽ¯ YOUR ROLE: Person C - Deep Value-Based Engineer

### What You Need to Do:

#### **Week 1 (Dec 2-5): Learn & Basic DQN**
1. Study Lab 5 & Lab 6 thoroughly
2. Understand Taxi-v3 environment
3. Run `python agents/dqn/train.py` to test basic DQN
4. Make sure it trains (reward should improve)

#### **Week 2 (Dec 9-12): Extended Taxi & Tuning**
1. Get Iustin's extended Taxi wrapper
2. Adapt DQN to handle new features
3. Tune hyperparameters:
   - Learning rate: [1e-3, 5e-4, 1e-4]
   - Batch size: [32, 64, 128]
   - Network size: [128-64], [256-128], [512-256-128]
4. Compare with Irina's Q-learning

#### **Week 3 (Dec 16-19): Experiments**
1. Run experiments with multiple seeds
2. Generate comparison plots
3. Ablation studies (without replay, without target network)
4. Start writing report sections

#### **Week 4 (Dec 23-26): Documentation**
1. Write your report sections (NO LLM!)
2. Create slides for presentation
3. Practice explaining DQN
4. Final code cleanup

---

## ðŸ“Š KEY METRICS TO TRACK

When training DQN, monitor:
- **Episode Reward:** Should increase over time
- **Loss:** Should decrease and stabilize
- **Success Rate:** % of successful passenger deliveries
- **Convergence Speed:** Episodes until good performance

Compare with Irina's Q-learning:
- Training speed (episodes to convergence)
- Sample efficiency
- Final performance
- Generalization to extended Taxi

---

## ðŸ”§ HYPERPARAMETERS TO TUNE

Start with these defaults (already in code):
```python
learning_rate = 1e-3
gamma = 0.99
epsilon_start = 1.0
epsilon_end = 0.01
epsilon_decay = 0.995
buffer_capacity = 10000
batch_size = 64
target_update_freq = 100
```

If training fails, try:
- **Loss explodes:** Reduce learning_rate to 5e-4 or 1e-4
- **Slow convergence:** Increase batch_size to 128
- **Unstable:** Update target network more often (freq=50)
- **Poor performance:** Increase network size or training episodes

---

## ðŸ“ FILES YOU'LL WORK WITH

### Your Code:
- `agents/dqn/network.py` - Modify network architecture
- `agents/dqn/agent.py` - Tune agent hyperparameters
- `agents/dqn/train.py` - Run experiments

### Reference:
- `PROIECT_ECHIPA/INGRID_DQN_TASKS.md` - Your detailed checklist
- `PROIECT_ECHIPA/01_PLAN_PROIECT.md` - Team plan
- Lab 5 notebooks - Q-learning to DQN
- Lab 6 notebooks - DQN improvements

---

## ðŸ¤ TEAM COORDINATION

### With Iustin (Environment):
- Ask for extended Taxi wrapper by Dec 9
- Understand new state space dimensions
- Test DQN works on extended version

### With Irina (Tabular RL):
- Share results format (CSV or JSON)
- Use same seeds for fair comparison (42, 123, 456)
- Coordinate on plotting utilities

### With Matei (Policy):
- Share comparison plot code
- Discuss which extensions challenge each algorithm
- Help with final presentation slides

---

## ðŸŽ¯ SUCCESS CRITERIA

You're done when you can:
- âœ… Train DQN successfully on Taxi-v3
- âœ… Beat random policy (avg reward > 0)
- âœ… Compare DQN vs Q-learning with plots
- âœ… Explain every hyperparameter choice
- âœ… Write report sections in your own words
- âœ… Present and answer questions confidently

---

## ðŸ†˜ TROUBLESHOOTING

### "DQN doesn't learn, reward stays low"
- Check epsilon is decaying (print it each episode)
- Verify replay buffer is filling up
- Try smaller learning rate (1e-4)
- Increase training episodes (2000 â†’ 5000)

### "Loss explodes to infinity"
- Reduce learning rate to 1e-4
- Add gradient clipping (already in code)
- Check reward scaling (normalize rewards?)
- Try smaller network

### "DQN worse than Q-learning"
- This is okay! Document why
- Taxi-v3 has small state space (500 states)
- Tabular methods can be better on small problems
- DQN shines on larger/continuous spaces
- Focus on extended Taxi (larger state space)

### "Can't compare fairly with Q-learning"
- Use same seeds: 42, 123, 456
- Same number of episodes
- Same evaluation protocol (100 test episodes)
- Report mean Â± std over seeds

---

## ðŸ“š RESOURCES

### Must Read:
- Lab 5: `lab5_q-learning_dqn_starter.ipynb`
- Lab 6: `lab6_01_dueling_dqn.ipynb` (optional)
- Course 3: Deep RL slides

### Papers (Optional):
- DQN: Mnih et al. (2015) - Nature paper
- Dueling DQN: Wang et al. (2016)

### Documentation:
- PyTorch docs: https://pytorch.org/docs/
- Gymnasium Taxi-v3: https://gymnasium.farama.org/environments/toy_text/taxi/

---

## ðŸŽ‰ NEXT STEPS

1. **Today:** Read `INGRID_DQN_TASKS.md` thoroughly
2. **Tomorrow:** Study Lab 5 & Lab 6
3. **This Week:** Get basic DQN training
4. **Next Week:** Coordinate with team, get extended Taxi
5. **End of Month:** Have results & start report

---

## âœ… REPOSITORY STATUS

**GitHub:** https://github.com/dirgnic/RL_project  
**Branch:** main  
**Last Push:** e8cf2cf4 - "Add DQN implementation for Person C (Ingrid)"

**What's Ready:**
- âœ… Clean repository (no .venv!)
- âœ… DQN implementation structure
- âœ… Training script
- âœ… Task documentation

**What's Next:**
- Iustin implements extended Taxi
- Irina implements Q-learning
- You test and tune DQN
- Matei implements REINFORCE

---

## ðŸ’ª YOU GOT THIS!

Remember:
- Start simple, then improve
- Ask team for help when stuck
- Document everything you try
- Compare results scientifically
- Write report in your own words (no LLM!)

**Good luck with your DQN implementation! ðŸš€**
