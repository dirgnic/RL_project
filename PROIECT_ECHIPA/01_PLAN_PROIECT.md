#  PLAN PROIECT RL - Echipa Irina, Ingrid, Matei, Iustin

**Deadline prezentare:** SÄƒptÄƒmÃ¢na 14 (ianuarie 2026)  
**Data ultimului update:** 30 noiembrie 2025

---

## ğŸ¯ ENVIRONMENT ALES: **TAXI-V3 (Extended)**

### Task Split (Official):
- **Person A (Iustin):** Taxi world designer - Add extensions (extra passengers/fuel/obstacles)
- **Person B (Irina):** Tabular RL engineer - Q-learning & SARSA implementation
- **Person C (Ingrid):** Deep value-based engineer - **DQN implementation**
- **Person D (Matei):** Policy-based engineer - REINFORCE implementation

---

##  CERINÈšE PROIECT (din documentaÈ›ie)

### Ce trebuie sÄƒ livrÄƒm:
 **Environment ales/modificat**
- âœ… **TAXI-V3** with extensions (extra passengers, fuel, traffic penalties)
- Iustin implementeazÄƒ wrapper-ul custom peste Taxi-v3

 **Minim 3 algoritmi RL** din categorii diferite
- âœ… **Tabular:** Q-learning & SARSA (Irina)
- âœ… **Deep Value-Based:** DQN (Ingrid)
- âœ… **Policy-Based:** REINFORCE (Matei)

 **Experimente & tuning hiperparametri**
- Comparare: Original Taxi-v3 vs Extended Taxi
- Tuning pentru fiecare algoritm
- Ablation studies pentru extensii

 **Rezultate vizualizate**
- Training curves (reward, loss, convergenÈ›Äƒ)
- Comparison plots: Tabular vs DQN vs REINFORCE
- State space analysis

 **DocumentaÈ›ie**
- PowerPoint / PDF / LaTeX
- **FÄ‚RÄ‚ LLM pentru text!** (proful verificÄƒ)
- StructurÄƒ: Intro â†’ Taxi Extensions â†’ Algoritmi â†’ Experimente â†’ Concluzii

 **Prezentare Ã®n echipÄƒ** (sÄƒptÄƒmÃ¢na 14)
- Fiecare prezintÄƒ partea sa
- ExplicÄƒm de ce Taxi-v3 + ce extensii am adÄƒugat

---

##  ETAPE PROIECT

###  ETAPA 1: Teorie + Alegere Environment (29 nov - 5 dec)
**Status:**  Ãn progres

#### Taskuri individuale:
- [x] **ToÈ›i:** CitiÈ›i primele 3 cursuri RL  (Ingrid a fÄƒcut rezumat)
- [ ] **ToÈ›i:** CitiÈ›i primele pagini din pdf-ul profului
- [ ] **Fiecare:** Research 1-2 environment-uri Gymnasium + idei reward shaping

#### Taskuri echipÄƒ:
- [ ] **Meeting #1** (pÃ¢nÄƒ pe 5 dec):
  - Fiecare prezintÄƒ 1-2 env propuse
  - Alegem 1-2 finaliste
  - Stabilim Ã®ntrebÄƒri pentru prof
- [ ] **Irina:** Scrie profului pe Teams cu propunerea

**Deliverables:**
- Document "02_ENVIRONMENT_RESEARCH.md" (completat de toÈ›i)
- 1-2 environment-uri finaliste
- Lista de Ã®ntrebÄƒri pentru prof

---

###  ETAPA 2: Setup Proiect (5 dec - 12 dec)
**Status:**  Nu a Ã®nceput

**Depinde de:** Confirmarea profului pentru environment

#### Taskuri:
- [ ] **ToÈ›i:** CreÄƒm repo GitHub
- [ ] **Matei + Iustin:** Setup Python environment
  - Python 3.10+
  - Gymnasium
  - PyTorch/TensorFlow
  - Stable Baselines3 (optional)
  - Matplotlib, wandb (logging)
  
- [ ] **StructurÄƒ proiect:**
  ```
  RL_PROJECT/
   env/                 # Environment + reward modificat
   agents/              # Script-uri algoritmi
      q_learning/
      dqn/
      ppo/
   experiments/         # Configuri + seed-uri
   results/             # Grafice + logs
   docs/                # DocumentaÈ›ie
   notebooks/           # Jupyter pentru explorare
  ```

- [ ] **Implementare environment:**
  - [ ] Load environment default
  - [ ] TestÄƒm cÄƒ merge (random agent)
  - [ ] ImplementÄƒm wrapper pentru reward modificat
  - [ ] DocumentÄƒm reward original vs modificat

**Deliverables:**
- Repo GitHub funcÈ›ional
- Environment ruleazÄƒ + wrapper reward

---

###  ETAPA 3: Implementare AgenÈ›i (12 dec - 30 dec)
**Status:**  Nu a Ã®nceput

#### ÃmpÄƒrÈ›ire roluri (sugestii):

#####  **Ingrid: Agent #1 - Q-Learning (Tabular)**
**De ce:** Simplu, bun pentru Ã®nvÄƒÈ›are, Ingrid Ã®È™i doreÈ™te sÄƒ stÄƒpÃ¢neascÄƒ algoritmii clasici

**Taskuri:**
- [ ] StudiazÄƒ Q-Learning (lab 4)
- [ ] ImplementeazÄƒ Q-table
- [ ] Îµ-greedy exploration
- [ ] Update rule: `Q(s,a) â† Q(s,a) + Î±[r + Î³*max Q(s',a') - Q(s,a)]`
- [ ] RuleazÄƒ pe reward default
- [ ] RuleazÄƒ pe reward modificat
- [ ] Logging: reward/episod, Q-values
- [ ] Mini-raport: ce parametri, ce rezultate

**Output:**
- Script `agents/q_learning/train.py`
- Config `experiments/q_learning_config.json`
- Grafice Ã®n `results/q_learning/`

---

#####  **Matei: Agent #2 - DQN (Deep Q-Network)**
**De ce:** Deep RL, reÈ›ele neuronale, scalabil

**Taskuri:**
- [ ] StudiazÄƒ DQN (lab 5)
- [ ] ImplementeazÄƒ neural network (PyTorch)
- [ ] Replay Buffer
- [ ] Target Network (actualizare periodicÄƒ)
- [ ] Loss function: MSE Ã®ntre Q-predicted È™i Q-target
- [ ] RuleazÄƒ pe reward default
- [ ] RuleazÄƒ pe reward modificat
- [ ] Logging: reward, loss, epsilon decay
- [ ] Mini-raport: arhitecturÄƒ reÈ›ea, hiperparametri

**Output:**
- Script `agents/dqn/train.py`
- Model salvat `agents/dqn/model.pth`
- Config `experiments/dqn_config.json`
- Grafice Ã®n `results/dqn/`

---

#####  **Iustin: Agent #3 - PPO (Policy-Based)**
**De ce:** State-of-the-art, policy gradient, stabilitate

**Taskuri:**
- [ ] StudiazÄƒ PPO (lab 7 + Stable Baselines3 docs)
- [ ] OpÈ›iune A: Implementare de la zero (advanced)
- [ ] OpÈ›iune B: FoloseÈ™te Stable Baselines3 (recomandat)
- [ ] Config: learning rate, clip ratio, GAE lambda
- [ ] RuleazÄƒ pe reward default
- [ ] RuleazÄƒ pe reward modificat
- [ ] Logging: reward, policy loss, value loss
- [ ] Mini-raport: arhitecturÄƒ, clip ratio, rezultate

**Output:**
- Script `agents/ppo/train.py` (sau wrapper SB3)
- Config `experiments/ppo_config.json`
- Grafice Ã®n `results/ppo/`

---

#####  **Irina: Coordonare + Environment + Setup Experimente**
**De ce:** Rol de coordonare, design environment, setup infrastructurÄƒ

**Taskuri:**
- [ ] Design reward shaping (2-3 variante)
- [ ] Wrapper environment custom
- [ ] Setup logging comun (wandb sau matplotlib)
- [ ] Template config pentru experimente
- [ ] Documentare reward original vs modificat
- [ ] VerificÄƒ cÄƒ toÈ›i respectÄƒ structura proiectului
- [ ] Comunicare cu proful (Ã®ntrebÄƒri, clarificÄƒri)
- [ ] Review code (ajutÄƒ la debug)

**Output:**
- `env/custom_env.py` (wrapper)
- `env/reward_analysis.md`
- `experiments/template_config.json`
- DocumentaÈ›ie Ã®n `docs/environment.md`

---

###  ETAPA 4: Experimente & Tuning (30 dec - 8 ian)
**Status:**  Nu a Ã®nceput

#### Scenarii de experimente:

**Experiment 1: Baseline (reward default)**
- RulÄƒm Q-Learning, DQN, PPO
- Seed fix: 42
- Hiperparametri default
- Logging: reward/episod, timp de training

**Experiment 2: Reward modificat**
- AceiaÈ™i algoritmi
- Reward shaping aplicat
- ComparaÈ›ie cu baseline

**Experiment 3: Tuning hiperparametri**
- VariaÈ›ii learning rate: [0.001, 0.0001]
- VariaÈ›ii epsilon decay
- VariaÈ›ii arhitecturÄƒ reÈ›ea (DQN)
- Grid search sau random search

**Experiment 4: Seed-uri multiple**
- 3-5 seed-uri diferite pentru fiecare algoritm
- CalculÄƒm medie + deviere standard
- VerificÄƒm stabilitatea

#### Metrici de comparat:
- Reward mediu per episod
- Timp de convergenÈ›Äƒ (cÃ¢te episoade pÃ¢nÄƒ la threshold)
- Stabilitate (varianÈ›a reward-ului)
- Sample efficiency (cÃ¢te samples pÃ¢nÄƒ la rezultat ok)

**Deliverables:**
- Toate rezultatele Ã®n `results/`
- Tabele comparative Ã®n Excel/CSV
- Grafice pentru fiecare experiment

---

###  ETAPA 5: Documentare & Prezentare (8 ian - 15 ian)
**Status:**  Nu a Ã®nceput

#### StructurÄƒ documentaÈ›ie:

**1. Introducere & MotivaÈ›ie** (1-2 slide-uri)
- De ce am ales tema asta?
- Ce probleme rezolvÄƒ?
- Context real (opÈ›ional)

**2. Environment** (2-3 slide-uri)
- Descriere mediu (stÄƒri, acÈ›iuni, terminare)
- Reward original
- Reward modificat (motivaÈ›ie, formulÄƒ)
- VisualizÄƒri (screenshot, schema)

**3. Algoritmi** (3-4 slide-uri, 1 per algoritm)
- ExplicaÈ›ie intuitivÄƒ (nu hardcore math)
- Pseudocod / schema
- Hiperparametri folosiÈ›i
- ArhitecturÄƒ (pentru DQN/PPO)

**4. Experimente** (3-4 slide-uri)
- Setup experimente (seed-uri, config)
- Rezultate: GRAFICE MARI È˜I CLARE
- Tabele comparative
- AnalizÄƒ: ce a mers, ce nu, de ce

**5. Probleme & SoluÈ›ii** (1-2 slide-uri)
- Ce a fost greu?
- Cum am rezolvat?
- InstabilitÄƒÈ›i, bug-uri interesante

**6. Concluzii & LecÈ›ii** (1 slide)
- Ce am Ã®nvÄƒÈ›at?
- Ce am face diferit?
- DirecÈ›ii viitoare

**7. Demo (opÈ›ional)** (1 slide + video scurt)
- Agent antrenat Ã®n acÈ›iune
- ComparaÈ›ie Ã®nainte/dupÄƒ

#### Taskuri documentare:
- [ ] **Irina:** Coordonare, introducere, environment
- [ ] **Ingrid:** SecÈ›iunea Q-Learning, explicaÈ›ii intuitive
- [ ] **Matei:** SecÈ›iunea DQN, arhitecturi
- [ ] **Iustin:** SecÈ›iunea PPO, experimente
- [ ] **ToÈ›i:** Review final, verificare text (fÄƒrÄƒ LLM!)

#### PregÄƒtire prezentare:
- [ ] Fiecare Ã®È™i prezintÄƒ partea (3-5 min/persoanÄƒ)
- [ ] Rehearsal complet (2-3 zile Ã®nainte)
- [ ] PregÄƒtim rÄƒspunsuri la Ã®ntrebÄƒri posibile
- [ ] VerificÄƒm cÄƒ toÈ›i putem explica codul

**Deliverables:**
- PowerPoint final
- README.md pentru GitHub
- Cod comentat È™i curat
- Video demo (optional, dar nice to have)

---

##  TRACKING PROGRES

### Week by week:

| SÄƒptÄƒmÃ¢nÄƒ | DatÄƒ | Milestone | Status |
|-----------|------|-----------|--------|
| S1 | 29 nov - 5 dec | Research env + meeting #1 |  Ãn progres |
| S2 | 5 dec - 12 dec | Confirmare env + setup proiect |  Planned |
| S3 | 12 dec - 19 dec | Implementare agent #1 + #2 |  Planned |
| S4 | 19 dec - 26 dec | Implementare agent #3 |  Planned |
| S5 | 26 dec - 2 ian | Finalizare agenÈ›i + teste |  Planned |
| S6 | 2 ian - 8 ian | Experimente & tuning |  Planned |
| S7 | 8 ian - 15 ian | Documentare + rehearsal |  Planned |
| S8 | 15 ian - 22 ian | PREZENTARE (sÄƒpt 14) |  Planned |

---

##  ÃNTREBÄ‚RI PENTRU PROF

**De Ã®ntrebat la urmÄƒtorul contact:**

1. Environment-ul nostru ales e ok? (dupÄƒ ce alegem)
2. Reward shaping Ã®nseamnÄƒ doar modificarea formulei de reward, sau È™i alte chestii?
3. E ok sÄƒ folosim Stable Baselines3 pentru unul din algoritmi, sau trebuie totul de la zero?
4. CÃ¢t de complexe trebuie sÄƒ fie experimentele? (cÃ¢te seed-uri, cÃ¢te combinaÈ›ii hiperparametri?)
5. Lungimea prezentÄƒrii? (15-20 min?)
6. Ce fel de Ã®ntrebÄƒri pune de obicei la prezentare?

---

##  NEXT STEPS IMEDIATE (TOATÄ‚ ECHIPA)

### PÃ¢nÄƒ pe 5 decembrie (Meeting #1):

1. **ToÈ›i citesc:**
   - [ ] `00_CHEAT_SHEET_RL.md` (fÄƒcut de Ingrid)
   - [ ] Primele 3 cursuri RL
   - [ ] `02_ENVIRONMENT_RESEARCH.md` (creÄƒm urmÄƒtorul pas)

2. **Fiecare completeazÄƒ Ã®n `02_ENVIRONMENT_RESEARCH.md`:**
   - 1-2 environment-uri propuse
   - Pro/contra pentru fiecare
   - Idei de reward shaping

3. **Ingrid (task special):**
   - AprofundeazÄƒ CartPole, MountainCar, LunarLander
   - PregÄƒteÈ™te comparaÈ›ie detaliatÄƒ
   - Idei concrete de reward modifications

4. **Irina:**
   - VerificÄƒ disponibilitate pentru meeting
   - PregÄƒteÈ™te Ã®ntrebÄƒrile pentru prof
   - VerificÄƒ cÄƒ toÈ›i au Ã®nÈ›eles cerinÈ›ele

5. **Meeting #1:**
   - PrezentÄƒm toÈ›i ce am gÄƒsit
   - Alegem 1-2 env finaliste
   - Stabilim urmÄƒtorii paÈ™i

---

##  RESURSE UTILE

- **Lab materials:** `/Laboratoare/` (avem deja!)
- **Cheat sheet:** `00_CHEAT_SHEET_RL.md`
- **Gymnasium docs:** https://gymnasium.farama.org/
- **Stable Baselines3:** https://stable-baselines3.readthedocs.io/
- **ChatGPT/Copilot:** OK pentru cod (dar sÄƒ Ã®nÈ›elegem!)

---

##  TIPS & TRICKS

### Pentru cod:
-  FolosiÈ›i seed-uri fixe (reproducibilitate)
-  Logging consistent (wandb sau csv)
-  ComentaÈ›i codul (ajutÄƒ la prezentare)
-  Git commits regulate (nu tot Ã®ntr-un commit)
-  TestaÈ›i pe reward default Ã®ntÃ¢i (baseline)

### Pentru documentaÈ›ie:
-  ScrieÈ›i Ã®n romÃ¢nÄƒ simplÄƒ (fÄƒrÄƒ LLM!)
-  Grafice MARI È™i clare
-  ExplicaÈ›ii intuitive (nu formule complicate)
-  Focus pe rezultate È™i insights
-  IncludeÈ›i "ce nu a mers" (e ok!)

### Pentru prezentare:
-  Fiecare È™tie ce a fÄƒcut
-  Putem explica codul dacÄƒ Ã®ntreabÄƒ
-  Rehearsal obligatoriu (2-3 zile Ã®nainte)
-  PregÄƒtim rÄƒspunsuri la Ã®ntrebÄƒri frecvente
-  Demo vizual (video/gif) e super util

---

**Last updated:** 29 noiembrie 2025  
**EchipÄƒ:** Irina , Ingrid ^^, Matei, Iustin  
**Vers: 1.0**

 **LET'S GO!!!** 
