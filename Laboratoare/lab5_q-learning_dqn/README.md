# ğŸ§  Lab 5 â€“ Deep Q-Learning vs Q-Learning

## ğŸ¯ Obiective
Ãn acest laborator vom explora diferenÈ›ele dintre algoritmul **Q-Learning** (clasic, tabular) È™i **Deep Q-Learning (DQN)** â€” o versiune care foloseÈ™te reÈ›ele neuronale pentru a aproxima funcÈ›ia de valoare \( Q(s, a) \).

La finalul laboratorului, studenÈ›ii vor putea:
- Ã®nÈ›eleagÄƒ ideea de **approximation function** Ã®n RL;
- implementeze un **agent DQN** folosind **PyTorch**;
- compare performanÈ›ele Ã®ntre Q-learning tabular È™i Deep Q-learning;
- Ã®nÈ›eleagÄƒ avantajele È™i limitÄƒrile celor douÄƒ abordÄƒri.

---

## âš™ï¸ 1ï¸âƒ£ Q-Learning vs. Deep Q-Learning

| Aspect | Q-Learning (Tabular) | Deep Q-Learning (DQN) |
|--------|----------------------|------------------------|
| **Reprezentarea funcÈ›iei Q** | Tabel (matrice) \( Q[s, a] \) | ReÈ›ea neuronalÄƒ care aproximeazÄƒ \( Q(s, a) \) |
| **SpaÈ›iul de stare** | Discret (necesitÄƒ discretizare manualÄƒ) | Poate gestiona stÄƒri continue |
| **Generalizare** | InexistentÄƒ (Ã®nvÄƒÈ›are separatÄƒ pentru fiecare stare) | GeneralizeazÄƒ Ã®ntre stÄƒri similare |
| **VitezÄƒ de antrenare** | Foarte rapid | Mai lent, necesitÄƒ backpropagation |
| **Scalabilitate** | LimitatÄƒ (exploadeazÄƒ dimensional) | Scalabil la probleme complexe |
| **Stabilitate** | Stabil (valoare exactÄƒ) | Poate fi instabil â†’ de aceea se folosesc *Replay Buffer* + *Target Network* |

> ğŸ’¡ *Intuitiv:* DQN Ã®nlocuieÈ™te tabelul Q cu un â€œaproximatorâ€ Ã®nvÄƒÈ›abil (neural network) care poate generaliza È™i Ã®nvÄƒÈ›a din experienÈ›Äƒ.

---

## ğŸ“˜ 2ï¸âƒ£ Parcursul laboratorului

1. **Recapitulare Q-Learning clasic:**
   - ReÃ®mprospÄƒtÄƒm principiul Q-learning: actualizarea \( Q(s,a) \) pe baza recompensei È™i a valorii maxime viitoare.
   - ImplementÄƒm un agent tabular pe un mediu discret (`CartPole-v1`, discretizat).

2. **Introducere Ã®n Deep Q-Learning:**
   - Ãnlocuim tabelul cu o reÈ›ea neuronalÄƒ simplÄƒ (PyTorch).
   - ImplementÄƒm experienÈ›a de Ã®nvÄƒÈ›are cu *Replay Buffer* È™i *Target Network*.

3. **ComparaÈ›ie:**
   - ObservÄƒm curbele de Ã®nvÄƒÈ›are pentru ambele abordÄƒri.
   - AnalizÄƒm stabilitatea, viteza È™i performanÈ›a.

4. **Experiment:**
   - RulÄƒm DQN pe `CartPole-v1`.
   - OpÈ›ional: rulaÈ›i pe `LunarLander-v2` pentru o problemÄƒ continuÄƒ, mai complexÄƒ.

---

## ğŸ§© 3ï¸âƒ£ Despre bibliotecile Torch È™i TensorFlow

### ğŸ”¹ **PyTorch**
- Framework dezvoltat de Facebook (Meta AI).
- Bazat pe conceptul de **computaÈ›ie dinamicÄƒ**: reÈ›eaua se construieÈ™te â€œdin mersâ€ Ã®n timpul execuÈ›iei (imperativ).
- UÈ™or de Ã®nÈ›eles, intuitiv, foarte popular Ã®n cercetare È™i prototipare.
- API familiar cu Python È™i suport excelent pentru GPU (`cuda`) È™i Apple MPS (Metal).

### ğŸ”¹ **TensorFlow**
- Framework dezvoltat de Google.
- IniÈ›ial bazat pe **computaÈ›ie staticÄƒ** (graf de execuÈ›ie declarat Ã®nainte de rulare), acum oferÄƒ È™i mod dinamic prin `tf.function`.
- Integrare puternicÄƒ cu ecosistemul Google (Colab, TPU, Keras).
- Folosit frecvent Ã®n producÈ›ie, dar mai puÈ›in intuitiv pentru experimente rapide.

| Aspect | PyTorch | TensorFlow |
|--------|----------|------------|
| SintaxÄƒ | Pythonic, intuitivÄƒ | BazatÄƒ pe grafuri È™i Keras |
| ExecuÈ›ie | DinamicÄƒ (define-by-run) | StaticÄƒ sau semi-dinamicÄƒ |
| Curba de Ã®nvÄƒÈ›are | SimplÄƒ | Mai abruptÄƒ |
| Utilizare tipicÄƒ | Cercetare, prototipare | ProducÈ›ie, deploy |
| Compatibilitate GPU | CUDA / MPS | CUDA / TPU |
| Ecosistem | TorchVision, TorchRL, Lightning | Keras, TF Agents, TFX |

> ğŸ’¬ Ãn laborator vom folosi **PyTorch**, deoarece oferÄƒ control complet asupra antrenÄƒrii È™i este mai uÈ™or de explicat pas cu pas.

## ğŸ’» 4ï¸âƒ£ Detectare automatÄƒ device

Ãn codul laboratorului se detecteazÄƒ automat resursa optimÄƒ disponibilÄƒ (CPU, GPU CUDA sau MPS pe Mac):

```python
if torch.backends.mps.is_available():
    device = torch.device("mps")
elif torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

print(f"Using device: {device}")

```

---

## ğŸ§  5ï¸âƒ£ Sfaturi pentru rulare eficientÄƒ

- FoloseÈ™te **CartPole-v1** pentru rezultate rapide (antrenare Ã®n 2â€“3 minute).  
- Nu depÄƒÈ™i **200â€“300 episoade** pentru test.  
- FoloseÈ™te **batch-uri mici (32â€“64)** pentru a evita blocarea procesorului.  
- DacÄƒ foloseÈ™ti un **Mac ARM**, `MPS` poate fi mai lent decÃ¢t `CPU` pentru modele mici â€” testeazÄƒ ambele opÈ›iuni.  
- ÃnlocuieÈ™te `plt.show()` cu `plt.savefig()` Ã®n notebook pentru a evita blocajele grafice.


## ğŸ“š 6ï¸âƒ£ Resurse utile

- *Mnih et al. (2015): [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)*  
  Articolul original DeepMind care introduce algoritmul **Deep Q-Network (DQN)**.

- [PyTorch Reinforcement Learning Documentation](https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)  
  DocumentaÈ›ia oficialÄƒ PyTorch â€“ include exemple de implementare a reÈ›elelor neuronale È™i suport GPU.

- [OpenAI Gymnasium Environments](https://gymnasium.farama.org/)  
  Biblioteca standard pentru medii de antrenament Ã®n RL (CartPole, LunarLander, etc.).

- [DeepMind x UCL RL Lectures](https://www.deepmind.com/learning-resources)  
  Seria de prelegeri video gratuite despre fundamentele Reinforcement Learning.

- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)  
  Framework modern pentru antrenarea rapidÄƒ a agenÈ›ilor DQN, PPO, A2C, etc.

> ğŸ’¡ Recomandare: Ã®ncepeÈ›i cu implementÄƒrile proprii din laborator, apoi comparaÈ›i performanÈ›a cu agenÈ›ii predefiniÈ›i din `stable-baselines3`.
