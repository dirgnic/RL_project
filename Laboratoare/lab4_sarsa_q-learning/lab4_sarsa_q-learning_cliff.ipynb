{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5051d482",
   "metadata": {},
   "source": [
    "### ⚖️ SARSA vs Q-Learning\n",
    "\n",
    "\n",
    "- **SARSA** (*on-policy*) – agentul învață pe baza acțiunilor reale efectuate, deci politica sa este mai precaută și tinde să evite riscurile.  \n",
    "- **Q-Learning** (*off-policy*) – agentul învață presupunând că va alege întotdeauna cea mai bună acțiune posibilă, ceea ce duce la o politică mai agresivă, dar potențial mai eficientă.\n",
    "\n",
    "Pe mediul **CliffWalking-v0**, diferența devine evidentă:  \n",
    "SARSA preferă trasee sigure departe de prăpastie,  \n",
    "în timp ce Q-Learning riscă mai mult pentru a ajunge mai repede la obiectiv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb02e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "alpha = 0.1      # learning rate\n",
    "gamma = 0.99     # discount factor\n",
    "epsilon = 1.0\n",
    "eps_min = 0.01\n",
    "eps_decay = 0.995\n",
    "episodes = 500\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "def epsilon_greedy(Q, s, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_actions)\n",
    "    return np.argmax(Q[s])\n",
    "\n",
    "def run_agent(algorithm=\"qlearning\"):\n",
    "    Q = np.zeros((n_states, n_actions))\n",
    "    rewards = []\n",
    "\n",
    "    global epsilon\n",
    "    epsilon = 1.0\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        s, _ = env.reset()\n",
    "        a = epsilon_greedy(Q, s, epsilon)\n",
    "        done = False\n",
    "        total = 0\n",
    "\n",
    "        while not done:\n",
    "            s2, r, done, truncated, _ = env.step(a)\n",
    "            a2 = epsilon_greedy(Q, s2, epsilon)\n",
    "            total += r\n",
    "\n",
    "            if algorithm == \"sarsa\":\n",
    "                Q[s, a] += alpha * (r + gamma * Q[s2, a2] * (not done) - Q[s, a])\n",
    "            elif algorithm == \"qlearning\":\n",
    "                Q[s, a] += alpha * (r + gamma * np.max(Q[s2]) * (not done) - Q[s, a])\n",
    "\n",
    "            s, a = s2, a2\n",
    "\n",
    "        epsilon = max(eps_min, epsilon * eps_decay)\n",
    "        rewards.append(total)\n",
    "\n",
    "    return Q, rewards\n",
    "\n",
    "Q_sarsa, r_sarsa = run_agent(\"sarsa\")\n",
    "Q_ql, r_ql = run_agent(\"qlearning\")\n",
    "\n",
    "\n",
    "window = 100\n",
    "plt.plot(np.convolve(r_sarsa, np.ones(window)/window, mode='valid'), label=\"SARSA\")\n",
    "plt.plot(np.convolve(r_ql, np.ones(window)/window, mode='valid'), label=\"Q-Learning\")\n",
    "plt.xlabel(\"Episod\")\n",
    "plt.ylabel(\"Reward mediu (100 episoade)\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparatie SARSA vs Q-Learning - CliffWalking\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5382881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(Q, episodes=1, render=True):\n",
    "    for ep in range(episodes):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        total = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            a = np.argmax(Q[s])\n",
    "            s2, r, done, truncated, _ = env.step(a)\n",
    "            total += r\n",
    "            steps += 1\n",
    "            s = s2\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "        print(f\"Episod {ep+1} -> Reward total: {total}, Pași: {steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703cd601",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Test SARSA ===\")\n",
    "test_agent(Q_sarsa, episodes=3, render=True)\n",
    "\n",
    "print(\"\\n=== Test Q-Learning ===\")\n",
    "test_agent(Q_ql, episodes=3, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c94156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory(Q):\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "    path = np.zeros(env.observation_space.n)\n",
    "    while not done:\n",
    "        path[s] += 1\n",
    "        a = np.argmax(Q[s])\n",
    "        s, r, done, truncated, _ = env.step(a)\n",
    "    return path.reshape((4, 12))  # dimensiunea grilei pentru CliffWalking\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.heatmap(trajectory(Q_sarsa), annot=False, cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Traiectorie SARSA\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.heatmap(trajectory(Q_ql), annot=False, cmap=\"Reds\", cbar=False)\n",
    "plt.title(\"Traiectorie Q-Learning\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
