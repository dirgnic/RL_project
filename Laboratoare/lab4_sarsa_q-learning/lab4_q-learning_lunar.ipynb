{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bad6d04",
   "metadata": {},
   "source": [
    "### ğŸŒ Environment: LunarLander-v2\n",
    "\n",
    "Mediul **`LunarLander-v2`** face parte din biblioteca `gymnasium` È™i simuleazÄƒ o navetÄƒ spaÈ›ialÄƒ care trebuie sÄƒ aterizeze controlat Ã®ntre douÄƒ steaguri.  \n",
    "Scopul agentului este sÄƒ Ã®nveÈ›e sÄƒ foloseascÄƒ motoarele pentru a-È™i stabiliza poziÈ›ia, unghiul È™i viteza, astfel Ã®ncÃ¢t sÄƒ aterizeze vertical, fÄƒrÄƒ impact violent.\n",
    "\n",
    "- **SpaÈ›iu de stare**: 8 dimensiuni (poziÈ›ie, vitezÄƒ, unghi, contact picioare)\n",
    "- **SpaÈ›iu de acÈ›iuni**: 4 acÈ›iuni discrete (`none`, `left engine`, `main engine`, `right engine`)\n",
    "- **RecompensÄƒ**:  \n",
    "  +100â€¦140 pentru aterizare reuÈ™itÄƒ,  \n",
    "  penalizÄƒri pentru coliziuni, unghiuri mari È™i consum excesiv de combustibil.\n",
    "\n",
    "Ãn codul de mai jos:\n",
    "- definim environmentul, parametrii de Ã®nvÄƒÈ›are È™i coeficienÈ›ii Q-Learning (`Î±`, `Î³`, `Îµ`);\n",
    "- aplicÄƒm **discretizare** pentru a transforma spaÈ›iul continuu Ã®ntr-un set finit de stÄƒri,  \n",
    "  astfel Ã®ncÃ¢t algoritmul Q-Learning sÄƒ poatÄƒ funcÈ›iona eficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f0896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=None)\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon, eps_min, eps_decay = 1.0, 0.01, 0.995\n",
    "episodes = 1000\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Discretizare: definim numÄƒrul de \"bin-uri\" pentru fiecare dimensiune\n",
    "bins = [\n",
    "    np.linspace(-1.5, 1.5, 7),  # x coord\n",
    "    np.linspace(-1.5, 1.5, 7),  # y coord\n",
    "    np.linspace(-2.0, 2.0, 7),  # x velocity\n",
    "    np.linspace(-2.0, 2.0, 7),  # y velocity\n",
    "    np.linspace(-np.pi, np.pi, 7),  # angle\n",
    "    np.linspace(-5.0, 5.0, 7),  # angular velocity\n",
    "    [0, 1],  # leg contact left\n",
    "    [0, 1],  # leg contact right\n",
    "]\n",
    "\n",
    "def discretize(obs):\n",
    "    state_idx = []\n",
    "    for i, b in enumerate(bins):\n",
    "        if len(b) == 2: \n",
    "            state_idx.append(int(obs[i]))\n",
    "        else:\n",
    "            state_idx.append(np.digitize(obs[i], b))\n",
    "    return tuple(state_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38fb99b",
   "metadata": {},
   "source": [
    "### ğŸ¤– Q-Learning â€“ Antrenarea agentului\n",
    "\n",
    "Ãn acest pas, agentul este antrenat folosind algoritmul **Q-Learning clasic**, care actualizeazÄƒ valorile Q pentru fiecare stare È™i acÈ›iune.  \n",
    "Agentul echilibreazÄƒ **explorarea** (alegerea aleatorie de acÈ›iuni) È™i **exploatarea** (folosirea celor mai bune acÈ›iuni cunoscute) prin parametrul `Îµ` (*epsilon*).  \n",
    "\n",
    "DupÄƒ fiecare episod:\n",
    "- valorile Q sunt ajustate Ã®n funcÈ›ie de recompensa primitÄƒ È™i starea urmÄƒtoare;\n",
    "- `Îµ` scade treptat, reducÃ¢nd explorarea Ã®n favoarea acÈ›iunilor Ã®nvÄƒÈ›ate;\n",
    "- se urmÄƒreÈ™te evoluÈ›ia **reward-ului mediu** pentru a evalua progresul agentului.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc22ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([len(b)+1 for b in bins] + [n_actions])\n",
    "rewards_per_episode = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    s, _ = env.reset()\n",
    "    s_disc = discretize(s)\n",
    "    done = False\n",
    "    total = 0\n",
    "\n",
    "    while not done:\n",
    "        # Explorare / exploatare\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = np.random.randint(n_actions)\n",
    "        else:\n",
    "            a = np.argmax(Q[s_disc])\n",
    "\n",
    "        s2, r, done, truncated, _ = env.step(a)\n",
    "        s2_disc = discretize(s2)\n",
    "\n",
    "        # Actualizare Q\n",
    "        Q[s_disc][a] += alpha * (r + gamma * np.max(Q[s2_disc]) * (not done) - Q[s_disc][a])\n",
    "        s_disc = s2_disc\n",
    "        total += r\n",
    "\n",
    "    epsilon = max(eps_min, epsilon * eps_decay)\n",
    "    rewards_per_episode.append(total)\n",
    "\n",
    "    if (ep + 1) % 100 == 0:\n",
    "        avg = np.mean(rewards_per_episode[-100:])\n",
    "        print(f\"Episod {ep+1}/{episodes} | Reward mediu: {avg:.2f} | Îµ={epsilon:.3f}\")\n",
    "\n",
    "window = 100\n",
    "plt.plot(np.convolve(rewards_per_episode, np.ones(window)/window, mode='valid'))\n",
    "plt.title(\"EvoluÈ›ia reward-ului mediu - LunarLander Q-Learning\")\n",
    "plt.xlabel(\"Episoade\")\n",
    "plt.ylabel(\"Reward mediu (100 episoade)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ea38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualizare dupÄƒ antrenare\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "\n",
    "def test_agent(Q, episodes=3):\n",
    "    for ep in range(episodes):\n",
    "        s, _ = env.reset()\n",
    "        s_disc = discretize(s)\n",
    "        done = False\n",
    "        total = 0\n",
    "        while not done:\n",
    "            a = np.argmax(Q[s_disc])\n",
    "            s, r, done, truncated, _ = env.step(a)\n",
    "            s_disc = discretize(s)\n",
    "            total += r\n",
    "        print(f\"Episod {ep+1} â†’ Reward total: {total:.2f}\")\n",
    "\n",
    "test_agent(Q) \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46529a1",
   "metadata": {},
   "source": [
    "### ğŸ§® Dimensiunea spaÈ›iului de stÄƒri È™i impactul asupra Q-Table\n",
    "\n",
    "Ãn algoritmii de tip **Q-Learning**, fiecare combinaÈ›ie posibilÄƒ de stare È™i acÈ›iune este stocatÄƒ Ã®n tabelul Q.  \n",
    "Prin urmare, dimensiunea spaÈ›iului de stÄƒri influenÈ›eazÄƒ direct **mÄƒrimea Q-table-ului** È™i memoria necesarÄƒ pentru antrenare.\n",
    "\n",
    "Codul de mai jos calculeazÄƒ È™i vizualizeazÄƒ cum creÈ™te dimensiunea Q-table-ului Ã®n funcÈ›ie de numÄƒrul de **bin-uri** folosite pentru discretizarea fiecÄƒrei variabile de stare.  \n",
    "Se observÄƒ o **creÈ™tere exponenÈ›ialÄƒ** â€” discretizÄƒri mai fine oferÄƒ precizie mai mare, dar cer mult mai multÄƒ memorie È™i timp de antrenare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def q_table_size(bins_per_dim, n_actions):\n",
    "    n_states = np.prod([b + 1 for b in bins_per_dim])\n",
    "    q_size = n_states * n_actions\n",
    "    mem_mb = q_size * 8 / (1024**2)\n",
    "    return n_states, q_size, mem_mb\n",
    "\n",
    "scenarios = {\n",
    "    \"3 bins\": [3,3,3,3,3,3,2,2],\n",
    "    \"5 bins\": [5,5,5,5,5,5,2,2],\n",
    "    \"7 bins\": [7,7,7,7,7,7,2,2],\n",
    "    \"10 bins\": [10,10,10,10,10,10,2,2],\n",
    "    \"15 bins\": [15,15,15,15,15,15,2,2],\n",
    "    \"20 bins\": [20,20,20,20,20,20,2,2],\n",
    "    \"50 bins\": [50,50,50,50,50,50,2,2]\n",
    "}\n",
    "\n",
    "for name, bins in scenarios.items():\n",
    "    n_states, q_size, mem = q_table_size(bins, 4)\n",
    "    print(f\"{name}: {n_states:,} stÄƒri | {q_size:,} Q-valori | {mem:.2f} MB\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins_range = [3, 5, 7, 10, 15, 20]\n",
    "mem_usage = []\n",
    "\n",
    "for b in bins_range:\n",
    "    n_states, q_size, mem = q_table_size([b]*6 + [2,2], 4)\n",
    "    mem_usage.append(mem)\n",
    "\n",
    "plt.plot(bins_range, mem_usage, marker='o')\n",
    "plt.title(\"CreÈ™terea memoriei Q-Table Ã®n funcÈ›ie de granularitate (LunarLander)\")\n",
    "plt.xlabel(\"NumÄƒr de bin-uri / dimensiune (primele 6 variabile)\")\n",
    "plt.ylabel(\"Dimensiune Q-table (MB)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1694dc",
   "metadata": {},
   "source": [
    "# OPTIMIZARI! \n",
    "# IMBUNATATIRI!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022c1e18",
   "metadata": {},
   "source": [
    "### ğŸ¯ Discretizare optimizatÄƒ a spaÈ›iului de stare\n",
    "\n",
    "Pentru a aplica Q-Learning pe un mediu cu stÄƒri continue, cum este **LunarLander-v2**, este necesarÄƒ o transformare discretÄƒ a observaÈ›iilor.  \n",
    "Ãn acest exemplu:\n",
    "\n",
    "- sunt pÄƒstrate doar cele **6 variabile esenÈ›iale** (poziÈ›ie, vitezÄƒ È™i unghi);  \n",
    "- valorile sunt **normalizate** Ã®n intervalul `[-1, 1]` pentru coerenÈ›Äƒ Ã®ntre dimensiuni;  \n",
    "- se aplicÄƒ o **discretizare neuniformÄƒ** cu mai multe bin-uri Ã®n zona de aterizare,  \n",
    "  pentru a creÈ™te precizia Ã®n regiunile critice ale mediului.\n",
    "\n",
    "AceastÄƒ metodÄƒ reduce dimensiunea Q-table-ului È™i accelereazÄƒ Ã®nvÄƒÈ›area fÄƒrÄƒ pierderi semnificative de performanÈ›Äƒ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03abe9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# IniÈ›ializÄƒm environmentul LunarLander\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# SelectÄƒm doar cele 6 variabile relevante din cele 8 disponibile:\n",
    "# [x, y, x_dot, y_dot, angle, angular_velocity]\n",
    "def select_features(obs):\n",
    "    return np.array(obs[:6])\n",
    "\n",
    "# Intervalele aproximative pentru fiecare variabilÄƒ (pentru normalizare)\n",
    "state_bounds = np.array([\n",
    "    [-1.5, 1.5],    # poziÈ›ie orizontalÄƒ\n",
    "    [-1.5, 1.5],    # poziÈ›ie verticalÄƒ\n",
    "    [-2.0, 2.0],    # vitezÄƒ orizontalÄƒ\n",
    "    [-2.0, 2.0],    # vitezÄƒ verticalÄƒ\n",
    "    [-np.pi, np.pi],# unghi\n",
    "    [-5.0, 5.0]     # vitezÄƒ unghiularÄƒ\n",
    "])\n",
    "\n",
    "# NormalizÄƒm starea Ã®n intervalul [-1, 1] pentru o scalare uniformÄƒ\n",
    "def normalize_state(state):\n",
    "    return 2 * (state - state_bounds[:,0]) / (state_bounds[:,1] - state_bounds[:,0]) - 1\n",
    "\n",
    "# Definim bin-urile pentru discretizare\n",
    "# y (poziÈ›ia verticalÄƒ) are bin-uri mai dense aproape de sol (unde are loc aterizarea)\n",
    "bins = [\n",
    "    np.linspace(-1.5, 1.5, 10),  \n",
    "    np.concatenate([np.linspace(-1.5, 0, 20), np.linspace(0, 1.5, 10)]),  \n",
    "    np.linspace(-2.0, 2.0, 8),   \n",
    "    np.linspace(-2.0, 2.0, 8),   \n",
    "    np.linspace(-np.pi, np.pi, 12),  \n",
    "    np.linspace(-5.0, 5.0, 10)   \n",
    "]\n",
    "\n",
    "# Convertim observaÈ›ia continuÄƒ Ã®ntr-un tuple de indici discreÈ›i\n",
    "def discretize(obs):\n",
    "    features = select_features(obs)\n",
    "    features = normalize_state(features)\n",
    "    state_idx = [np.digitize(features[i], bins[i]) for i in range(len(bins))]\n",
    "    return tuple(state_idx)\n",
    "\n",
    "# TestÄƒm discretizarea pentru o observaÈ›ie nouÄƒ\n",
    "obs, _ = env.reset()\n",
    "disc_state = discretize(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2739fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, gamma = 0.1, 0.99\n",
    "epsilon, eps_min, eps_decay = 1.0, 0.05, 0.995\n",
    "episodes = 2000\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "Q = np.zeros([len(b)+1 for b in bins] + [n_actions])\n",
    "rewards = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    s = discretize(obs)\n",
    "    done = False\n",
    "    total = 0\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = np.random.randint(n_actions)\n",
    "        else:\n",
    "            a = np.argmax(Q[s])\n",
    "        obs2, r, done, truncated, _ = env.step(a)\n",
    "        s2 = discretize(obs2)\n",
    "        Q[s][a] += alpha * (r + gamma * np.max(Q[s2]) * (not done) - Q[s][a])\n",
    "        s = s2\n",
    "        total += r\n",
    "\n",
    "    epsilon = max(eps_min, epsilon * eps_decay)\n",
    "    rewards.append(total)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "window = 100\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(np.convolve(rewards, np.ones(window)/window, mode='valid'))\n",
    "plt.title(\"EvoluÈ›ia reward-ului mediu - Q-Learning LunarLander\")\n",
    "plt.xlabel(\"Episoade\")\n",
    "plt.ylabel(\"Reward mediu (100 episoade)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ac11e",
   "metadata": {},
   "source": [
    "### ğŸšï¸ Epsilon Decay Adaptiv\n",
    "\n",
    "Ãn loc ca rata de explorare `Îµ` sÄƒ scadÄƒ constant, se ajusteazÄƒ dinamic Ã®n funcÈ›ie de performanÈ›a agentului:  \n",
    "- dacÄƒ **reward-ul mediu creÈ™te**, `Îµ` scade mai rapid â†’ agentul exploreazÄƒ mai puÈ›in È™i exploateazÄƒ mai mult;  \n",
    "- dacÄƒ **performanÈ›a stagneazÄƒ sau scade**, `Îµ` creÈ™te uÈ™or â†’ agentul revine la explorare.  \n",
    "\n",
    "Astfel, agentul Ã®È™i adapteazÄƒ comportamentul Ã®n timp, echilibrÃ¢nd mai bine explorarea È™i exploatarea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# IniÈ›ializÄƒm environmentul LunarLander\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "alpha, gamma = 0.1, 0.99\n",
    "epsilon, eps_min, eps_decay = 1.0, 0.05, 0.995\n",
    "episodes = 2000\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "Q = np.zeros([len(b)+1 for b in bins] + [n_actions])\n",
    "rewards = []\n",
    "eps_history = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    s = discretize(obs)\n",
    "    done = False\n",
    "    total = 0\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = np.random.randint(n_actions)\n",
    "        else:\n",
    "            a = np.argmax(Q[s])\n",
    "        obs2, r, done, truncated, _ = env.step(a)\n",
    "        s2 = discretize(obs2)\n",
    "        Q[s][a] += alpha * (r + gamma * np.max(Q[s2]) * (not done) - Q[s][a])\n",
    "        s = s2\n",
    "        total += r\n",
    "\n",
    "    rewards.append(total)\n",
    "    eps_history.append(epsilon)\n",
    "\n",
    "    if ep > 100:\n",
    "        avg_recent = np.mean(rewards[-100:])\n",
    "        avg_past = np.mean(rewards[-200:-100]) if ep >= 200 else -np.inf\n",
    "        if avg_recent > avg_past + 10:\n",
    "            epsilon *= 0.9\n",
    "        elif avg_recent < avg_past - 5:\n",
    "            epsilon = min(1.0, epsilon * 1.05)\n",
    "        else:\n",
    "            epsilon *= eps_decay\n",
    "    epsilon = max(eps_min, epsilon)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "window = 100\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(np.convolve(rewards, np.ones(window)/window, mode='valid'))\n",
    "plt.title(\"EvoluÈ›ia reward-ului mediu - Q-Learning cu epsilon adaptiv\")\n",
    "plt.xlabel(\"Episoade\")\n",
    "plt.ylabel(\"Reward mediu (100 episoade)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(eps_history)\n",
    "plt.title(\"EvoluÈ›ia dinamicÄƒ a epsilon\")\n",
    "plt.xlabel(\"Episoade\")\n",
    "plt.ylabel(\"Îµ\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b689219",
   "metadata": {},
   "source": [
    "### âš™ï¸ Reward Shaping\n",
    "\n",
    "Pentru a accelera Ã®nvÄƒÈ›area È™i a ghida agentul spre comportamente mai naturale,  \n",
    "se modificÄƒ recompensa implicitÄƒ adÄƒugÃ¢nd **penalizÄƒri** È™i **bonusuri** suplimentare:\n",
    "\n",
    "- penalizÄƒri pentru unghiuri mari È™i vitezÄƒ orizontalÄƒ excesivÄƒ aproape de sol;  \n",
    "- bonusuri pentru stabilitate (unghi mic È™i miÈ™care controlatÄƒ).  \n",
    "\n",
    "AceastÄƒ tehnicÄƒ, numitÄƒ *reward shaping*, ajutÄƒ agentul sÄƒ Ã®nveÈ›e mai rapid manevre sigure È™i sÄƒ evite comportamente instabile chiar Ã®nainte de convergenÈ›a completÄƒ a politicii.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "alpha, gamma = 0.1, 0.99\n",
    "epsilon, eps_min, eps_decay = 1.0, 0.05, 0.995\n",
    "episodes = 1500\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# SimplificÄƒm discretizarea pentru exemplu\n",
    "bins = [np.linspace(-1.5, 1.5, 8),\n",
    "        np.linspace(-1.5, 1.5, 8),\n",
    "        np.linspace(-2.0, 2.0, 8),\n",
    "        np.linspace(-2.0, 2.0, 8),\n",
    "        np.linspace(-np.pi, np.pi, 8),\n",
    "        np.linspace(-5.0, 5.0, 8)]\n",
    "\n",
    "def discretize(obs):\n",
    "    features = obs[:6]\n",
    "    return tuple(np.digitize(features[i], bins[i]) for i in range(len(bins)))\n",
    "\n",
    "# FuncÈ›ia de reward shaping\n",
    "def shape_reward(r, obs):\n",
    "    angle, x_dot, y = obs[4], obs[2], obs[1]\n",
    "    shaped_r = r\n",
    "\n",
    "    # AplicÄƒm shaping doar cÃ¢nd agentul e aproape de sol\n",
    "    if y < 0.2:\n",
    "        shaped_r -= 2 * abs(angle)\n",
    "        shaped_r -= 1 * abs(x_dot)\n",
    "\n",
    "        # Bonus pentru stabilitate\n",
    "        if abs(angle) < 0.1 and abs(x_dot) < 0.1:\n",
    "            shaped_r += 3\n",
    "\n",
    "    return shaped_r\n",
    "\n",
    "Q = np.zeros([len(b)+1 for b in bins] + [n_actions])\n",
    "rewards = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    s = discretize(obs)\n",
    "    done = False\n",
    "    total = 0\n",
    "\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = np.random.randint(n_actions)\n",
    "        else:\n",
    "            a = np.argmax(Q[s])\n",
    "        obs2, r, done, truncated, _ = env.step(a)\n",
    "        r_shaped = shape_reward(r, obs2)\n",
    "        s2 = discretize(obs2)\n",
    "        Q[s][a] += alpha * (r_shaped + gamma * np.max(Q[s2]) * (not done) - Q[s][a])\n",
    "        s = s2\n",
    "        total += r_shaped\n",
    "\n",
    "    epsilon = max(eps_min, epsilon * eps_decay)\n",
    "    rewards.append(total)\n",
    "\n",
    "window = 100\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(np.convolve(rewards, np.ones(window)/window, mode='valid'))\n",
    "plt.title(\"EvoluÈ›ia reward-ului mediu - Q-Learning cu Reward Shaping\")\n",
    "plt.xlabel(\"Episoade\")\n",
    "plt.ylabel(\"Reward mediu (100 episoade)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
