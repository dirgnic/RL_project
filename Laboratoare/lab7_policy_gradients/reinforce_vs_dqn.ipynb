{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b066721a",
   "metadata": {},
   "source": [
    "## REINFORCE + Baseline - Lunar Lander v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b08f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# Networks\n",
    "# ============================================================\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 4),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Returns (discounted sum of rewards)\n",
    "# ============================================================\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    G = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    return returns\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training Loop\n",
    "# ============================================================\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "policy = PolicyNet()\n",
    "value_net = ValueNet()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    list(policy.parameters()) + list(value_net.parameters()),\n",
    "    lr=0.0005\n",
    ")\n",
    "\n",
    "episodes = 1500\n",
    "all_rewards = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    log_probs, values, rewards = [], [], []\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        state_t = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        probs = policy(state_t)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        value = value_net(state_t)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        log_probs.append(dist.log_prob(action))\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # episode reward\n",
    "    total_reward = sum(rewards)\n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "    # returns + advantages\n",
    "    returns = torch.tensor(compute_returns(rewards), dtype=torch.float32)\n",
    "    values = torch.cat(values).squeeze()\n",
    "\n",
    "    # normalize returns (helps a lot)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-7)\n",
    "\n",
    "    advantages = returns - values.detach()\n",
    "\n",
    "    # losses\n",
    "    policy_loss = -(advantages * torch.stack(log_probs)).sum()\n",
    "    value_loss = nn.functional.mse_loss(values, returns)\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if ep % 50 == 0:\n",
    "        print(f\"[REINFORCE] Episode {ep}, Reward: {total_reward}\")\n",
    "\n",
    "# ============================================================\n",
    "# PLOT\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_rewards, alpha=0.4)\n",
    "plt.plot(np.convolve(all_rewards, np.ones(20)/20, mode=\"valid\"))\n",
    "plt.title(\"REINFORCE + Baseline on LunarLander-v2\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad360e",
   "metadata": {},
   "source": [
    "## DQN + Target - Lunar Lander v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673153a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "# ============================================================\n",
    "# Q-Network\n",
    "# ============================================================\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters\n",
    "# ============================================================\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "lr = 0.0005\n",
    "gamma = 0.99\n",
    "episodes = 600\n",
    "batch_size = 64\n",
    "buffer_size = 100_000\n",
    "min_replay = 1000\n",
    "target_update = 1000\n",
    "\n",
    "epsilon = 1.0\n",
    "eps_min = 0.01\n",
    "eps_decay = 0.995\n",
    "\n",
    "replay = deque(maxlen=buffer_size)\n",
    "\n",
    "q_net = QNet()\n",
    "target_net = QNet()\n",
    "target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "\n",
    "all_rewards = []\n",
    "step_count = 0\n",
    "\n",
    "# ============================================================\n",
    "# Training Loop\n",
    "# ============================================================\n",
    "for ep in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = q_net(torch.tensor(state, dtype=torch.float32)).argmax().item()\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        replay.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # training only when enough samples\n",
    "        if len(replay) >= min_replay:\n",
    "            batch = random.sample(replay, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32)\n",
    "            actions = torch.tensor(actions).long()\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "            # Q(s,a)\n",
    "            q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "            # target = r + Î³ max_a Q_target(s',a)\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(next_states).max(1)[0]\n",
    "                target = rewards + gamma * next_q * (1 - dones)\n",
    "\n",
    "            loss = nn.functional.mse_loss(q_values, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # update target network\n",
    "        if step_count % target_update == 0:\n",
    "            target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "    # epsilon decay\n",
    "    epsilon = max(eps_min, epsilon * eps_decay)\n",
    "\n",
    "    if ep % 20 == 0:\n",
    "        print(f\"[DQN] Episode {ep}, Reward: {total_reward}, epsilon={epsilon:.3f}\")\n",
    "\n",
    "# ============================================================\n",
    "# PLOT\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_rewards, alpha=0.4)\n",
    "plt.plot(np.convolve(all_rewards, np.ones(20)/20, mode=\"valid\"))\n",
    "plt.title(\"DQN on LunarLander-v2\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
