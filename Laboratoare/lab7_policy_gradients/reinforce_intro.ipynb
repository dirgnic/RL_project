{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3838c5f8",
   "metadata": {},
   "source": [
    "# Algoritmul REINFORCE (Policy Gradient)\n",
    "\n",
    "Algoritmul REINFORCE este forma de bază a tehnicilor de *policy gradient*. Scopul său este de a învăța direct o politică stochastică, fără a folosi funcții de valoare precum Q(s, a). Politica este parametrizată și produce probabilități pentru acțiuni.\n",
    "\n",
    "---\n",
    "\n",
    "## Obiectiv\n",
    "\n",
    "Scopul trainingului este maximizarea rewardului total așteptat:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[R]\n",
    "$$\n",
    "\n",
    "Gradientul folosit pentru actualizarea politicii este:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[ G_t \\cdot \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Pașii algoritmului\n",
    "\n",
    "### 1. Generarea unui episod\n",
    "\n",
    "Agentul rulează un episod complet și colectează:\n",
    "\n",
    "- stările \n",
    "- acțiunile \n",
    "- log-probabilitățile \n",
    "- recompensele\n",
    "\n",
    "Politica este stochastică, deci acțiunile sunt alese prin *sampling* din distribuție.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Calcularea returnurilor\n",
    "\n",
    "Pentru fiecare pas t calculăm returnul discountat:\n",
    "\n",
    "$$\n",
    "G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots\n",
    "$$\n",
    "\n",
    "Acesta reprezintă suma rewardurilor începând de la momentul \\( t \\).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Normalizarea returnurilor\n",
    "\n",
    "Pentru a reduce varianța gradientului, returnurile pot fi normalizate:\n",
    "\n",
    "$$\n",
    "\\hat{G_t} = \\frac{G_t - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Acest pas este opțional, dar recomandat pentru stabilitatea antrenării.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Calcularea funcției de pierdere\n",
    "\n",
    "Funcția de pierdere folosită în REINFORCE este:\n",
    "\n",
    "$$\n",
    "L(\\theta) = - \\sum_t G_t \\cdot \\log \\pi_\\theta(a_t \\mid s_t)\n",
    "$$\n",
    "\n",
    "Interpretare:\n",
    "\n",
    "- acțiunile care au dus la reward mare primesc gradient pozitiv → devin mai probabile\n",
    "- acțiunile care au dus la reward mic primesc gradient negativ → devin mai puțin probabile\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Actualizarea parametrilor\n",
    "\n",
    "Parametrii politicii sunt actualizați prin gradient descent:\n",
    "\n",
    "- se calculează gradientul parametrului theta\n",
    "- se aplică un optimizer precum Adam\n",
    "\n",
    "---\n",
    "\n",
    "## Intuiție\n",
    "\n",
    "Algoritmul întărește acțiunile care au dus la reward mare și penalizează acțiunile care au dus la reward mic. Este un algoritm simplu și elegant, dar are varianță mare deoarece update-urile se fac doar la finalul episodului.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitări\n",
    "\n",
    "- varianță ridicată a gradientului  \n",
    "- poate „uita” comportament bun între episoade  \n",
    "- învățare instabilă în medii complexe  \n",
    "\n",
    "---\n",
    "\n",
    "## Observații\n",
    "\n",
    "REINFORCE este excelent pentru învățare conceptuală și pentru medii simple precum CartPole.  \n",
    "Algoritmi precum Actor-Critic, A2C sau PPO sunt variante mai stabile construite pe aceleași principii.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f137550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# Policy Network\n",
    "# ---------------------------\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Softmax(dim=-1)   # produce probabilitățile pentru acțiuni\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# REINFORCE Algorithm\n",
    "# ---------------------------\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    G = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    return returns\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "policy = PolicyNet()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "episodes = 1000\n",
    "all_rewards = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        state_t = torch.tensor(state, dtype=torch.float32)\n",
    "        probs = policy(state_t)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "\n",
    "    # total reward for logging\n",
    "    total_reward = sum(rewards)\n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "    # compute discounted returns\n",
    "    returns = compute_returns(rewards)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    # normalize returns to reduce variance\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-7)\n",
    "\n",
    "    # compute loss (REINFORCE)\n",
    "    loss = 0\n",
    "    for log_p, G in zip(log_probs, returns):\n",
    "        loss -= log_p * G\n",
    "\n",
    "    # gradient step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if ep % 20 == 0:\n",
    "        print(f\"Episode {ep}, reward: {total_reward}\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Plot performance\n",
    "# ---------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def smooth(data, window=20):\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "\n",
    "plt.plot(all_rewards, alpha=0.4, label=\"Raw (noisy)\")\n",
    "plt.plot(smooth(all_rewards, 20), linewidth=2, label=\"Smoothed\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title(\"REINFORCE on CartPole (Smoothed)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860b36ae",
   "metadata": {},
   "source": [
    "### REINFORCE + Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a87a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# Policy Network\n",
    "# ---------------------------\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Value Network (Baseline)\n",
    "# ---------------------------\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)     # produce estimarea V(s)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Compute discounted returns\n",
    "# ---------------------------\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    G = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    return returns\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "policy = PolicyNet()\n",
    "value_fn = ValueNet()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    list(policy.parameters()) + list(value_fn.parameters()),\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "episodes = 1000\n",
    "all_rewards = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        state_t = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        # policy forward\n",
    "        probs = policy(state_t)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        # critic forward\n",
    "        value = value_fn(state_t)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        log_probs.append(dist.log_prob(action))\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    total_reward = sum(rewards)\n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "    # discounted returns\n",
    "    returns = torch.tensor(compute_returns(rewards), dtype=torch.float32)\n",
    "\n",
    "    # compute advantages\n",
    "    values = torch.cat(values).squeeze()\n",
    "    advantages = returns - values.detach()  # IMPORTANT: no grad through values here\n",
    "\n",
    "    # ---------------------------\n",
    "    # Loss = Policy loss + Value loss\n",
    "    # ---------------------------\n",
    "    policy_loss = -(log_probs[0] * advantages[0])\n",
    "    for log_p, adv in zip(log_probs, advantages):\n",
    "        policy_loss += -(log_p * adv)\n",
    "\n",
    "    # MSE loss pentru baseline\n",
    "    value_loss = nn.functional.mse_loss(values, returns)\n",
    "\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if ep % 20 == 0:\n",
    "        print(f\"Episode {ep}, reward: {total_reward}\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Plot performance\n",
    "# ---------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def smooth(data, window=20):\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.plot(all_rewards, alpha=0.4, label=\"Raw\")\n",
    "plt.plot(smooth(all_rewards, 20), label=\"Smoothed\", linewidth=2)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title(\"REINFORCE + Baseline on CartPole\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
