{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d41cf3",
   "metadata": {},
   "source": [
    "## ğŸ§­ Introducere Ã®n Markov Decision Processes (MDP)\n",
    "\n",
    "Un **Markov Decision Process (MDP)** este o formulare matematicÄƒ a unui mediu Ã®n care un agent interacÈ›ioneazÄƒ Ã®n mod secvenÈ›ial, luÃ¢nd decizii care influenÈ›eazÄƒ stÄƒrile viitoare È™i recompensele primite.\n",
    "\n",
    "Un MDP este definit astfel:\n",
    "\n",
    "$$\n",
    "\\text{MDP} = (S, A, P, R, \\gamma)\n",
    "$$\n",
    "\n",
    "unde:\n",
    "- **S** â€” setul de stÄƒri posibile;\n",
    "- **A** â€” setul de acÈ›iuni disponibile;\n",
    "- **P(s'|s, a)** â€” probabilitatea de a ajunge Ã®n starea `s'` dupÄƒ ce agentul alege acÈ›iunea `a` Ã®n starea `s`;\n",
    "- **R(s, a, s')** â€” recompensa primitÄƒ pentru tranziÈ›ia `(s â†’ s')`;\n",
    "- **Î³** â€” factorul de discount (0 â‰¤ Î³ â‰¤ 1), care controleazÄƒ importanÈ›a viitorului.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Proprietatea Markov\n",
    "\n",
    "MDP-urile respectÄƒ **proprietatea Markov**:\n",
    "\n",
    "> Viitorul depinde doar de prezent, nu È™i de trecut.\n",
    "\n",
    "Formal:\n",
    "\n",
    "$$\n",
    "P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \\dots) = P(s_{t+1} | s_t, a_t)\n",
    "$$\n",
    "\n",
    "Aceasta presupune cÄƒ **starea curentÄƒ conÈ›ine toate informaÈ›iile relevante** pentru a prezice evoluÈ›ia sistemului.  \n",
    "Ãn practicÄƒ, aceastÄƒ condiÈ›ie este rareori perfect Ã®ndeplinitÄƒ â€” de aceea, definirea stÄƒrii devine o cea mai importantÄƒ parte Ã®n designul mediilor RL.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§® Politici È™i funcÈ›ii de valoare\n",
    "\n",
    "O **politicÄƒ** Ï€ este o strategie de decizie a agentului:\n",
    "\n",
    "$$\n",
    "\\pi(a|s) = P(A_t = a | S_t = s)\n",
    "$$\n",
    "\n",
    "Ea defineÈ™te distribuÈ›ia probabilitÄƒÈ›ilor asupra acÈ›iunilor posibile Ã®n fiecare stare.  \n",
    "DacÄƒ politica este deterministÄƒ, putem scrie simplu:\n",
    "\n",
    "$$\n",
    "a = \\pi(s)\n",
    "$$\n",
    "\n",
    "FuncÈ›ia de **valoare a unei stÄƒri** sub o politicÄƒ Ï€ se defineÈ™te ca:\n",
    "\n",
    "$$\n",
    "V^{\\pi}(s) = \\mathbb{E}_\\pi [ G_t \\,|\\, S_t = s ]\n",
    "$$\n",
    "\n",
    "unde $G_t$ este **returnul** total obÈ›inut de agent:\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\n",
    "$$\n",
    "\n",
    "Intuitiv, $V^{\\pi}(s)$ ne spune **cÃ¢t de â€bunÄƒâ€ este o stare**, dacÄƒ agentul continuÄƒ sÄƒ urmeze politica Ï€.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” EcuaÈ›iile Bellman\n",
    "\n",
    "Valorile stÄƒrilor se leagÄƒ Ã®ntre ele prin **ecuaÈ›iile Bellman**, care exprimÄƒ recurenÈ›a dintre valoarea curentÄƒ È™i valorile stÄƒrilor urmÄƒtoare:\n",
    "\n",
    "$$\n",
    "V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) \\big[ R(s,a,s') + \\gamma V^{\\pi}(s') \\big]\n",
    "$$\n",
    "\n",
    "Aceasta este baza tuturor metodelor de evaluare È™i Ã®mbunÄƒtÄƒÈ›ire a politicilor Ã®n RL:\n",
    "- **Evaluare**: estimÄƒm $V^{\\pi}$ pentru o politicÄƒ datÄƒ.  \n",
    "- **ÃmbunÄƒtÄƒÈ›ire**: ajustÄƒm politica pentru a maximiza valoarea stÄƒrilor.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§­ Obiectivul agentului\n",
    "\n",
    "Scopul unui agent RL este sÄƒ gÄƒseascÄƒ **politica optimÄƒ**:\n",
    "\n",
    "$$\n",
    "\\pi^* = \\arg\\max_\\pi V^{\\pi}(s)\n",
    "$$\n",
    "\n",
    "care maximizeazÄƒ recompensa cumulativÄƒ aÈ™teptatÄƒ din orice stare iniÈ›ialÄƒ.\n",
    "\n",
    "Ãn practicÄƒ, agentul nu cunoaÈ™te funcÈ›iile $P$ È™i $R$, ci le descoperÄƒ prin **explorare activÄƒ** â€” acesta fiind motivul pentru care RL se bazeazÄƒ pe Ã®nvÄƒÈ›are din interacÈ›iune, nu din demonstraÈ›ii explicite.\n",
    "\n",
    "---\n",
    "\n",
    "**Pe scurt:**  \n",
    "- Un MDP este â€scheletulâ€ teoretic al Reinforcement Learning-ului.\n",
    "- MDP defineÈ™te *cum* acÈ›ioneazÄƒ agentul, *ce* primeÈ™te Ã®n schimb È™i *cum* Ã®nvaÈ›Äƒ sÄƒ ia decizii mai bune Ã®n timp.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85fc48",
   "metadata": {},
   "source": [
    "## Definirea unui mediu custom (GridWorld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ae3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Definim dimensiunile GridWorld-ului\n",
    "nrow, ncol = 4, 4\n",
    "S = nrow * ncol  # numÄƒrul total de stÄƒri\n",
    "A = 4  # 0: sus, 1: dreapta, 2: jos, 3: stÃ¢nga\n",
    "gamma = 0.99\n",
    "\n",
    "# StÄƒri terminale: +1 È™i -1\n",
    "terminal_plus, terminal_minus = 15, 12\n",
    "\n",
    "def to_rc(s):\n",
    "    \"\"\"Conversie index -> (linie, coloanÄƒ)\"\"\"\n",
    "    return divmod(s, ncol)\n",
    "\n",
    "def to_s(r, c):\n",
    "    \"\"\"Conversie (linie, coloanÄƒ) -> index\"\"\"\n",
    "    return r * ncol + c\n",
    "\n",
    "def step_deterministic(s, a):\n",
    "    \"\"\"TranziÈ›ie deterministÄƒ simplÄƒ\"\"\"\n",
    "    if s in (terminal_plus, terminal_minus):\n",
    "        return s, 0.0, True  # stÄƒri terminale\n",
    "    \n",
    "    r, c = to_rc(s)\n",
    "    if a == 0: r = max(0, r - 1)      # sus\n",
    "    elif a == 1: c = min(ncol - 1, c + 1)  # dreapta\n",
    "    elif a == 2: r = min(nrow - 1, r + 1)  # jos\n",
    "    elif a == 3: c = max(0, c - 1)      # stÃ¢nga\n",
    "    \n",
    "    s2 = to_s(r, c)\n",
    "    if s2 == terminal_plus:\n",
    "        return s2, +1.0, True\n",
    "    elif s2 == terminal_minus:\n",
    "        return s2, -1.0, True\n",
    "    else:\n",
    "        return s2, -0.01, False  # micÄƒ penalizare pentru fiecare pas\n",
    "\n",
    "# === FuncÈ›ie pentru afiÈ™area grilei ===\n",
    "def render_grid(s, visited=None):\n",
    "    \"\"\"AfiÈ™eazÄƒ grila 4x4, cu agentul È™i stÄƒrile terminale\"\"\"\n",
    "    grid = [[\".\" for _ in range(ncol)] for _ in range(nrow)]\n",
    "    r, c = to_rc(s)\n",
    "    grid[r][c] = \"A\"\n",
    "    tr, tc = to_rc(terminal_plus)\n",
    "    grid[tr][tc] = \"G\"  # goal\n",
    "    tr, tc = to_rc(terminal_minus)\n",
    "    grid[tr][tc] = \"X\"  # pericol\n",
    "    \n",
    "    if visited:\n",
    "        for v in visited:\n",
    "            if v not in (s, terminal_plus, terminal_minus):\n",
    "                vr, vc = to_rc(v)\n",
    "                grid[vr][vc] = \"*\"\n",
    "    \n",
    "    print()\n",
    "    for row in grid:\n",
    "        print(\" \".join(row))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22ee412",
   "metadata": {},
   "source": [
    "## Utilizarea mediului custom cu un agent random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Simulare episod + salvare cadre text ===\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def policy(s): return rng.integers(0, A)\n",
    "\n",
    "def run_episode_with_render(max_steps=50, delay=0.3):\n",
    "    s = rng.integers(0, S)\n",
    "    total_reward = 0\n",
    "    frames = []\n",
    "    visited = set()\n",
    "    \n",
    "    print(f\"ğŸ¬ Pornim un episod (stare iniÈ›ialÄƒ: {s})\")\n",
    "    for t in range(max_steps):\n",
    "        visited.add(s)\n",
    "        frame_text = f\"\\n--- PASUL {t+1} ---\\n\"\n",
    "        frame_text += f\"Stare: {s}\\n\"\n",
    "        frame_text += f\"RecompensÄƒ totalÄƒ: {total_reward:.2f}\\n\"\n",
    "        frames.append(frame_text)\n",
    "        print(frame_text)\n",
    "        render_grid(s, visited)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        a = policy(s)\n",
    "        s2, r, done = step_deterministic(s, a)\n",
    "        total_reward += r\n",
    "        s = s2\n",
    "        if done:\n",
    "            render_grid(s, visited)\n",
    "            print(f\"ğŸ Episod terminat dupÄƒ {t+1} paÈ™i. RecompensÄƒ totalÄƒ: {total_reward:.2f}\")\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "# === RuleazÄƒ exemplu ===\n",
    "frames = run_episode_with_render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
