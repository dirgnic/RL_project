{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c99748",
   "metadata": {},
   "source": [
    "# Lab 2 â€” Gymnasium Environments Tour\n",
    "\n",
    "Ãn acest notebook explorÄƒm cÃ¢teva medii, modul lor de iniÈ›ializare, particularitÄƒÈ›i, seeding, wrappers È™i rularea unui agent aleator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681f10d",
   "metadata": {},
   "source": [
    "## ğŸ”¹ Setup / Importuri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb741951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, numpy as np, matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "print('Python:', sys.version)\n",
    "print('Gymnasium:', gym.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f998f9",
   "metadata": {},
   "source": [
    "## âš–ï¸ CartPole-v1\n",
    "\n",
    "**CartPole** este unul dintre cele mai simple È™i mai utilizate medii din `gymnasium`, ideal pentru a Ã®nÈ›elege conceptele de *stare*, *acÈ›iune*, *recompensÄƒ* È™i *episoade*.  \n",
    "Scopul agentului este sÄƒ **menÈ›inÄƒ o barÄƒ verticalÄƒ echilibratÄƒ** pe un cÄƒrucior care se deplaseazÄƒ pe o linie orizontalÄƒ.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Descrierea mediului\n",
    "\n",
    "- ğŸ§© **SpaÈ›iul de observaÈ›ie (`observation_space`)**  \n",
    "  Vector cu 4 valori continue:\n",
    "  1. PoziÈ›ia cÄƒruciorului *(cart position)*  \n",
    "  2. Viteza cÄƒruciorului *(cart velocity)*  \n",
    "  3. Unghiul barei *(pole angle)*  \n",
    "  4. Viteza unghiularÄƒ a barei *(pole angular velocity)*  \n",
    "\n",
    "  Exemplu: `[-0.005, 0.03, 0.012, -0.05]`\n",
    "\n",
    "- ğŸ® **SpaÈ›iul de acÈ›iuni (`action_space`)**  \n",
    "  Discret, cu douÄƒ posibile acÈ›iuni:\n",
    "  - `0` â†’ Ã®mpinge cÄƒruciorul spre stÃ¢nga  \n",
    "  - `1` â†’ Ã®mpinge cÄƒruciorul spre dreapta  \n",
    "\n",
    "- ğŸ† **RecompensÄƒ (`reward`)**  \n",
    "  +1 pentru fiecare pas Ã®n care bara rÄƒmÃ¢ne Ã®n poziÈ›ie stabilÄƒ.  \n",
    "  Episodul se terminÄƒ dacÄƒ bara cade prea mult (|angle| > 12Â°) sau dacÄƒ cÄƒruciorul pÄƒrÄƒseÈ™te zona permisÄƒ (|position| > 2.4).  \n",
    "\n",
    "- â±ï¸ **Durata maximÄƒ**  \n",
    "  500 de paÈ™i per episod (Ã®n versiunea `v1`).\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“˜ DocumentaÈ›ie oficialÄƒ\n",
    "\n",
    "ğŸ”— [CartPole-v1 â€” Gymnasium Documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¬ Curiozitate\n",
    "\n",
    "Acesta este unul dintre primele exemple de RL!!! â€”  \n",
    "Ã®n anii â€™80â€“â€™90, agenÈ›i *Q-learning* È™i *Policy Gradient* erau antrenaÈ›i sÄƒ Ã®nveÈ›e echilibrul doar prin Ã®ncercÄƒri È™i recompense, fÄƒrÄƒ nicio informaÈ›ie explicitÄƒ despre fizicÄƒ.\n",
    "\n",
    "---\n",
    "\n",
    "**Obiectiv:**  \n",
    "> Agentul trebuie sÄƒ maximizeze recompensa totalÄƒ (sÄƒ È›inÄƒ bara echilibratÄƒ cÃ¢t mai mult timp).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791beb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "\n",
    "# === IniÈ›ializare mediu ===\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "frames = []         \n",
    "states = []         \n",
    "actions = []        \n",
    "rewards = []        \n",
    "\n",
    "# === Rulare episod random ===\n",
    "for t in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    frames.append(env.render()) \n",
    "    states.append(obs)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# === Rezumat episod ===\n",
    "steps = len(states)\n",
    "total_reward = sum(rewards)\n",
    "print(f\"ğŸ§© Episod Ã®ncheiat Ã®n {steps} paÈ™i\")\n",
    "print(f\"ğŸ† RecompensÄƒ totalÄƒ: {total_reward:.2f}\\n\")\n",
    "\n",
    "# === Tabel cu stÄƒri È™i acÈ›iuni ===\n",
    "df = pd.DataFrame(states, columns=[\"Cart Position\", \"Cart Velocity\", \"Pole Angle\", \"Pole Angular Velocity\"])\n",
    "df[\"Action\"] = actions\n",
    "df[\"Reward\"] = rewards\n",
    "display(df.head(10))\n",
    "\n",
    "# === AnimaÈ›ie ===\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(frames[0])\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"CartPole-v1 â€” Random agent\")\n",
    "\n",
    "def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, update, frames=frames, interval=50, blit=True, repeat=False\n",
    ")\n",
    "\n",
    "plt.close(fig)\n",
    "display(HTML(ani.to_jshtml()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d0ad4",
   "metadata": {},
   "source": [
    "## ğŸ”ï¸ MountainCar-v0\n",
    "\n",
    "**MountainCar** este un mediu clasic de control din `gymnasium`, folosit pentru a studia conceptele de **explorare**, **recompense Ã®ntÃ¢rziate** È™i **Ã®nvÄƒÈ›are Ã®n medii sub-optime**.  \n",
    "Scopul agentului este sÄƒ **urce o maÈ™inuÈ›Äƒ pe vÃ¢rful dealului din dreapta**, deÈ™i motorul nu este suficient de puternic pentru a ajunge direct acolo.\n",
    "\n",
    "Agentul trebuie sÄƒ Ã®nveÈ›e sÄƒ **balanseze Ã®ntre cei doi versanÈ›i** (stÃ¢ngaâ€“dreapta) pentru a acumula suficientÄƒ energie cineticÄƒ È™i a depÄƒÈ™i obstacolul.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Descrierea mediului\n",
    "\n",
    "- ğŸ§© **SpaÈ›iul de observaÈ›ie (`observation_space`)**  \n",
    "  Vector cu **2 valori continue**:\n",
    "  1. PoziÈ›ia maÈ™inii *(position)*  \n",
    "  2. Viteza maÈ™inii *(velocity)*  \n",
    "\n",
    "  Exemplu: `[-0.45, 0.02]`\n",
    "\n",
    "  Domenii posibile:  \n",
    "  - PoziÈ›ia âˆˆ [âˆ’1.2, 0.6]  \n",
    "  - Viteza âˆˆ [âˆ’0.07, 0.07]\n",
    "\n",
    "- ğŸ® **SpaÈ›iul de acÈ›iuni (`action_space`)**  \n",
    "  Discret, cu **3 acÈ›iuni posibile**:\n",
    "  - `0` â†’ accelereazÄƒ spre stÃ¢nga  \n",
    "  - `1` â†’ nu accelereazÄƒ (rÄƒmÃ¢ne pe loc)  \n",
    "  - `2` â†’ accelereazÄƒ spre dreapta  \n",
    "\n",
    "- ğŸ† **RecompensÄƒ (`reward`)**  \n",
    "  - âˆ’1 pentru fiecare pas (agentul este penalizat continuu pÃ¢nÄƒ cÃ¢nd rezolvÄƒ sarcina).  \n",
    "  - Episodul se terminÄƒ dacÄƒ:\n",
    "    - PoziÈ›ia maÈ™inii â‰¥ `0.5` (ajunge Ã®n vÃ¢rful dealului din dreapta) â†’ succes ğŸ‰  \n",
    "    - NumÄƒrul de paÈ™i depÄƒÈ™eÈ™te limita maximÄƒ (default: 200).  \n",
    "\n",
    "- â±ï¸ **Durata maximÄƒ**  \n",
    "  200 de paÈ™i per episod.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“˜ DocumentaÈ›ie oficialÄƒ\n",
    "\n",
    "ğŸ”— [MountainCar-v0 â€” Gymnasium Documentation](https://gymnasium.farama.org/environments/classic_control/mountain_car/)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¬ Curiozitate\n",
    "\n",
    "Acest mediu este un exemplu clasic de **problemÄƒ cu recompensÄƒ Ã®ntÃ¢rziatÄƒ** â€”  \n",
    "agentul trebuie sÄƒ Ã®nveÈ›e cÄƒ acÈ›iunile â€negativeâ€ (alunecarea Ã®napoi) pot fi **necesare** pentru a atinge scopul final.  \n",
    "\n",
    "Ãn primele experimente, algoritmi precum *Q-learning* È™i *SARSA* au fost testaÈ›i aici pentru a evalua **strategiile de explorare vs. exploatare**.\n",
    "\n",
    "---\n",
    "\n",
    "**Obiectiv:**  \n",
    "> Agentul trebuie sÄƒ ajungÄƒ Ã®n vÃ¢rful dealului din dreapta (poziÈ›ia â‰¥ 0.5) Ã®n cÃ¢t mai puÈ›ini paÈ™i, Ã®nvÄƒÈ›Ã¢nd sÄƒ foloseascÄƒ inerÈ›ia È™i sÄƒ planifice acÈ›iuni eficiente pe termen lung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "\n",
    "# === IniÈ›ializare mediu ===\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "frames = []         \n",
    "states = []         \n",
    "actions = []        \n",
    "rewards = []        \n",
    "\n",
    "# === Rulare episod random ===\n",
    "for t in range(200): \n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    frames.append(env.render())  \n",
    "    states.append(obs)\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# === Rezumat episod ===\n",
    "steps = len(states)\n",
    "total_reward = sum(rewards)\n",
    "print(f\"ğŸ”ï¸ Episod Ã®ncheiat Ã®n {steps} paÈ™i\")\n",
    "print(f\"ğŸ† RecompensÄƒ totalÄƒ: {total_reward:.2f}\\n\")\n",
    "\n",
    "# === Tabel cu stÄƒri È™i acÈ›iuni ===\n",
    "df = pd.DataFrame(states, columns=[\"Position\", \"Velocity\"])\n",
    "df[\"Action\"] = actions\n",
    "df[\"Reward\"] = rewards\n",
    "display(df.head(10))  \n",
    "\n",
    "# === AnimaÈ›ie ===\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(frames[0])\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"MountainCar-v0 â€” Random agent\")\n",
    "\n",
    "def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, update, frames=frames, interval=50, blit=True, repeat=False\n",
    ")\n",
    "\n",
    "plt.close(fig)\n",
    "display(HTML(ani.to_jshtml()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98edfc1",
   "metadata": {},
   "source": [
    "### ğŸ§© TODO-uri pentru studenÈ›i\n",
    "1. ModificaÈ›i codul astfel Ã®ncÃ¢t sÄƒ ruleze 10 episoade È™i sÄƒ afiÈ™eze media recompenselor.\n",
    "2. ImplementaÈ›i un agent simplu care accelereazÄƒ spre dreapta doar dacÄƒ viteza > 0 (\"greedy\").\n",
    "3. ComparaÈ›i numÄƒrul de paÈ™i pÃ¢nÄƒ la succes Ã®ntre agentul aleator È™i agentul â€greedyâ€.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921f595",
   "metadata": {},
   "source": [
    "## ğŸ§Š FrozenLake-v1\n",
    "\n",
    "**FrozenLake** este un mediu clasic din `gymnasium` care modeleazÄƒ o problemÄƒ de **navigare Ã®ntr-un mediu discret È™i stocastic**.  \n",
    "Agentul trebuie sÄƒ ajungÄƒ dintr-un punct de start `S` la un punct È›intÄƒ `G`, deplasÃ¢ndu-se pe o suprafaÈ›Äƒ alunecoasÄƒ de gheaÈ›Äƒ, fÄƒrÄƒ sÄƒ cadÄƒ Ã®n gÄƒurile / copcile `H`.  \n",
    "\n",
    "Este un exemplu excelent pentru a studia conceptele de **politicÄƒ stocasticÄƒ**, **tranziÈ›ii nedeterministe**, **explorare sigurÄƒ** È™i **valoarea stÄƒrilor**.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Descrierea mediului\n",
    "\n",
    "- ğŸ§© **SpaÈ›iul de observaÈ›ie (`observation_space`)**  \n",
    "  Discret, reprezentÃ¢nd poziÈ›ia agentului pe o grilÄƒ pÄƒtratÄƒ (de exemplu 4Ã—4 â†’ 16 stÄƒri posibile).  \n",
    "  Fiecare stare este indexatÄƒ de la `0` la `n-1`.  \n",
    "  Exemplu: `obs = 5` Ã®nseamnÄƒ cÄƒ agentul se aflÄƒ pe a doua linie, a doua coloanÄƒ.\n",
    "\n",
    "- ğŸ® **SpaÈ›iul de acÈ›iuni (`action_space`)**  \n",
    "  Discret, cu **4 acÈ›iuni posibile**:\n",
    "  - `0` â†’ mergi la **stÃ¢nga**  \n",
    "  - `1` â†’ mergi la **jos**  \n",
    "  - `2` â†’ mergi la **dreapta**  \n",
    "  - `3` â†’ mergi la **sus**\n",
    "\n",
    "- ğŸ† **RecompensÄƒ (`reward`)**  \n",
    "  - `+1` dacÄƒ agentul ajunge la È›intÄƒ `G`;  \n",
    "  - `0` Ã®n toate celelalte cazuri.  \n",
    "  Nu existÄƒ penalizare explicitÄƒ pentru cÄƒdere, dar episodul se Ã®ncheie instantaneu dacÄƒ agentul ajunge Ã®ntr-o gaurÄƒ `H`.\n",
    "\n",
    "- â±ï¸ **Durata maximÄƒ**  \n",
    "  100 de paÈ™i per episod (implicit).\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒ¨ï¸ ConfiguraÈ›ia hÄƒrÈ›ii\n",
    "\n",
    "Harta standard 4Ã—4 aratÄƒ astfel:\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "\n",
    "\n",
    "Legenda:\n",
    "- **S** â€“ poziÈ›ia de start  \n",
    "- **F** â€“ gheaÈ›Äƒ sigurÄƒ (*Frozen tile*)  \n",
    "- **H** â€“ gaurÄƒ (*Hole*)  \n",
    "- **G** â€“ È›intÄƒ (*Goal*)  \n",
    "\n",
    "---\n",
    "\n",
    "### â„ï¸ Mod â€alunecosâ€ (`is_slippery=True`)\n",
    "\n",
    "CÃ¢nd mediul este configurat cu `is_slippery=True`, acÈ›iunile au un caracter **stocastic**:  \n",
    "agentul poate aluneca Ã®n altÄƒ direcÈ›ie decÃ¢t cea intenÈ›ionatÄƒ, cu o probabilitate predefinitÄƒ (~33%).  \n",
    "\n",
    "Acest comportament transformÄƒ problema Ã®ntr-un **proces de decizie markovian stocastic**, unde tranziÈ›iile nu mai sunt deterministe.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“˜ DocumentaÈ›ie oficialÄƒ\n",
    "\n",
    "ğŸ”— [FrozenLake-v1 â€” Gymnasium Documentation](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Obiectiv:**  \n",
    "> Agentul trebuie sÄƒ Ã®nveÈ›e o politicÄƒ optimÄƒ care sÄƒ maximizeze È™ansa de a ajunge la È›intÄƒ `G`, evitÃ¢nd gÄƒurile `H`, chiar È™i atunci cÃ¢nd suprafaÈ›a este alunecoasÄƒ.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "\n",
    "# === IniÈ›ializare mediu ===\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "frames = []        \n",
    "states = [obs]      \n",
    "actions = []        \n",
    "rewards = []        \n",
    "\n",
    "# === Agent aleator ===\n",
    "for step in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    frames.append(env.render())\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    states.append(obs)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# === Rezumat episod ===\n",
    "steps = len(actions)\n",
    "total_reward = sum(rewards)\n",
    "success = rewards[-1] == 1.0 if rewards else False\n",
    "\n",
    "print(f\"ğŸ§Š Episod finalizat Ã®n {steps} paÈ™i.\")\n",
    "print(f\"ğŸ† RecompensÄƒ totalÄƒ: {total_reward:.2f}\")\n",
    "print(\"âœ… Agentul a ajuns la È›intÄƒ!\" if success else \"ğŸ’€ Agentul NU a ajuns la È›intÄƒ.\")\n",
    "\n",
    "# === Tabel cu tranziÈ›iile ===\n",
    "df = pd.DataFrame({\n",
    "    \"Step\": np.arange(1, steps + 1),\n",
    "    \"State\": states[:-1],\n",
    "    \"Action\": actions,\n",
    "    \"Next State\": states[1:],\n",
    "    \"Reward\": rewards\n",
    "})\n",
    "action_labels = {0: \"â¬…ï¸ Left\", 1: \"â¬‡ï¸ Down\", 2: \"â¡ï¸ Right\", 3: \"â¬†ï¸ Up\"}\n",
    "df[\"Action\"] = df[\"Action\"].map(action_labels)\n",
    "display(df.head(10))\n",
    "\n",
    "# === AnimaÈ›ie episod ===\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "im = ax.imshow(frames[0])\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"FrozenLake-v1 â€” Random Agent\")\n",
    "\n",
    "def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, update, frames=frames, interval=400, blit=True, repeat=False\n",
    ")\n",
    "\n",
    "plt.close(fig)\n",
    "display(HTML(ani.to_jshtml()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0aaa13",
   "metadata": {},
   "source": [
    "### ğŸ§© TODO-uri\n",
    "1. RulaÈ›i codul cu `is_slippery=False` È™i observaÈ›i diferenÈ›ele.\n",
    "2. RulaÈ›i 1000 de episoade È™i estimaÈ›i probabilitatea de succes a unui agent random.\n",
    "3. ÃnlocuiÈ›i agentul aleator cu unul care preferÄƒ acÈ›iunea `â¡ï¸` È™i observaÈ›i impactul.\n",
    "4. ModificaÈ›i harta folosind o specificaÈ›ie custom, conform codului de mai jos È™i Ã®ncercaÈ›i sÄƒ rulaÈ›i un episod, cu tot cu vizualizare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8827fef",
   "metadata": {},
   "source": [
    "### Custom map (exemplu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fddfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_desc = ['SFFF','FHFH','FFFH','HFFG']\n",
    "env = gym.make('FrozenLake-v1', desc=custom_desc, is_slippery=False, render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58830d95",
   "metadata": {},
   "source": [
    "## ğŸ² Seeding\n",
    "\n",
    "DouÄƒ rulÄƒri consecutive ale aceluiaÈ™i cod pot produce **rezultate diferite**, deoarece multe componente ale mediilor È™i algoritmilor folosesc **numere pseudo-aleatoare** (de exemplu: acÈ›iuni aleatorii, iniÈ›ializÄƒri de greutÄƒÈ›i, zgomot Gaussian, etc.).\n",
    "\n",
    "Pentru a controla acest comportament, folosim conceptul de **random seed** â€” o valoare numericÄƒ ce determinÄƒ punctul de plecare al generatorului de numere aleatoare.  \n",
    "Astfel, dacÄƒ folosim acelaÈ™i *seed*, experimentul va produce **exact aceleaÈ™i rezultate** la fiecare rulare.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Seeding Ã®n Gymnasium\n",
    "\n",
    "Ãn `gymnasium`, sursele de aleatorism sunt:\n",
    "- **Mediul (`env`)** â€” foloseÈ™te un generator intern pentru iniÈ›ializarea stÄƒrilor;  \n",
    "- **SpaÈ›iul de acÈ›iuni (`action_space`)** â€” foloseÈ™te un RNG separat pentru acÈ›iuni aleatorii;  \n",
    "- **Biblioteca NumPy** â€” poate fi folositÄƒ de agent sau de wrapper-e pentru decizii stocastice.\n",
    "\n",
    "Pentru a asigura consistenÈ›a totalÄƒ, trebuie sÄƒ setÄƒm seed-ul **Ã®n toate cele trei locuri**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Seed pentru mediu\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "# Seed pentru spaÈ›iul de acÈ›iuni\n",
    "env.action_space.seed(42)\n",
    "\n",
    "# Seed pentru NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Test seeding\n",
    "for i in range(10):\n",
    "    print(\"Primul sample:\", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dded88d",
   "metadata": {},
   "source": [
    "## ğŸ§° Wrappers â€” `TimeLimit`, `RecordEpisodeStatistics`, `TransformObservation`\n",
    "\n",
    "Ãn `gymnasium`, **wrappers** sunt obiecte care Ã®nconjoarÄƒ mediul original È™i Ã®i adaugÄƒ funcÈ›ionalitÄƒÈ›i suplimentare, fÄƒrÄƒ a-i modifica codul sursÄƒ.  \n",
    "Ele reprezintÄƒ o metodÄƒ elegantÄƒ de **extindere modularÄƒ** a mediilor RL, permiÈ›Ã¢nd logare, transformÄƒri, limitÄƒri È™i preprocesÄƒri automate.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Exemple de wrapper-e frecvente\n",
    "\n",
    "- **`TimeLimit(env, max_episode_steps)`**  \n",
    "  LimiteazÄƒ durata unui episod la un numÄƒr fix de paÈ™i.  \n",
    "  Este util pentru a preveni blocarea antrenamentelor atunci cÃ¢nd agentul nu atinge o stare terminalÄƒ.\n",
    "\n",
    "- **`RecordEpisodeStatistics(env)`**  \n",
    "  ÃnregistreazÄƒ automat informaÈ›ii despre fiecare episod â€” numÄƒrul de paÈ™i, recompensa totalÄƒ, timpii etc.  \n",
    "  Datele sunt accesibile ulterior prin `env.return_queue` sau `info`.\n",
    "\n",
    "- **`TransformObservation(env, func)`**  \n",
    "  AplicÄƒ o funcÈ›ie asupra observaÈ›iilor Ã®nainte de a fi returnate agentului (ex: normalizare, extragerea unei componente, conversie Ã®n tensori).  \n",
    "  Este folosit des pentru **preprocesarea datelor** Ã®nainte de inputul Ã®ntr-o reÈ›ea neuronalÄƒ.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§  De ce sunt importante\n",
    "\n",
    "Wrapperele permit:\n",
    "- controlul È™i instrumentarea mediilor fÄƒrÄƒ a le modifica direct;  \n",
    "- reutilizarea codului Ã®ntre experimente;  \n",
    "- logare È™i analizÄƒ mai simplÄƒ a performanÈ›ei agenÈ›ilor.\n",
    "\n",
    "> Practic, un wrapper este ca un â€filtruâ€ care se aplicÄƒ peste mediu â€” adÄƒugÃ¢nd o nouÄƒ funcÈ›ionalitate la fiecare pas sau reset.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium.wrappers import (\n",
    "    TimeLimit,\n",
    "    RecordEpisodeStatistics,\n",
    "    TransformObservation\n",
    ")\n",
    "\n",
    "# === CreÄƒm mediul de bazÄƒ ===\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# === AplicÄƒm wrapper-ele ===\n",
    "env = TimeLimit(env, max_episode_steps=300)  \n",
    "env = RecordEpisodeStatistics(env)           \n",
    "env = TransformObservation(env, lambda obs: obs / np.linalg.norm(obs))  \n",
    "\n",
    "# === ResetÄƒm mediul ===\n",
    "obs, info = env.reset(seed=42)\n",
    "print(\"ğŸ”¹ ObservaÈ›ie normalizatÄƒ (primii 4 parametri):\", np.round(obs, 4))\n",
    "\n",
    "# === Rulare scurtÄƒ cu agent aleator ===\n",
    "total_reward = 0\n",
    "for t in range(50):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "# === Rezumat episod ===\n",
    "print(f\"\\nğŸ“Š Rezumat episod:\")\n",
    "print(f\"   - PaÈ™i executaÈ›i: {t+1}\")\n",
    "print(f\"   - RecompensÄƒ totalÄƒ: {total_reward:.2f}\")\n",
    "if \"episode\" in info:\n",
    "    print(f\"   - Media recompenselor (din RecordEpisodeStatistics): {info['episode']['r'].mean():.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
