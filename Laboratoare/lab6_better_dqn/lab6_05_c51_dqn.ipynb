{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ae12fc",
   "metadata": {},
   "source": [
    "# C51 Distributional DQN\n",
    "\n",
    "**C51** este primul algoritm major de *distributional reinforcement learning*.  \n",
    "√én loc sƒÉ prezicƒÉ un singur scalar Q(s, a), C51 prezice **o distribu»õie √ÆntreagƒÉ** peste valorile posibile ale Q.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Ideea-cheie\n",
    "\n",
    "DQN clasic √Ænva»õƒÉ:\n",
    "\n",
    "$$\n",
    "Q(s,a)\n",
    "$$\n",
    "\n",
    "C51 √Ænva»õƒÉ:\n",
    "\n",
    "$$\n",
    "P(Z \\mid s,a)\n",
    "$$\n",
    "\n",
    "unde \\(Z\\) este o variabilƒÉ aleatoare ce reprezintƒÉ distribu»õia recompenselor viitoare.  \n",
    "Astfel, agentul are acces nu doar la valoare, ci »ôi la **incertitudine**, **risc**, **varia»õie** etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üéõ Suportul distribu»õiei\n",
    "\n",
    "C51 folose»ôte o distribu»õie discretizatƒÉ pe **51 de atomi** (de unde vine ‚ÄûC51‚Äù):\n",
    "\n",
    "$$\n",
    "z_i = v_{\\min} + i \\cdot \\Delta z,\n",
    "\\qquad i = 0,1,\\dots,50\n",
    "$$\n",
    "\n",
    "unde:\n",
    "\n",
    "$$\n",
    "\\Delta z = \\frac{v_{\\max} - v_{\\min}}{N_{\\text{atoms}} - 1}\n",
    "$$\n",
    "\n",
    "Re»õeaua prezice o distribu»õie softmax peste ace»ôti atomi pentru fiecare ac»õiune.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Projection Step (Bellman Projection)\n",
    "\n",
    "Predic»õia target trebuie ‚ÄûproiectatƒÉ‚Äù √Ænapoi pe suportul fix:\n",
    "\n",
    "$$\n",
    "Tz = r + \\gamma (1 - d) z\n",
    "$$\n",
    "\n",
    "Se calculeazƒÉ pozi»õia fiecƒÉrui \\(Tz_i\\) √Æntre atomi »ôi se redistribuie masa probabilisticƒÉ:\n",
    "\n",
    "- se determinƒÉ pozi»õiile frac»õionate:  \n",
    "  $$b = \\frac{Tz - v_{\\min}}{\\Delta z}$$\n",
    "- se proiecteazƒÉ √Ænapoi prin *interpolare liniarƒÉ* pe atomi vecini.\n",
    "\n",
    "Acesta este pasul esen»õial care face C51 diferit de DQN.\n",
    "\n",
    "---\n",
    "\n",
    "## üéÆ Alegerea ac»õiunii\n",
    "\n",
    "Q-value se ob»õine ca media distribu»õiei:\n",
    "\n",
    "$$\n",
    "Q(s,a) = \\sum_i p_i \\, z_i\n",
    "$$\n",
    "\n",
    "Apoi agentul alege ac»õiunea cu Q cel mai mare.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚≠ê Avantaje\n",
    "\n",
    "- surprinde **incertitudinea** »ôi **riscul**  \n",
    "- √ÆnvƒÉ»õare mai stabilƒÉ dec√¢t DQN clasic  \n",
    "- fundament pentru **Rainbow DQN**  \n",
    "- convergen»õƒÉ mai rapidƒÉ √Æn multe environment-uri  \n",
    "- precizie mai bunƒÉ √Æn probleme stocastice  \n",
    "\n",
    "---\n",
    "\n",
    "## üìå Rezumat\n",
    "\n",
    "**C51 = DQN + distribu»õie pe atomi + proiec»õie Bellman.**\n",
    "\n",
    "Este primul algoritm complet distributional, mult mai expresiv dec√¢t DQN clasic »ôi un ingredient esen»õial √Æn algoritmii moderni de RL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83196425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras import Model, layers, optimizers\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Replay Buffer\n",
    "# ============================================================\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, state_dim):\n",
    "        self.capacity = capacity\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "\n",
    "        self.states = np.zeros((capacity, state_dim), dtype=np.float32)\n",
    "        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.dones = np.zeros(capacity, dtype=np.float32)\n",
    "\n",
    "    def store(self, s, s2, a, r, d):\n",
    "        self.states[self.ptr] = s\n",
    "        self.next_states[self.ptr] = s2\n",
    "        self.actions[self.ptr] = a\n",
    "        self.rewards[self.ptr] = r\n",
    "        self.dones[self.ptr] = d\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        if self.ptr == 0:\n",
    "            self.full = True\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_size = self.capacity if self.full else self.ptr\n",
    "        idx = np.random.choice(max_size, batch_size, replace=False)\n",
    "        return (\n",
    "            self.states[idx],\n",
    "            self.next_states[idx],\n",
    "            self.actions[idx],\n",
    "            self.rewards[idx],\n",
    "            self.dones[idx]\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# C51 Network (Distributional Q-learning)\n",
    "# ============================================================\n",
    "\n",
    "class C51Network(Model):\n",
    "    def __init__(self, n_actions, n_atoms, v_min, v_max):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.n_atoms = n_atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "\n",
    "        # Support (z-values / atoms)\n",
    "        self.support = tf.linspace(v_min, v_max, n_atoms)\n",
    "        self.support = tf.cast(self.support, tf.float32) \n",
    "\n",
    "        # Architecture\n",
    "        self.fc1 = layers.Dense(128, activation='relu')\n",
    "        self.fc2 = layers.Dense(128, activation='relu')\n",
    "        self.logits = layers.Dense(n_actions * n_atoms)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        x = tf.nn.relu(self.fc1(x))\n",
    "        x = tf.nn.relu(self.fc2(x))\n",
    "\n",
    "        logits = self.logits(x)\n",
    "        logits = tf.reshape(logits, (-1, self.n_actions, self.n_atoms))\n",
    "\n",
    "        probs = tf.nn.softmax(logits, axis=2)\n",
    "        return probs\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state[np.newaxis, :]\n",
    "        dist = self.call(state)\n",
    "        q_values = tf.reduce_sum(dist * self.support, axis=2) \n",
    "        return int(tf.argmax(q_values[0]).numpy())\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "BATCH = 64\n",
    "MEM_SIZE = 50000\n",
    "NUM_EPISODES = 600\n",
    "\n",
    "# C51 atoms\n",
    "N_ATOMS = 51\n",
    "V_MIN = -10\n",
    "V_MAX = 10\n",
    "\n",
    "# Epsilon-greedy\n",
    "INITIAL_EPS = 1.0\n",
    "FINAL_EPS = 0.02\n",
    "EPS_DECAY = 0.995\n",
    "\n",
    "TAU = 0.005\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Soft update (Polyak averaging)\n",
    "# ============================================================\n",
    "\n",
    "def soft_update(target, online, tau=TAU):\n",
    "    tw = target.get_weights()\n",
    "    ow = online.get_weights()\n",
    "    new_w = [t * (1 - tau) + o * tau for t, o in zip(tw, ow)]\n",
    "    target.set_weights(new_w)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# C51 Projection Step (core of the algorithm)\n",
    "# ============================================================\n",
    "\n",
    "def project_distribution(next_dist, rewards, dones, gamma, support, v_min, v_max):\n",
    "    batch_size = rewards.shape[0]\n",
    "    n_atoms = support.shape[0]\n",
    "    delta_z = (v_max - v_min) / (n_atoms - 1)\n",
    "\n",
    "    projected = np.zeros((batch_size, n_atoms), dtype=np.float32)\n",
    "\n",
    "    # Bellman update on support\n",
    "    t_z = rewards[:, None] + gamma * (1 - dones[:, None]) * support[None, :]\n",
    "    t_z = np.clip(t_z, v_min, v_max)\n",
    "\n",
    "    # Positions inside support\n",
    "    b = (t_z - v_min) / delta_z\n",
    "    l = np.floor(b).astype(np.int32)\n",
    "    u = np.ceil(b).astype(np.int32)\n",
    "\n",
    "    next_dist = next_dist.numpy()\n",
    "\n",
    "    # Distribute probability mass\n",
    "    for i in range(batch_size):\n",
    "        for j in range(n_atoms):\n",
    "            lj = l[i, j]\n",
    "            uj = u[i, j]\n",
    "            p = next_dist[i, j]\n",
    "\n",
    "            if lj == uj:\n",
    "                projected[i, lj] += p\n",
    "            else:\n",
    "                projected[i, lj] += p * (uj - b[i, j])\n",
    "                projected[i, uj] += p * (b[i, j] - lj)\n",
    "\n",
    "    return projected\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training Loop ‚Äî C51 Distributional DQN\n",
    "# ============================================================\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "buffer = ReplayBuffer(MEM_SIZE, state_dim)\n",
    "online = C51Network(n_actions, N_ATOMS, V_MIN, V_MAX)\n",
    "target = C51Network(n_actions, N_ATOMS, V_MIN, V_MAX)\n",
    "target.set_weights(online.get_weights())\n",
    "\n",
    "optimizer = optimizers.Adam(LR)\n",
    "epsilon = INITIAL_EPS\n",
    "reward_history = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for step in range(200):\n",
    "\n",
    "        # Epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = online.act(state)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        ep_reward += reward\n",
    "\n",
    "        buffer.store(state, next_state, action, reward, float(done))\n",
    "        state = next_state\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # TRAINING\n",
    "        # ----------------------------------------------------\n",
    "        if buffer.ptr > 1000 or buffer.full:\n",
    "\n",
    "            states, next_states, actions, rewards, dones = buffer.sample(BATCH)\n",
    "\n",
    "            # Next distributions\n",
    "            next_dists = target(next_states)\n",
    "\n",
    "            # Double DQN ‚Äî choose action via ONLINE network\n",
    "            next_q = tf.reduce_sum(next_dists * online.support, axis=2)\n",
    "            next_actions = tf.argmax(next_q, axis=1, output_type=tf.int32)\n",
    "\n",
    "            # Extract chosen distributions\n",
    "            idx = tf.stack([tf.range(BATCH, dtype=tf.int32), next_actions], axis=1)\n",
    "            best_dists = tf.gather_nd(next_dists, idx)\n",
    "\n",
    "            # Projected target distribution\n",
    "            projected = project_distribution(\n",
    "                best_dists, rewards, dones,\n",
    "                GAMMA, online.support.numpy(), V_MIN, V_MAX\n",
    "            )\n",
    "\n",
    "            # Train\n",
    "            with tf.GradientTape() as tape:\n",
    "                dists = online(states)\n",
    "                a_idx = tf.stack([tf.range(BATCH, dtype=tf.int32), actions], axis=1)\n",
    "                chosen = tf.gather_nd(dists, a_idx)\n",
    "\n",
    "                loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    labels=projected,\n",
    "                    logits=tf.math.log(chosen + 1e-8)\n",
    "                )\n",
    "                loss = tf.reduce_mean(loss)\n",
    "\n",
    "            grads = tape.gradient(loss, online.trainable_variables)\n",
    "            grads = [tf.clip_by_norm(g, 5.0) for g in grads]\n",
    "            optimizer.apply_gradients(zip(grads, online.trainable_variables))\n",
    "\n",
    "            soft_update(target, online)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    epsilon = max(FINAL_EPS, epsilon * EPS_DECAY)\n",
    "    reward_history.append(ep_reward)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # LIVE PLOT\n",
    "    # ----------------------------------------------------\n",
    "    if episode % 5 == 0:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10,4))\n",
    "\n",
    "        # raw rewards\n",
    "        plt.plot(\n",
    "            reward_history,\n",
    "            label=\"Reward\",\n",
    "            color=\"blue\",\n",
    "            alpha=0.3,\n",
    "            linewidth=1\n",
    "        )\n",
    "\n",
    "        # moving average\n",
    "        if len(reward_history) > 20:\n",
    "            ma = np.convolve(reward_history, np.ones(20)/20, mode='valid')\n",
    "            plt.plot(\n",
    "                range(19, len(reward_history)),\n",
    "                ma,\n",
    "                label=\"Moving Avg (20 eps)\",\n",
    "                color=\"orange\",\n",
    "                linewidth=2.5\n",
    "            )\n",
    "\n",
    "        plt.title(\"C51 Distributional DQN ‚Äî Training Progress\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
