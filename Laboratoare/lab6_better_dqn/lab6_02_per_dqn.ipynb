{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b6e475",
   "metadata": {},
   "source": [
    "# Prioritized Experience Replay (PER)\n",
    "\n",
    "**Prioritized Experience Replay (PER)** Ã®mbunÄƒtÄƒÈ›eÈ™te replay buffer-ul clasic din DQN.  \n",
    "ÃŽn loc ca tranziÈ›iile sÄƒ fie selectate uniform, sampling-ul este ghidat de **prioritatea** fiecÄƒrei tranziÈ›ii â€” cele mai â€žimportanteâ€ sunt Ã®nvÄƒÈ›ate mai des.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ De ce avem nevoie de PER?\n",
    "\n",
    "Unele tranziÈ›ii din mediul RL sunt:\n",
    "\n",
    "- **mult mai utile** (reward mare, evenimente rare)\n",
    "- **mult mai informative** (TD-error mare â†’ agentul greÈ™eÈ™te mult acolo)\n",
    "\n",
    "Replay buffer-ul uniform trateazÄƒ toate tranziÈ›iile egal â†’ Ã®nvÄƒÈ›are mai lentÄƒ.  \n",
    "PER pune accent exact pe tranziÈ›iile cu impact mare.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Principiul PER\n",
    "\n",
    "FiecÄƒrei tranziÈ›ii i se ataÈ™eazÄƒ o prioritate:\n",
    "\n",
    "$$\n",
    "p_i = |TD\\_\\text{error, i}| + \\epsilon\n",
    "$$\n",
    "\n",
    "Apoi probabilitatea de sampling devine:\n",
    "\n",
    "$$\n",
    "P(i) = \\frac{p_i^\\alpha}{\\sum_j p_j^\\alpha}\n",
    "$$\n",
    "\n",
    "unde **Î±** controleazÄƒ cÃ¢t de puternicÄƒ este prioritizarea  \n",
    "(Î± = 0 â†’ sampling uniform, Î± = 1 â†’ sampling proporÈ›ional total).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Importance Sampling Weights (IS)\n",
    "\n",
    "Pentru a corecta bias-ul indus de sampling-ul neuniform, folosim:\n",
    "\n",
    "$$\n",
    "w_i = \\left( \\frac{1}{N \\cdot P(i)} \\right)^{\\beta}\n",
    "$$\n",
    "\n",
    "unde **Î²** creÈ™te treptat cÄƒtre 1 Ã®n timp.  \n",
    "Astfel, tranziÈ›iile rare sau foarte prioritizate sunt echilibrate corect Ã®n loss.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§± Cum funcÈ›ioneazÄƒ PER Ã®n practicÄƒ?\n",
    "\n",
    "1. TranziÈ›iile sunt salvate cu o prioritate iniÈ›ialÄƒ mare.  \n",
    "2. Sampling-ul se face Ã®mpÄƒrÈ›ind suma prioritÄƒÈ›ilor Ã®n segmente.  \n",
    "3. Se extrag eÈ™antioane proporÈ›ional cu prioritatea lor.  \n",
    "4. Se calculeazÄƒ È™i se returneazÄƒ:\n",
    "   - tranziÈ›iile  \n",
    "   - indicii pentru update  \n",
    "   - IS weights  \n",
    "5. PrioritÄƒÈ›ile sunt actualizate dupÄƒ fiecare batch pe baza TD-error-ului nou.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Avantajele PER\n",
    "\n",
    "- ÃŽnvaÈ›Äƒ **mult mai rapid** decÃ¢t sampling-ul uniform  \n",
    "- Agentul se concentreazÄƒ pe greÈ™elile cele mai importante  \n",
    "- Mai eficient Ã®n environment-uri complexe  \n",
    "- Este componentÄƒ oficialÄƒ din **Rainbow DQN**\n",
    "\n",
    "---\n",
    "\n",
    "## â­ Rezumat\n",
    "\n",
    "**PER = Replay Buffer + Prioritizare dupÄƒ TD-error + Importance Sampling**  \n",
    "â†’ agentul Ã®nvaÈ›Äƒ *mai inteligent*, nu aleator.\n",
    "\n",
    "Este una dintre cele mai importante Ã®mbunÄƒtÄƒÈ›iri aduse lui DQN È™i baza multor algoritmi moderni.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc80993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras import Model, layers, optimizers\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SumTree (used by PER)\n",
    "# ============================================================\n",
    "\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1, dtype=np.float32)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.write = 0\n",
    "\n",
    "    def add(self, priority, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, priority)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, priority):\n",
    "        change = priority - self.tree[idx]\n",
    "        self.tree[idx] = priority\n",
    "\n",
    "        # propagate upwards\n",
    "        while idx != 0:\n",
    "            idx = (idx - 1) // 2\n",
    "            self.tree[idx] += change\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = 0\n",
    "\n",
    "        while True:\n",
    "            left = 2 * idx + 1\n",
    "            right = left + 1\n",
    "\n",
    "            if left >= len(self.tree):\n",
    "                break\n",
    "\n",
    "            if s <= self.tree[left]:\n",
    "                idx = left\n",
    "            else:\n",
    "                s -= self.tree[left]\n",
    "                idx = right\n",
    "\n",
    "        data_idx = idx - self.capacity + 1\n",
    "        return idx, self.tree[idx], self.data[data_idx]\n",
    "\n",
    "    @property\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Prioritized Replay Buffer (PER)\n",
    "# ============================================================\n",
    "\n",
    "class PERBuffer:\n",
    "    def __init__(self, capacity, state_dim, alpha=0.6):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 1.0\n",
    "        self.size = 0\n",
    "\n",
    "        # vectorized storage\n",
    "        self.state_dim = state_dim\n",
    "\n",
    "    def store(self, state, next_state, action, reward, done):\n",
    "        transition = (state, next_state, action, reward, done)\n",
    "\n",
    "        # initial priority = max priority\n",
    "        priority = self.max_priority ** self.alpha\n",
    "        self.tree.add(priority, transition)\n",
    "\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        batch = []\n",
    "        priorities = []\n",
    "        idxs = []\n",
    "\n",
    "        segment = self.tree.total / batch_size\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            s = np.random.uniform(segment * i, segment * (i + 1))\n",
    "            idx, p, data = self.tree.get(s)\n",
    "            batch.append(data)\n",
    "            priorities.append(p)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        # unpack transitions\n",
    "        states, next_states, actions, rewards, dones = map(\n",
    "            np.array, zip(*batch)\n",
    "        )\n",
    "\n",
    "        # compute normalized importance sampling weights\n",
    "        probs = np.array(priorities) / self.tree.total\n",
    "        weights = (self.size * probs) ** (-beta)\n",
    "        weights = weights / weights.max()\n",
    "\n",
    "        return (\n",
    "            states.astype(np.float32),\n",
    "            next_states.astype(np.float32),\n",
    "            actions.astype(np.int32),\n",
    "            rewards.astype(np.float32),\n",
    "            dones.astype(np.float32),\n",
    "            np.array(idxs),\n",
    "            weights.astype(np.float32),\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, idxs, td_errors):\n",
    "        td_errors = np.abs(td_errors) + 1e-5\n",
    "\n",
    "        for idx, err in zip(idxs, td_errors):\n",
    "            priority = err ** self.alpha\n",
    "            self.tree.update(idx, priority)\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Dueling Network (same as before)\n",
    "# ============================================================\n",
    "\n",
    "class DuelingQNetwork(Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = layers.Dense(128, activation='relu')\n",
    "        self.fc2 = layers.Dense(128, activation='relu')\n",
    "\n",
    "        self.value_fc = layers.Dense(128, activation='relu')\n",
    "        self.adv_fc = layers.Dense(128, activation='relu')\n",
    "\n",
    "        self.value = layers.Dense(1)\n",
    "        self.advantage = layers.Dense(n_actions)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        V = self.value(self.value_fc(x))\n",
    "        A = self.advantage(self.adv_fc(x))\n",
    "\n",
    "        A_mean = tf.reduce_mean(A, axis=1, keepdims=True)\n",
    "        Q = V + (A - A_mean)\n",
    "\n",
    "        return tf.clip_by_value(Q, -500, 500)\n",
    "\n",
    "    def act(self, obs):\n",
    "        obs = obs[np.newaxis, :].astype(np.float32)\n",
    "        Q = self.call(obs)[0]\n",
    "        return int(tf.argmax(Q).numpy())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "GAMMA = 0.99\n",
    "LR = 3e-4\n",
    "BATCH = 32\n",
    "MEM_SIZE = 50000\n",
    "NUM_EPISODES = 600\n",
    "\n",
    "INITIAL_EPS = 1.0\n",
    "FINAL_EPS = 0.02\n",
    "EPS_DECAY = 0.995\n",
    "\n",
    "TAU = 0.005\n",
    "BETA_START = 0.4\n",
    "BETA_INCREMENT = 1e-4\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Soft Update\n",
    "# ============================================================\n",
    "\n",
    "def soft_update(target, online, tau=TAU):\n",
    "    target_w = target.get_weights()\n",
    "    online_w = online.get_weights()\n",
    "    new_w = []\n",
    "\n",
    "    for tw, ow in zip(target_w, online_w):\n",
    "        new_w.append(tw * (1 - tau) + ow * tau)\n",
    "\n",
    "    target.set_weights(new_w)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training Loop (PER integrated)\n",
    "# ============================================================\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "buffer = PERBuffer(MEM_SIZE, state_dim)\n",
    "online = DuelingQNetwork(n_actions)\n",
    "target = DuelingQNetwork(n_actions)\n",
    "target.set_weights(online.get_weights())\n",
    "\n",
    "optimizer = optimizers.Adam(LR)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "epsilon = INITIAL_EPS\n",
    "beta = BETA_START\n",
    "reward_history = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for step in range(200):\n",
    "\n",
    "        # ----------------------------\n",
    "        # Epsilon-greedy\n",
    "        # ----------------------------\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = online.act(state)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        ep_reward += reward\n",
    "\n",
    "        buffer.store(state, next_state, action, reward, float(done))\n",
    "        state = next_state\n",
    "\n",
    "        # ----------------------------\n",
    "        # TRAINING\n",
    "        # ----------------------------\n",
    "        if buffer.size > 1000:\n",
    "\n",
    "            states, next_states, actions, rewards, dones, idxs, weights = \\\n",
    "                buffer.sample(BATCH, beta)\n",
    "\n",
    "            # Double DQN\n",
    "            next_actions = tf.argmax(\n",
    "                online(next_states), axis=1, output_type=tf.int32\n",
    "            )\n",
    "            indices = tf.stack(\n",
    "                [tf.range(BATCH, dtype=tf.int32), next_actions], axis=1\n",
    "            )\n",
    "\n",
    "            next_Q = tf.gather_nd(target(next_states), indices)\n",
    "            y = rewards + GAMMA * (1 - dones) * next_Q\n",
    "\n",
    "            # ----------------------------\n",
    "            # Compute loss (with weights)\n",
    "            # ----------------------------\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_pred = online(states)\n",
    "                a_idx = tf.stack(\n",
    "                    [tf.range(BATCH, dtype=tf.int32), actions], axis=1\n",
    "                )\n",
    "                pred = tf.gather_nd(q_pred, a_idx)\n",
    "\n",
    "                td_errors = y - pred\n",
    "                loss = tf.reduce_mean(weights * mse(pred, y))\n",
    "\n",
    "            grads = tape.gradient(loss, online.trainable_variables)\n",
    "            grads = [tf.clip_by_norm(g, 5.0) for g in grads]\n",
    "            optimizer.apply_gradients(zip(grads, online.trainable_variables))\n",
    "\n",
    "            # update priorities\n",
    "            buffer.update_priorities(idxs, td_errors.numpy())\n",
    "\n",
    "            # soft update\n",
    "            soft_update(target, online)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # decay epsilon & beta\n",
    "    epsilon = max(FINAL_EPS, epsilon * EPS_DECAY)\n",
    "    beta = min(1.0, beta + BETA_INCREMENT)\n",
    "\n",
    "    reward_history.append(ep_reward)\n",
    "\n",
    "    # ----------------------------\n",
    "    # LIVE PLOT\n",
    "    # ----------------------------\n",
    "    if episode % 5 == 0:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10,4))\n",
    "\n",
    "        # Reward\n",
    "        plt.plot(\n",
    "            reward_history,\n",
    "            label=\"Reward\",\n",
    "            color=\"blue\",\n",
    "            alpha=0.3,   \n",
    "            linewidth=1\n",
    "        )\n",
    "\n",
    "        # Moving average\n",
    "        if len(reward_history) > 20:\n",
    "            ma = np.convolve(reward_history, np.ones(20)/20, mode='valid')\n",
    "            plt.plot(\n",
    "                range(19, len(reward_history)),\n",
    "                ma,\n",
    "                label=\"Moving Avg (20 eps)\",\n",
    "                color=\"orange\",\n",
    "                linewidth=2.5\n",
    "            )\n",
    "\n",
    "        plt.title(\"PER DQN â€” Training Progress\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
