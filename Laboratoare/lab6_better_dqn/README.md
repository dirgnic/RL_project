# ğŸ“˜ Laborator 6 â€” Advanced Value-Based RL (Deep Q-Learning Extensions)

Ãn acest laborator explorÄƒm cele mai importante extensii moderne ale algoritmului DQN.  
Scopul este sÄƒ Ã®nÈ›elegem cum putem construi agenÈ›i **mai stabili, mai rapizi È™i mai inteligenÈ›i** prin Ã®mbunÄƒtÄƒÈ›iri aduse modului de Ã®nvÄƒÈ›are, explorare È™i estimare a valorilor.

---

## ğŸ¯ Obiective

- SÄƒ implementÄƒm È™i sÄƒ comparÄƒm cele mai populare variante avansate ale DQN.  
- SÄƒ Ã®nÈ›elegem rolul fiecÄƒrei componente Ã®n stabilitatea È™i performanÈ›a agentului.  
- SÄƒ folosim TensorFlow/Keras pentru a construi reÈ›ele neurale moderne folosite Ã®n RL.

---

## ğŸ§© Algoritmii studiaÈ›i

### **1. Dueling DQN**  
SeparÄƒ estimarea valorii stÄƒrii de estimarea avantajului acÈ›iunilor â†’ Ã®nvaÈ›Äƒ mai eficient Ã®n stÄƒri â€ne-informativeâ€.

### **2. Prioritized Experience Replay (PER)**  
TranziÈ›iile importante (cu TD-error mare) sunt eÈ™antionate mai des â†’ convergenÈ›Äƒ mai rapidÄƒ.

### **3. Noisy DQN**  
ÃnlocuieÈ™te epsilon-greedy cu explorare prin zgomot Ã®n parametrii reÈ›elei â†’ explorare Ã®nvÄƒÈ›abilÄƒ.

### **4. Multi-Step DQN (N-step returns)**  
PropagÄƒ reward-ul mai repede, combinÃ¢nd beneficii de la TD È™i Monte Carlo.

### **5. C51 Distributional DQN**  
ÃnvaÈ›Äƒ distribuÈ›ia completÄƒ a valorilor viitoare, nu doar media â†’ agent mai robust.

### **6. Rainbow DQN**  
CombinÄƒ toate tehnicile anterioare:
- Double DQN  
- Dueling  
- PER  
- Multi-step  
- Noisy Nets  
- C51  

â†’ unul dintre cei mai puternici algoritmi value-based.

---