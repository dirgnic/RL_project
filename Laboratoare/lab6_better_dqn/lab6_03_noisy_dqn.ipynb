{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2795b36",
   "metadata": {},
   "source": [
    "# Noisy DQN â€” Explorare prin Zgomot Parametric\n",
    "\n",
    "**Noisy DQN** Ã®nlocuieÈ™te mecanismul clasic de explorare *Îµ-greedy* cu un mod\n",
    "mult mai stabil È™i inteligent: **explorarea prin zgomot injectat direct Ã®n greutÄƒÈ›ile reÈ›elei**.\n",
    "ÃŽn loc sÄƒ aleagÄƒ uneori acÈ›iuni random, reÈ›eaua genereazÄƒ Q-values uÈ™or diferite\n",
    "datoritÄƒ zgomotului din parametri.\n",
    "\n",
    "Rezultat: agentul exploreazÄƒ natural, fÄƒrÄƒ epsilon È™i fÄƒrÄƒ tuning manual.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ De ce Noisy Networks?\n",
    "\n",
    "LimitÄƒrile Îµ-greedy:\n",
    "- explorare nenaturalÄƒ (salturi bruÈ™te Ã®ntre acÈ›iuni)\n",
    "- necesitÄƒ programare manualÄƒ a epsilon-ului\n",
    "- performanÈ›Äƒ slabÄƒ Ã®n probleme complexe\n",
    "\n",
    "Avantajele Noisy Nets:\n",
    "- explorare **dependentÄƒ de stare**\n",
    "- magnitudinea zgomotului este **Ã®nvÄƒÈ›atÄƒ automat**\n",
    "- agentul decide singur cÃ¢t exploreazÄƒ sau exploateazÄƒ\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Noisy Linear Layer (NoisyDense)\n",
    "\n",
    "Un layer obiÈ™nuit calculeazÄƒ:\n",
    "\n",
    "$$\n",
    "y = Wx + b\n",
    "$$\n",
    "\n",
    "Un layer *noisy* foloseÈ™te parametri perturbaÈ›i:\n",
    "\n",
    "$$\n",
    "y = (\\mu_W + \\sigma_W \\odot \\epsilon_W)\\, x \\;+\\; \n",
    "    (\\mu_b + \\sigma_b \\odot \\epsilon_b)\n",
    "$$\n",
    "\n",
    "unde:\n",
    "- **Î¼** â€” componenta deterministicÄƒ  \n",
    "- **Ïƒ** â€” mÄƒrimea zgomotului (Ã®nvÄƒÈ›abilÄƒ)  \n",
    "- **Îµ** â€” zgomot Gaussian factorized  \n",
    "- zgomotul se regenereazÄƒ la fiecare forward pass\n",
    "\n",
    "Acest zgomot **factorizat** este stabil È™i eficient â€” folosit Ã®n Rainbow DQN.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ Mecanismul de explorare\n",
    "\n",
    "Zgomotul din greutÄƒÈ›i face ca Q-values sÄƒ varieze la fiecare apel:\n",
    "\n",
    "- nu mai avem *Îµ-random actions*\n",
    "- explorarea rezultÄƒ din modificarea naturalÄƒ a Q-values\n",
    "- politica devine stocasticÄƒ prin Ã®nsÄƒÈ™i reÈ›eaua neuronalÄƒ\n",
    "\n",
    "Orice apel la `model(x)` include implicit explorare.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Compatibilitate cu alte Ã®mbunÄƒtÄƒÈ›iri\n",
    "\n",
    "Noisy DQN se combinÄƒ foarte bine cu:\n",
    "- **Double DQN**\n",
    "- **Dueling Networks**\n",
    "- **PER**\n",
    "- **Multi-Step Learning**\n",
    "- **Rainbow DQN**\n",
    "\n",
    "ÃŽn laboratorul tÄƒu, versiunea implementatÄƒ este **Dueling + Noisy + Double**.\n",
    "\n",
    "---\n",
    "\n",
    "## â­ Avantaje faÈ›Äƒ de Îµ-greedy\n",
    "\n",
    "- explorare inteligentÄƒ È™i adaptivÄƒ  \n",
    "- nu necesitÄƒ decay pentru epsilon  \n",
    "- mai stabil Ã®n environment-uri cu acÈ›iuni multiple  \n",
    "- componentÄƒ esenÈ›ialÄƒ Ã®n **Rainbow**  \n",
    "- permite un comportament explorator â€žcontinuumâ€, nu brusc\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Rezumat\n",
    "\n",
    "**Noisy DQN = DQN + explorare Ã®nvÄƒÈ›abilÄƒ direct Ã®n greutÄƒÈ›ile reÈ›elei.**  \n",
    "Agentul devine explorator prin arhitecturÄƒ, fÄƒrÄƒ zgomot extern sau reguli artificiale.\n",
    "\n",
    "Este una dintre cele mai elegante È™i eficiente metode moderne de explorare Ã®n RL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ebb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras import Model, layers, optimizers\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Noisy Linear Layer (Factorized Gaussian Noise)\n",
    "# ============================================================\n",
    "\n",
    "class NoisyDense(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n_in = int(input_shape[-1])\n",
    "\n",
    "        # Trainable parameters\n",
    "        self.mu_w = self.add_weight(\n",
    "            name=\"mu_w\",\n",
    "            shape=(n_in, self.units),\n",
    "            initializer=tf.random_uniform_initializer(-1/np.sqrt(n_in), 1/np.sqrt(n_in)),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.sigma_w = self.add_weight(\n",
    "            name=\"sigma_w\",\n",
    "            shape=(n_in, self.units),\n",
    "            initializer=tf.constant_initializer(0.5 / np.sqrt(n_in)),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.mu_b = self.add_weight(\n",
    "            name=\"mu_b\",\n",
    "            shape=(self.units,),\n",
    "            initializer=tf.random_uniform_initializer(-1/np.sqrt(n_in), 1/np.sqrt(n_in)),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.sigma_b = self.add_weight(\n",
    "            name=\"sigma_b\",\n",
    "            shape=(self.units,),\n",
    "            initializer=tf.constant_initializer(0.5 / np.sqrt(n_in)),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # Factorized Gaussian noise\n",
    "        eps_in = tf.random.normal((self.mu_w.shape[0],))\n",
    "        eps_out = tf.random.normal((self.units,))\n",
    "\n",
    "        f_in = tf.sign(eps_in) * tf.sqrt(tf.abs(eps_in))\n",
    "        f_out = tf.sign(eps_out) * tf.sqrt(tf.abs(eps_out))\n",
    "\n",
    "        # Outer product f_in âŠ— f_out â†’ shape (n_in, units)\n",
    "        w_noise = tf.tensordot(f_in, f_out, axes=0)\n",
    "        b_noise = f_out\n",
    "\n",
    "        w = self.mu_w + self.sigma_w * w_noise\n",
    "        b = self.mu_b + self.sigma_b * b_noise\n",
    "\n",
    "        return tf.matmul(x, w) + b\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Replay Buffer (same structure as before)\n",
    "# ============================================================\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size, state_dim):\n",
    "        self.size = size\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "\n",
    "        self.states = np.zeros((size, state_dim), dtype=np.float32)\n",
    "        self.next_states = np.zeros((size, state_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros(size, dtype=np.int32)\n",
    "        self.rewards = np.zeros(size, dtype=np.float32)\n",
    "        self.dones = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "    def store(self, s, s2, a, r, d):\n",
    "        self.states[self.ptr] = s\n",
    "        self.next_states[self.ptr] = s2\n",
    "        self.actions[self.ptr] = a\n",
    "        self.rewards[self.ptr] = r\n",
    "        self.dones[self.ptr] = d\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.size\n",
    "        if self.ptr == 0:\n",
    "            self.full = True\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_size = self.size if self.full else self.ptr\n",
    "        idx = np.random.choice(max_size, batch_size, replace=False)\n",
    "        return (\n",
    "            self.states[idx],\n",
    "            self.next_states[idx],\n",
    "            self.actions[idx],\n",
    "            self.rewards[idx],\n",
    "            self.dones[idx],\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Noisy DQN Network\n",
    "# ============================================================\n",
    "\n",
    "class NoisyDQN(Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.fc1 = NoisyDense(128)\n",
    "        self.fc2 = NoisyDense(128)\n",
    "        self.fc3 = NoisyDense(n_actions)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        x = tf.nn.relu(self.fc1(x))\n",
    "        x = tf.nn.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state[np.newaxis, :]\n",
    "        Q = self.call(state)[0]\n",
    "        return int(tf.argmax(Q).numpy())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "GAMMA = 0.99\n",
    "LR = 3e-4\n",
    "BATCH = 32\n",
    "MEM_SIZE = 50000\n",
    "NUM_EPISODES = 600\n",
    "\n",
    "# No epsilon decay â€” Noisy Nets handle exploration automatically\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training Loop â€” Noisy DQN\n",
    "# ============================================================\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "buffer = ReplayBuffer(MEM_SIZE, state_dim)\n",
    "online = NoisyDQN(n_actions)\n",
    "target = NoisyDQN(n_actions)\n",
    "target.set_weights(online.get_weights())\n",
    "\n",
    "optimizer = optimizers.Adam(LR)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "reward_history = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for step in range(200):\n",
    "\n",
    "        # ----------------------------\n",
    "        # Noisy exploration (no epsilon)\n",
    "        # ----------------------------\n",
    "        action = online.act(state)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        ep_reward += reward\n",
    "\n",
    "        buffer.store(state, next_state, action, reward, float(done))\n",
    "        state = next_state\n",
    "\n",
    "        # ----------------------------\n",
    "        # TRAINING\n",
    "        # ----------------------------\n",
    "        if buffer.ptr > 1000 or buffer.full:\n",
    "\n",
    "            states, next_states, actions, rewards, dones = buffer.sample(BATCH)\n",
    "\n",
    "            next_actions = tf.argmax(online(next_states), axis=1, output_type=tf.int32)\n",
    "            idx = tf.stack([tf.range(BATCH, dtype=tf.int32), next_actions], axis=1)\n",
    "\n",
    "            target_Q_all = target(next_states)\n",
    "            next_Q = tf.gather_nd(target_Q_all, idx)\n",
    "\n",
    "            y = rewards + GAMMA * (1 - dones) * next_Q\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_pred = online(states)\n",
    "                a_idx = tf.stack(\n",
    "                    [tf.range(BATCH, dtype=tf.int32), actions], axis=1\n",
    "                )\n",
    "                pred = tf.gather_nd(q_pred, a_idx)\n",
    "                loss = mse(pred, y)\n",
    "\n",
    "            grads = tape.gradient(loss, online.trainable_variables)\n",
    "            grads = [tf.clip_by_norm(g, 5.0) for g in grads]\n",
    "            optimizer.apply_gradients(zip(grads, online.trainable_variables))\n",
    "\n",
    "            # Soft update\n",
    "            target_weights = target.get_weights()\n",
    "            online_weights = online.get_weights()\n",
    "            tau = 0.005\n",
    "            new_weights = [\n",
    "                tw * (1 - tau) + ow * tau\n",
    "                for tw, ow in zip(target_weights, online_weights)\n",
    "            ]\n",
    "            target.set_weights(new_weights)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    reward_history.append(ep_reward)\n",
    "\n",
    "    # ----------------------------\n",
    "    # LIVE PLOT\n",
    "    # ----------------------------\n",
    "    if episode % 5 == 0:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10,4))\n",
    "\n",
    "        # Reward\n",
    "        plt.plot(\n",
    "            reward_history,\n",
    "            label=\"Reward\",\n",
    "            color=\"blue\",\n",
    "            alpha=0.3,  \n",
    "            linewidth=1\n",
    "        )\n",
    "\n",
    "        # Moving average\n",
    "        if len(reward_history) > 20:\n",
    "            ma = np.convolve(reward_history, np.ones(20)/10, mode='valid')\n",
    "            plt.plot(\n",
    "                range(19, len(reward_history)),\n",
    "                ma,\n",
    "                label=\"Moving Avg (20 eps)\",\n",
    "                color=\"orange\",\n",
    "                linewidth=2.5\n",
    "            )\n",
    "\n",
    "        plt.title(\"Noisy DQN â€” Training Progress\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
