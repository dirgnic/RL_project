{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85715a4d",
   "metadata": {},
   "source": [
    "# üåà Rainbow DQN ‚Äî Rezumat\n",
    "\n",
    "**Rainbow DQN** combinƒÉ cele mai eficiente 6 √ÆmbunƒÉtƒÉ»õiri ale lui DQN √Æntr-un singur algoritm stabil »ôi performant.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Componentele Rainbow\n",
    "\n",
    "### **1. Double DQN**\n",
    "Reduce supraestimarea Q-values separ√¢nd selec»õia ac»õiunii de evaluare:\n",
    "\n",
    "$$\n",
    "y = r + \\gamma \\, Q_{\\text{target}}\\big(s', \\arg\\max_a Q_{\\text{online}}(s',a)\\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Dueling Network Architecture**\n",
    "Descompune estimarea Q √Æn:\n",
    "\n",
    "$$\n",
    "Q(s,a) = V(s) + A(s,a) - \\frac{1}{|\\mathcal{A}|}\\sum_{a'} A(s,a')\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Prioritized Experience Replay (PER)**\n",
    "SelecteazƒÉ mai des tranzi»õiile cu TD-error mare:\n",
    "\n",
    "$$\n",
    "P(i) = \\frac{p_i^\\alpha}{\\sum_j p_j^\\alpha}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Multi-Step Returns**\n",
    "\n",
    "$$\n",
    "R^{(n)} = \n",
    "\\sum_{k=0}^{n-1} \\gamma^k r_k\n",
    "\\;+\\;\n",
    "\\gamma^n V(s_n)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Noisy Networks**\n",
    "Explorare √ÆnvƒÉ»õabilƒÉ prin zgomot parametric:\n",
    "\n",
    "$$\n",
    "y = (\\mu_W + \\sigma_W \\odot \\epsilon_W)\\, x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **6. C51 Distributional RL**\n",
    "ModeleazƒÉ distribu»õia √ÆntreagƒÉ a valorii viitoare:\n",
    "\n",
    "- 51 atomi \\((z_i)\\)\n",
    "- Softmax ‚Üí probabilitƒÉ»õi\n",
    "- Proiec»õie distribuitƒÉ pe suport\n",
    "\n",
    "---\n",
    "\n",
    "## ‚≠ê Pe scurt\n",
    "\n",
    "**Rainbow = Double DQN + Dueling + PER + Multi-Step + Noisy Nets + C51**\n",
    "\n",
    "‚Üí una dintre cele mai robuste »ôi avansate metode value-based.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras import Model, layers, optimizers\n",
    "\n",
    "# ============================================================\n",
    "# Prioritized Multi-Step Replay Buffer (PER + N-step)\n",
    "# ============================================================\n",
    "\n",
    "class PrioritizedNStepBuffer:\n",
    "    def __init__(self, capacity, state_dim, n_step=3, gamma=0.99, alpha=0.6, beta=0.4):\n",
    "        self.capacity = capacity\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "\n",
    "        self.states = np.zeros((capacity, state_dim), dtype=np.float32)\n",
    "        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.dones = np.zeros(capacity, dtype=np.float32)\n",
    "        self.priorities = np.ones(capacity, dtype=np.float32)\n",
    "\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "        self.buffer = collections.deque(maxlen=n_step)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def store(self, s, s2, a, r, done):\n",
    "        self.buffer.append((s, s2, a, r, done))\n",
    "        if len(self.buffer) < self.n_step:\n",
    "            return\n",
    "\n",
    "        R = 0\n",
    "        discount = 1\n",
    "        for (_, _, _, r_i, d_i) in self.buffer:\n",
    "            R += r_i * discount\n",
    "            discount *= self.gamma\n",
    "            if d_i:\n",
    "                break\n",
    "\n",
    "        s0, _, a0, _, _ = self.buffer[0]\n",
    "        _, s_n, _, _, d_n = self.buffer[-1]\n",
    "\n",
    "        idx = self.ptr\n",
    "        self.states[idx] = s0\n",
    "        self.next_states[idx] = s_n\n",
    "        self.actions[idx] = a0\n",
    "        self.rewards[idx] = R\n",
    "        self.dones[idx] = float(d_n)\n",
    "        self.priorities[idx] = self.priorities.max()\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        if self.ptr == 0: self.full = True\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_mem = self.capacity if self.full else self.ptr\n",
    "        probs = self.priorities[:max_mem] ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        idx = np.random.choice(max_mem, batch_size, p=probs)\n",
    "        weights = (max_mem * probs[idx]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        return (\n",
    "            self.states[idx], self.next_states[idx], self.actions[idx],\n",
    "            self.rewards[idx], self.dones[idx], idx, weights.astype(np.float32)\n",
    "        )\n",
    "\n",
    "    def update_priorities(self, indexes, p):\n",
    "        self.priorities[indexes] = p\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Noisy Dense Layer (Factorized Noisy Nets)\n",
    "# ============================================================\n",
    "\n",
    "class NoisyDense(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        in_dim = int(input_shape[-1])\n",
    "\n",
    "        self.mu_w = self.add_weight(shape=(in_dim, self.units), initializer=\"glorot_uniform\")\n",
    "        self.sigma_w = self.add_weight(shape=(in_dim, self.units), initializer=tf.constant_initializer(0.017))\n",
    "\n",
    "        self.mu_b = self.add_weight(shape=(self.units,), initializer=\"zeros\")\n",
    "        self.sigma_b = self.add_weight(shape=(self.units,), initializer=tf.constant_initializer(0.017))\n",
    "\n",
    "    def call(self, x):\n",
    "        eps_in = tf.random.normal((x.shape[-1],))\n",
    "        eps_out = tf.random.normal((self.units,))\n",
    "\n",
    "        f_in = tf.sign(eps_in) * tf.sqrt(tf.abs(eps_in))\n",
    "        f_out = tf.sign(eps_out) * tf.sqrt(tf.abs(eps_out))\n",
    "\n",
    "        w_noise = tf.expand_dims(f_in, -1) * tf.expand_dims(f_out, 0)\n",
    "        b_noise = f_out\n",
    "\n",
    "        w = self.mu_w + self.sigma_w * w_noise\n",
    "        b = self.mu_b + self.sigma_b * b_noise\n",
    "\n",
    "        return tf.matmul(x, w) + b\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Rainbow Network (Noisy + Dueling + C51)\n",
    "# ============================================================\n",
    "\n",
    "class RainbowNetwork(Model):\n",
    "    def __init__(self, n_actions, n_atoms=51, v_min=-10, v_max=10):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.n_atoms = n_atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "\n",
    "        self.support = tf.cast(tf.linspace(v_min, v_max, n_atoms), tf.float32)\n",
    "\n",
    "        self.fc1 = NoisyDense(128)\n",
    "        self.fc2 = NoisyDense(128)\n",
    "\n",
    "        self.val1 = NoisyDense(128)\n",
    "        self.val2 = NoisyDense(n_atoms)\n",
    "\n",
    "        self.adv1 = NoisyDense(128)\n",
    "        self.adv2 = NoisyDense(n_actions * n_atoms)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "\n",
    "        x = tf.nn.relu(self.fc1(x))\n",
    "        x = tf.nn.relu(self.fc2(x))\n",
    "\n",
    "        V = tf.nn.relu(self.val1(x))\n",
    "        V = tf.reshape(self.val2(V), (-1, 1, self.n_atoms))\n",
    "\n",
    "        A = tf.nn.relu(self.adv1(x))\n",
    "        A = tf.reshape(self.adv2(A), (-1, self.n_actions, self.n_atoms))\n",
    "\n",
    "        A_mean = tf.reduce_mean(A, axis=1, keepdims=True)\n",
    "        logits = V + (A - A_mean)\n",
    "\n",
    "        dist = tf.nn.softmax(logits, axis=2)\n",
    "        return dist\n",
    "\n",
    "    def act(self, state):\n",
    "        dist = self(state[np.newaxis, :])\n",
    "        q = tf.reduce_sum(dist * self.support, axis=2)\n",
    "        return int(tf.argmax(q[0]).numpy())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# C51 Projection\n",
    "# ============================================================\n",
    "\n",
    "def c51_projection(next_dist, rewards, dones, gamma, support, v_min, v_max):\n",
    "    batch = rewards.shape[0]\n",
    "    n_atoms = support.shape[0]\n",
    "    delta = (v_max - v_min) / (n_atoms - 1)\n",
    "\n",
    "    proj = np.zeros((batch, n_atoms), dtype=np.float32)\n",
    "\n",
    "    tz = rewards[:, None] + gamma * (1 - dones[:, None]) * support[None, :]\n",
    "    tz = np.clip(tz, v_min, v_max)\n",
    "\n",
    "    b = (tz - v_min) / delta\n",
    "    l = np.floor(b).astype(np.int32)\n",
    "    u = np.ceil(b).astype(np.int32)\n",
    "\n",
    "    next_dist = next_dist.numpy()\n",
    "\n",
    "    for i in range(batch):\n",
    "        for j in range(n_atoms):\n",
    "            p = next_dist[i, j]\n",
    "            lj, uj = l[i, j], u[i, j]\n",
    "            if lj == uj:\n",
    "                proj[i, lj] += p\n",
    "            else:\n",
    "                proj[i, lj] += p * (uj - b[i, j])\n",
    "                proj[i, uj] += p * (b[i, j] - lj)\n",
    "\n",
    "    return proj\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "BATCH = 32\n",
    "MEMORY = 50000\n",
    "N_STEP = 3\n",
    "NUM_EPISODES = 600\n",
    "\n",
    "N_ATOMS = 51\n",
    "V_MIN, V_MAX = -10, 10\n",
    "TAU = 0.005\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Soft Update\n",
    "# ============================================================\n",
    "\n",
    "def soft_update(target, online, tau=TAU):\n",
    "    tw = target.get_weights()\n",
    "    ow = online.get_weights()\n",
    "    target.set_weights([t * (1 - tau) + o * tau for t, o in zip(tw, ow)])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training Loop ‚Äî FULL RAINBOW\n",
    "# ============================================================\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "buffer = PrioritizedNStepBuffer(MEMORY, state_dim, n_step=N_STEP, gamma=GAMMA)\n",
    "online = RainbowNetwork(n_actions)\n",
    "target = RainbowNetwork(n_actions)\n",
    "target.set_weights(online.get_weights())\n",
    "\n",
    "optimizer = optimizers.Adam(LR)\n",
    "reward_history = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for step in range(200):\n",
    "        action = online.act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        ep_reward += reward\n",
    "\n",
    "        buffer.store(state, next_state, action, reward, float(done))\n",
    "        state = next_state\n",
    "\n",
    "        # TRAIN\n",
    "        if buffer.ptr > 1000 or buffer.full:\n",
    "            s, s2, a, r, d, idx, weights = buffer.sample(BATCH)\n",
    "\n",
    "            next_dist = target(s2)\n",
    "            next_q = tf.reduce_sum(next_dist * online.support, axis=2)\n",
    "            next_actions = tf.argmax(next_q, axis=1, output_type=tf.int32)\n",
    "\n",
    "            next_sel = tf.gather_nd(next_dist,\n",
    "                                    tf.stack([tf.range(BATCH), next_actions], axis=1))\n",
    "\n",
    "            proj = c51_projection(next_sel, r, d,\n",
    "                                  GAMMA, online.support.numpy(), V_MIN, V_MAX)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                dist = online(s)\n",
    "                chosen = tf.gather_nd(dist,\n",
    "                                      tf.stack([tf.range(BATCH), a], axis=1))\n",
    "\n",
    "                loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    labels=proj,\n",
    "                    logits=tf.math.log(chosen + 1e-8)\n",
    "                )\n",
    "                loss = tf.reduce_mean(weights * loss)\n",
    "\n",
    "            grads = tape.gradient(loss, online.trainable_variables)\n",
    "            grads = [tf.clip_by_norm(g, 5.0) for g in grads]\n",
    "            optimizer.apply_gradients(zip(grads, online.trainable_variables))\n",
    "\n",
    "            buffer.update_priorities(idx, loss.numpy() + 1e-6)\n",
    "            soft_update(target, online)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    reward_history.append(ep_reward)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # LIVE PLOT\n",
    "    # ----------------------------------------------------\n",
    "    if episode % 5 == 0:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10,4))\n",
    "\n",
    "        # raw rewards\n",
    "        plt.plot(\n",
    "            reward_history,\n",
    "            label=\"Reward\",\n",
    "            color=\"blue\",\n",
    "            alpha=0.3,\n",
    "            linewidth=1\n",
    "        )\n",
    "\n",
    "        # moving average\n",
    "        if len(reward_history) > 20:\n",
    "            ma = np.convolve(reward_history, np.ones(20)/20, mode='valid')\n",
    "            plt.plot(\n",
    "                range(19, len(reward_history)),\n",
    "                ma,\n",
    "                label=\"Moving Avg (20 eps)\",\n",
    "                color=\"orange\",\n",
    "                linewidth=2.5\n",
    "            )\n",
    "\n",
    "        plt.title(\"Rainbow DQN ‚Äî Training Progress\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
