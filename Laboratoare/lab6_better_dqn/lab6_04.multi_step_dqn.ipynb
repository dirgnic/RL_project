{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6459485",
   "metadata": {},
   "source": [
    "# Multi-Step DQN â€” Rezumat\n",
    "\n",
    "**Multi-Step DQN** extinde algoritmul DQN prin folosirea unui *n-step return*,\n",
    "adicÄƒ ia Ã®n considerare mai multe recompense viitoare Ã®nainte de a actualiza\n",
    "valoarea unei acÈ›iuni.\n",
    "\n",
    "Ãn loc de actualizarea clasicÄƒ pe un singur pas:\n",
    "\n",
    "$$\n",
    "y = r + \\gamma \\, Q(s', a')\n",
    "$$\n",
    "\n",
    "Multi-Step foloseÈ™te:\n",
    "\n",
    "$$\n",
    "y^{(n)} = \n",
    "\\sum_{k=0}^{n-1} \\gamma^{k} r_{k}\n",
    "\\;+\\;\n",
    "\\gamma^{n} Q(s_{t+n}, a_{t+n})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ De ce Multi-Step?\n",
    "\n",
    "- oferÄƒ un **semnal de Ã®nvÄƒÈ›are mai bogat**  \n",
    "- reduce dependenÈ›a de targeturile instabile one-step  \n",
    "- antreneazÄƒ agentul sÄƒ Ã®nveÈ›e **lanÈ›uri Ã®ntregi de consecinÈ›e**, nu doar pasul urmÄƒtor  \n",
    "- convergenÈ›Äƒ mai rapidÄƒ È™i comportament mai stabil\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Cum funcÈ›ioneazÄƒ?\n",
    "\n",
    "1. StocÄƒm tranziÈ›iile Ã®ntr-un buffer temporar de **n paÈ™i**.  \n",
    "2. CÃ¢nd se strÃ¢ng `n` tranziÈ›ii, calculÄƒm suma discountatÄƒ a recompenselor.  \n",
    "3. ÃnvÄƒÈ›Äƒm din primul stat â†’ È™i ultimul stat din fereastra de n paÈ™i.  \n",
    "4. AplicÄƒm update-ul Double DQN normal, dar cu targetul multi-step.\n",
    "\n",
    "---\n",
    "\n",
    "## â­ Avantaje\n",
    "\n",
    "- Ã®nvaÈ›Äƒ secvenÈ›e, nu doar reacÈ›ii imediate  \n",
    "- mult mai eficient Ã®n medii cu recompense Ã®ntÃ¢rziate  \n",
    "- este parte esenÈ›ialÄƒ din **Rainbow DQN**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Pe scurt\n",
    "\n",
    "**Multi-Step DQN = DQN + update pe n paÈ™i cu discount acumulat**,  \n",
    "ce oferÄƒ un semnal mai puternic È™i accelereazÄƒ Ã®nvÄƒÈ›area.\n",
    "\n",
    "Este o Ã®mbunÄƒtÄƒÈ›ire simplÄƒ, dar foarte importantÄƒ, folositÄƒ Ã®n toate DQN-urile moderne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras import Model, layers, optimizers\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Multi-Step Replay Buffer (n-step returns)\n",
    "# ============================================================\n",
    "\n",
    "class MultiStepReplayBuffer:\n",
    "    def __init__(self, capacity, state_dim, n_step=3, gamma=0.99):\n",
    "        self.capacity = capacity\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "\n",
    "        self.states = np.zeros((capacity, state_dim), dtype=np.float32)\n",
    "        self.next_states = np.zeros((capacity, state_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros(capacity, dtype=np.int32)\n",
    "        self.rewards = np.zeros(capacity, dtype=np.float32)\n",
    "        self.dones = np.zeros(capacity, dtype=np.float32)\n",
    "\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "        self.n_step_buffer = collections.deque(maxlen=n_step)\n",
    "\n",
    "    def store(self, s, s2, a, r, d):\n",
    "        # Add transition to n-step buffer\n",
    "        self.n_step_buffer.append((s, s2, a, r, d))\n",
    "\n",
    "        # Wait until we have full n-step window\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return\n",
    "\n",
    "        # Compute n-step return\n",
    "        R = 0\n",
    "        discount = 1\n",
    "        for (_, _, _, reward_i, done_i) in self.n_step_buffer:\n",
    "            R += reward_i * discount\n",
    "            discount *= self.gamma\n",
    "            if done_i:\n",
    "                break\n",
    "\n",
    "        # First transition in buffer is the one that gets recorded\n",
    "        s_0, _, a_0, _, _ = self.n_step_buffer[0]\n",
    "        _, s_n, _, _, d_n = self.n_step_buffer[-1]\n",
    "\n",
    "        idx = self.ptr\n",
    "        self.states[idx] = s_0\n",
    "        self.next_states[idx] = s_n\n",
    "        self.actions[idx] = a_0\n",
    "        self.rewards[idx] = R\n",
    "        self.dones[idx] = float(d_n)\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        if self.ptr == 0:\n",
    "            self.full = True\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_size = self.capacity if self.full else self.ptr\n",
    "        idx = np.random.choice(max_size, batch_size, replace=False)\n",
    "\n",
    "        return (\n",
    "            self.states[idx],\n",
    "            self.next_states[idx],\n",
    "            self.actions[idx],\n",
    "            self.rewards[idx],\n",
    "            self.dones[idx]\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Basic DQN Network (2 hidden layers)\n",
    "# ============================================================\n",
    "\n",
    "class QNetwork(Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.fc1 = layers.Dense(128, activation='relu')\n",
    "        self.fc2 = layers.Dense(128, activation='relu')\n",
    "        self.fc3 = layers.Dense(n_actions)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        x = tf.nn.relu(self.fc1(x))\n",
    "        x = tf.nn.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.fc3.units - 1)\n",
    "        state = state[np.newaxis, :]\n",
    "        Q = self.call(state)[0]\n",
    "        return int(tf.argmax(Q).numpy())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "BATCH = 32\n",
    "MEM_SIZE = 50000\n",
    "NUM_EPISODES = 600\n",
    "\n",
    "N_STEP = 3               # <<< multi-step return\n",
    "GAMMA_N = GAMMA ** N_STEP\n",
    "\n",
    "INITIAL_EPS = 1.0\n",
    "FINAL_EPS = 0.02\n",
    "EPS_DECAY = 0.995\n",
    "\n",
    "TAU = 0.005\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Soft update function\n",
    "# ============================================================\n",
    "\n",
    "def soft_update(target, online, tau=TAU):\n",
    "    tw = target.get_weights()\n",
    "    ow = online.get_weights()\n",
    "    new_w = [t * (1 - tau) + o * tau for t, o in zip(tw, ow)]\n",
    "    target.set_weights(new_w)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training Loop â€” Multi-Step DQN\n",
    "# ============================================================\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "buffer = MultiStepReplayBuffer(MEM_SIZE, state_dim, n_step=N_STEP, gamma=GAMMA)\n",
    "online = QNetwork(n_actions)\n",
    "target = QNetwork(n_actions)\n",
    "target.set_weights(online.get_weights())\n",
    "\n",
    "optimizer = optimizers.Adam(LR)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "epsilon = INITIAL_EPS\n",
    "reward_history = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for step in range(200):\n",
    "\n",
    "        action = online.act(state, epsilon)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        ep_reward += reward\n",
    "\n",
    "        buffer.store(state, next_state, action, reward, float(done))\n",
    "        state = next_state\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # TRAINING\n",
    "        # ----------------------------------------------------\n",
    "        if buffer.ptr > 1000 or buffer.full:\n",
    "\n",
    "            states, next_states, actions, rewards, dones = buffer.sample(BATCH)\n",
    "\n",
    "            # Double DQN: choose action with online, evaluate with target\n",
    "            next_actions = tf.argmax(\n",
    "                online(next_states), axis=1, output_type=tf.int32\n",
    "            )\n",
    "            idx = tf.stack([tf.range(BATCH, dtype=tf.int32), next_actions], axis=1)\n",
    "\n",
    "            target_Q_values = tf.gather_nd(target(next_states), idx)\n",
    "\n",
    "            y = rewards + (1 - dones) * GAMMA_N * target_Q_values\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_pred = online(states)\n",
    "                a_idx = tf.stack([tf.range(BATCH, dtype=tf.int32), actions], axis=1)\n",
    "                selected_q = tf.gather_nd(q_pred, a_idx)\n",
    "                loss = mse(selected_q, y)\n",
    "\n",
    "            grads = tape.gradient(loss, online.trainable_variables)\n",
    "            grads = [tf.clip_by_norm(g, 5.0) for g in grads]\n",
    "            optimizer.apply_gradients(zip(grads, online.trainable_variables))\n",
    "\n",
    "            soft_update(target, online)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    epsilon = max(FINAL_EPS, epsilon * EPS_DECAY)\n",
    "    reward_history.append(ep_reward)\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # LIVE PLOT\n",
    "    # ----------------------------------------------------\n",
    "    if episode % 5 == 0:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10,4))\n",
    "\n",
    "        # Reward\n",
    "        plt.plot(\n",
    "            reward_history,\n",
    "            label=\"Reward\",\n",
    "            color=\"blue\",\n",
    "            alpha=0.3,   \n",
    "            linewidth=1\n",
    "        )\n",
    "\n",
    "        # Moving average\n",
    "        if len(reward_history) > 20:\n",
    "            ma = np.convolve(reward_history, np.ones(20)/20, mode='valid')\n",
    "            plt.plot(\n",
    "                range(19, len(reward_history)),\n",
    "                ma,\n",
    "                label=\"Moving Avg (20 eps)\",\n",
    "                color=\"orange\",\n",
    "                linewidth=2.5\n",
    "            )\n",
    "\n",
    "        plt.title(\"Multi-Step DQN â€” Training Progress\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
