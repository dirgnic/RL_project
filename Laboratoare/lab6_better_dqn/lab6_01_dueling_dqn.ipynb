{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f797934a",
   "metadata": {},
   "source": [
    "# Dueling Double DQN\n",
    "\n",
    "**Dueling Double DQN** combinƒÉ douƒÉ idei puternice:\n",
    "1. **Dueling Network Architecture**  \n",
    "2. **Double DQN (separarea ac»õiunii de evaluare)**\n",
    "\n",
    "AceastƒÉ combina»õie produce unul dintre cei mai stabili »ôi performan»õi algoritmi\n",
    "value-based pentru control discret.\n",
    "\n",
    "---\n",
    "\n",
    "## üé≠ 1. Dueling Network Architecture\n",
    "\n",
    "√én loc ca re»õeaua sƒÉ prezicƒÉ direct Q-values pentru fiecare ac»õiune, Dueling DQN\n",
    "√Æmparte estimarea √Æn douƒÉ fluxuri:\n",
    "\n",
    "### üîπ Value Stream (V)\n",
    "\n",
    "EvalueazƒÉ ‚Äûvaloarea‚Äù stƒÉrii:\n",
    "\n",
    "$$\n",
    "V(s)\n",
    "$$\n",
    "\n",
    "### üîπ Advantage Stream (A)\n",
    "\n",
    "EvalueazƒÉ avantajul fiecƒÉrei ac»õiuni:\n",
    "\n",
    "$$\n",
    "A(s, a)\n",
    "$$\n",
    "\n",
    "### üîπ Combina»õia finalƒÉ (normalizare a avantajelor)\n",
    "\n",
    "$$\n",
    "Q(s,a) = V(s) + A(s,a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s,a')\n",
    "$$\n",
    "\n",
    "AceastƒÉ formulƒÉ ajutƒÉ √Æn stƒÉrile unde *ac»õiunea exactƒÉ nu conteazƒÉ foarte mult*.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ 2. Double DQN\n",
    "\n",
    "Double DQN previne supraestimarea Q-values separ√¢nd alegerea ac»õiunii de evaluarea ei:\n",
    "\n",
    "- Re»õeaua **online** alege ac»õiunea  \n",
    "- Re»õeaua **target** o evalueazƒÉ  \n",
    "\n",
    "$$\n",
    "y = r + \\gamma \\, Q_{\\text{target}}\n",
    "\\big(s',\\, \\arg\\max_a Q_{\\text{online}}(s',a)\\big)\n",
    "$$\n",
    "\n",
    "Acest mecanism stabilizeazƒÉ puternic √ÆnvƒÉ»õarea.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß 3. Soft Target Updates (œÑ)\n",
    "\n",
    "√én loc de o copie brutalƒÉ a re»õelei online √Æn target, folosim un update lin:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{target}} \\leftarrow \n",
    "\\tau \\, \\theta_{\\text{online}} +\n",
    "(1 - \\tau) \\, \\theta_{\\text{target}}\n",
    "$$\n",
    "\n",
    "unde œÑ ‚âà 0.005.\n",
    "\n",
    "Avantaje:\n",
    "- stabilitate mult mai mare  \n",
    "- convergen»õƒÉ linƒÉ  \n",
    "- folosit √Æn algoritmii moderni (SAC, TD3)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Beneficii majore\n",
    "\n",
    "‚úî √énvƒÉ»õare mai rapidƒÉ √Æn stƒÉri ‚Äûne-informative‚Äù  \n",
    "‚úî Target fƒÉrƒÉ supraestimƒÉri  \n",
    "‚úî Variabilitate redusƒÉ  \n",
    "‚úî Performan»õƒÉ excelentƒÉ pe probleme complexe precum:\n",
    "- LunarLander  \n",
    "- MountainCar  \n",
    "- Acrobot  \n",
    "- Atari  \n",
    "\n",
    "---\n",
    "\n",
    "## üß© Pipeline (pe scurt)\n",
    "\n",
    "1. SelectƒÉm ac»õiunea cu **re»õeaua online**  \n",
    "2. CalculƒÉm valoarea »õintƒÉ cu **re»õeaua target**  \n",
    "3. CombinƒÉm V + A ‚Üí Q prin formula dueling  \n",
    "4. Facem gradient descent  \n",
    "5. AplicƒÉm soft update (œÑ)  \n",
    "\n",
    "---\n",
    "\n",
    "## üìå Pe scurt\n",
    "\n",
    "**Dueling Double DQN =  \n",
    "Double DQN (stabil) + Dueling Network (eficient) + Soft Updates (modern)**\n",
    "\n",
    "Una dintre cele mai bune arhitecturi value-based pentru control discret »ôi\n",
    "componentƒÉ esen»õialƒÉ √Æn **Rainbow DQN**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3cf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras import Model, layers, optimizers\n",
    "\n",
    "# ============================================================\n",
    "# Replay Buffer\n",
    "# ============================================================\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, memory_size: int) -> None:\n",
    "        self.memory_size = memory_size\n",
    "        self.buffer = deque(maxlen=memory_size)\n",
    "\n",
    "    def add(self, experience) -> None:\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        indexes = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "        return [self.buffer[i] for i in indexes]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Dueling Network\n",
    "# ============================================================\n",
    "\n",
    "class Qnetwork(Model):\n",
    "    def __init__(self):\n",
    "        super(Qnetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = layers.Dense(64, activation='relu')\n",
    "        self.fc_value = layers.Dense(256, activation='relu')\n",
    "        self.fc_adv = layers.Dense(256, activation='relu')\n",
    "\n",
    "        self.value = layers.Dense(1)     # V(s)\n",
    "        self.adv = layers.Dense(2)       # A(s,a) ‚Äî CartPole: 2 actions\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        value = self.fc_value(x)\n",
    "        adv = self.fc_adv(x)\n",
    "\n",
    "        value = self.value(value)\n",
    "        adv = self.adv(adv)\n",
    "\n",
    "        adv_mean = tf.reduce_mean(adv, axis=1, keepdims=True)\n",
    "        Q = value + adv - adv_mean  # dueling combine\n",
    "\n",
    "        return Q\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        obs = np.array(obs, dtype=np.float32)[np.newaxis, :]\n",
    "        Q = self.call(obs)\n",
    "        return int(tf.argmax(Q[0]).numpy())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "GAMMA = 0.99\n",
    "INITIAL_EPSILON = 0.99\n",
    "FINAL_EPSILON = 0.0001\n",
    "EXPLORATION_STEPS = 20000\n",
    "REPLAY_MEMORY = 50000\n",
    "BATCH = 32\n",
    "UPDATE_STEPS = 10\n",
    "LR = 1e-3 * 5\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "# ============================================================\n",
    "# Main\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "    memory_replay = Memory(REPLAY_MEMORY)\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    learn_steps = 0\n",
    "    begin_learn = False\n",
    "\n",
    "    writer = SummaryWriter('ddqn-tf2')\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    online_q = Qnetwork()\n",
    "    target_q = Qnetwork()\n",
    "    target_q.set_weights(online_q.get_weights())\n",
    "\n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    optim = optimizers.Adam(LR)\n",
    "\n",
    "    reward_history = []\n",
    "\n",
    "    for epoch in range(NUM_EPISODES):\n",
    "\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in range(200):\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # Epsilon-greedy\n",
    "            # ------------------------------------------------\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randint(0, 1)\n",
    "            else:\n",
    "                action = online_q.select_action(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "\n",
    "            # store experience\n",
    "            memory_replay.add((state, next_state, action, reward, done))\n",
    "            state = next_state\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # Learn\n",
    "            # ------------------------------------------------\n",
    "            if memory_replay.size() > 100:\n",
    "                if not begin_learn:\n",
    "                    print(\"Learning starts!\")\n",
    "                    begin_learn = True\n",
    "\n",
    "                learn_steps += 1\n",
    "\n",
    "                # sync target network\n",
    "                if learn_steps % UPDATE_STEPS == 0:\n",
    "                    target_q.set_weights(online_q.get_weights())\n",
    "\n",
    "                batch = memory_replay.sample(BATCH)\n",
    "                batch_state, batch_next_state, batch_action, batch_reward, batch_done = zip(*batch)\n",
    "\n",
    "                batch_state = np.asarray(batch_state, dtype=np.float32)\n",
    "                batch_next_state = np.asarray(batch_next_state, dtype=np.float32)\n",
    "                batch_action = np.asarray(batch_action, dtype=np.int32)\n",
    "                batch_reward = np.asarray(batch_reward, dtype=np.float32)\n",
    "                batch_done = np.asarray(batch_done, dtype=np.float32)\n",
    "\n",
    "                # -------------------------------\n",
    "                # Double DQN update\n",
    "                # -------------------------------\n",
    "                next_action_online = tf.argmax(online_q(batch_next_state), axis=1)\n",
    "                next_action_online = tf.cast(next_action_online, tf.int32)\n",
    "                next_index = tf.stack([tf.range(tf.shape(next_action_online)[0]), next_action_online], axis=1)\n",
    "\n",
    "                q_target_next = target_q(batch_next_state)\n",
    "                q_target_selected = tf.gather_nd(q_target_next, next_index)\n",
    "\n",
    "                y = batch_reward + (1.0 - batch_done) * GAMMA * q_target_selected\n",
    "\n",
    "                # -------------------------------\n",
    "                # Compute loss\n",
    "                # -------------------------------\n",
    "                with tf.GradientTape() as tape:\n",
    "                    q_pred = online_q(batch_state)\n",
    "                    batch_action_idx = tf.stack([tf.range(tf.shape(batch_action)[0]), batch_action], axis=1)\n",
    "                    q_pred_selected = tf.gather_nd(q_pred, batch_action_idx)\n",
    "                    loss = mse(q_pred_selected, y)\n",
    "\n",
    "                grads = tape.gradient(loss, online_q.trainable_variables)\n",
    "                optim.apply_gradients(zip(grads, online_q.trainable_variables))\n",
    "\n",
    "                # update epsilon\n",
    "                if epsilon > FINAL_EPSILON:\n",
    "                    epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORATION_STEPS\n",
    "\n",
    "                writer.add_scalar(\"loss\", float(loss), learn_steps)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        reward_history.append(episode_reward)\n",
    "        writer.add_scalar(\"episode_reward\", episode_reward, epoch)\n",
    "\n",
    "        # ----------------------------------------------------\n",
    "        # LIVE PLOT\n",
    "        # ----------------------------------------------------\n",
    "        # ----------------------------\n",
    "        # LIVE PLOT\n",
    "        # ----------------------------\n",
    "        if epoch % 5 == 0:\n",
    "            clear_output(wait=True)\n",
    "            plt.figure(figsize=(10,4))\n",
    "\n",
    "            # Reward\n",
    "            plt.plot(\n",
    "                reward_history,\n",
    "                label=\"Reward\",\n",
    "                color=\"blue\",\n",
    "                alpha=0.3,  \n",
    "                linewidth=1\n",
    "            )\n",
    "\n",
    "            # Moving average\n",
    "            if len(reward_history) > 20:\n",
    "                ma = np.convolve(reward_history, np.ones(20)/20, mode='valid')\n",
    "                plt.plot(\n",
    "                    range(19, len(reward_history)),\n",
    "                    ma,\n",
    "                    label=\"Moving Avg (20 eps)\",\n",
    "                    color=\"orange\",\n",
    "                    linewidth=2.5\n",
    "                )\n",
    "\n",
    "            plt.title(\"Dueling Double DQN ‚Äî Training Progress\")\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904dc79",
   "metadata": {},
   "source": [
    "# OptimizƒÉri pentru Dueling Double DQN\n",
    "\n",
    "## ‚úî Soft Updates (œÑ = 0.005)\n",
    "\n",
    "√énlocuie»ôte copia bruscƒÉ a re»õelei target cu o actualizare linƒÉ:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{target}} \\leftarrow \n",
    "\\tau\\,\\theta_{\\text{online}} + (1-\\tau)\\,\\theta_{\\text{target}}\n",
    "$$\n",
    "\n",
    "‚Üí reduce oscila»õiile »ôi stabilizeazƒÉ √ÆnvƒÉ»õarea.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úî Gradient Clipping\n",
    "\n",
    "LimiteazƒÉ norma gradientului pentru a evita divergen»õa »ôi valori NaN.  \n",
    "AsigurƒÉ update-uri mici, controlate »ôi stabile.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úî Q-value Clipping\n",
    "\n",
    "AplicƒÉm limitare pe predic»õiile Q:\n",
    "\n",
    "$$\n",
    "Q \\leftarrow \\text{clip}(Q,\\,-500,\\,500)\n",
    "$$\n",
    "\n",
    "‚Üí previne ‚Äûexplozia‚Äù valorilor √Æn stƒÉri instabile.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úî Learning Rate mic + Re»õea mai ad√¢ncƒÉ\n",
    "\n",
    "- **LR = 3e-4**  \n",
    "- douƒÉ straturi dense a c√¢te **128 neuroni**\n",
    "\n",
    "‚Üí stabilitate mai mare »ôi reprezentare mai bunƒÉ a func»õiei Q.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úî Smooth Epsilon Decay (0.995)\n",
    "\n",
    "Agentul reduce epsilon treptat:\n",
    "\n",
    "- explorare ‚Üí exploatare fƒÉrƒÉ salturi bru»ôte  \n",
    "- comportament mai natural »ôi mai stabil\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úî Replay Buffer Vectorizat\n",
    "\n",
    "Memoria folose»ôte matrici NumPy, nu liste:\n",
    "\n",
    "- sampling rapid  \n",
    "- update-uri eficiente  \n",
    "- integrare optimƒÉ cu TensorFlow  \n",
    "\n",
    "---\n",
    "\n",
    "## üìå Pe scurt\n",
    "\n",
    "Aceste optimizƒÉri fac Dueling Double DQN **mult mai stabil, mai rapid »ôi mai robust**, oferind un training consistent chiar »ôi √Æn environmente dificile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60331bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras import Model, layers, optimizers\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Replay Buffer (optimized)\n",
    "# ============================================================\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size, state_dim):\n",
    "        self.mem_size = size\n",
    "        self.mem_cntr = 0\n",
    "\n",
    "        self.state_memory = np.zeros((size, state_dim), dtype=np.float32)\n",
    "        self.next_state_memory = np.zeros((size, state_dim), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(size, dtype=np.float32)\n",
    "        self.done_memory = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "    def store(self, state, next_state, action, reward, done):\n",
    "        idx = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[idx] = state\n",
    "        self.next_state_memory[idx] = next_state\n",
    "        self.action_memory[idx] = action\n",
    "        self.reward_memory[idx] = reward\n",
    "        self.done_memory[idx] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        return (\n",
    "            self.state_memory[batch],\n",
    "            self.next_state_memory[batch],\n",
    "            self.action_memory[batch],\n",
    "            self.reward_memory[batch],\n",
    "            self.done_memory[batch]\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Dueling Network (optimized)\n",
    "# ============================================================\n",
    "\n",
    "class DuelingQNetwork(Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = layers.Dense(128, activation='relu')\n",
    "        self.fc2 = layers.Dense(128, activation='relu')\n",
    "\n",
    "        self.value_fc = layers.Dense(128, activation='relu')\n",
    "        self.adv_fc = layers.Dense(128, activation='relu')\n",
    "\n",
    "        self.value = layers.Dense(1)\n",
    "        self.advantage = layers.Dense(n_actions)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        V = self.value(self.value_fc(x))\n",
    "        A = self.advantage(self.adv_fc(x))\n",
    "\n",
    "        A_mean = tf.reduce_mean(A, axis=1, keepdims=True)\n",
    "        Q = V + (A - A_mean)\n",
    "\n",
    "        return tf.clip_by_value(Q, -500, 500)\n",
    "\n",
    "    def act(self, obs):\n",
    "        obs = obs[np.newaxis, :].astype(np.float32)\n",
    "        Q = self.call(obs)[0]\n",
    "        return int(tf.argmax(Q).numpy())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "GAMMA = 0.99\n",
    "LR = 3e-4\n",
    "BATCH = 32\n",
    "MEM_SIZE = 50000\n",
    "NUM_EPISODES = 600\n",
    "\n",
    "INITIAL_EPS = 1.0\n",
    "FINAL_EPS = 0.02\n",
    "EPS_DECAY = 0.995\n",
    "\n",
    "TAU = 0.005   # soft update\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Soft update function\n",
    "# ============================================================\n",
    "\n",
    "def soft_update(target, online, tau=TAU):\n",
    "    target_weights = target.get_weights()\n",
    "    online_weights = online.get_weights()\n",
    "    new_weights = []\n",
    "\n",
    "    for tw, ow in zip(target_weights, online_weights):\n",
    "        new_weights.append(tw * (1 - tau) + ow * tau)\n",
    "\n",
    "    target.set_weights(new_weights)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training Loop\n",
    "# ============================================================\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "buffer = ReplayBuffer(MEM_SIZE, state_dim)\n",
    "online = DuelingQNetwork(n_actions)\n",
    "target = DuelingQNetwork(n_actions)\n",
    "target.set_weights(online.get_weights())\n",
    "\n",
    "optimizer = optimizers.Adam(LR)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "epsilon = INITIAL_EPS\n",
    "reward_history = []\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for step in range(200):\n",
    "\n",
    "        # ----------------------------\n",
    "        # Epsilon-greedy action\n",
    "        # ----------------------------\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = online.act(state)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        ep_reward += reward\n",
    "\n",
    "        # store transition\n",
    "        buffer.store(state, next_state, action, reward, float(done))\n",
    "        state = next_state\n",
    "\n",
    "        # ----------------------------\n",
    "        # TRAINING\n",
    "        # ----------------------------\n",
    "        if buffer.mem_cntr > 1000:\n",
    "\n",
    "            states, next_states, actions, rewards, dones = buffer.sample(BATCH)\n",
    "\n",
    "            # Double DQN ‚Äî FIXED with correct dtypes\n",
    "            next_actions = tf.argmax(\n",
    "                online(next_states), axis=1, output_type=tf.int32\n",
    "            )\n",
    "\n",
    "            indices = tf.stack(\n",
    "                [tf.range(BATCH, dtype=tf.int32), next_actions], axis=1\n",
    "            )\n",
    "\n",
    "            next_Q = tf.gather_nd(target(next_states), indices)\n",
    "            y = rewards + GAMMA * (1 - dones) * next_Q\n",
    "\n",
    "            # ----------------------------\n",
    "            # Compute loss\n",
    "            # ----------------------------\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_pred = online(states)\n",
    "                action_idx = tf.stack(\n",
    "                    [tf.range(BATCH, dtype=tf.int32), actions], axis=1\n",
    "                )\n",
    "                pred = tf.gather_nd(q_pred, action_idx)\n",
    "                loss = mse(pred, y)\n",
    "\n",
    "            grads = tape.gradient(loss, online.trainable_variables)\n",
    "            grads = [tf.clip_by_norm(g, 5.0) for g in grads]\n",
    "            optimizer.apply_gradients(zip(grads, online.trainable_variables))\n",
    "\n",
    "            # Soft update for stability\n",
    "            soft_update(target, online)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # ----------------------------\n",
    "    # Epsilon decay\n",
    "    # ----------------------------\n",
    "    epsilon = max(FINAL_EPS, epsilon * EPS_DECAY)\n",
    "    reward_history.append(ep_reward)\n",
    "\n",
    "    # ----------------------------\n",
    "    # LIVE PLOT\n",
    "    # ----------------------------\n",
    "    if episode % 5 == 0:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10,4))\n",
    "\n",
    "        # Reward\n",
    "        plt.plot(\n",
    "            reward_history,\n",
    "            label=\"Reward\",\n",
    "            color=\"blue\",\n",
    "            alpha=0.3,  \n",
    "            linewidth=1\n",
    "        )\n",
    "\n",
    "        # Moving average\n",
    "        if len(reward_history) > 20:\n",
    "            ma = np.convolve(reward_history, np.ones(20)/20, mode='valid')\n",
    "            plt.plot(\n",
    "                range(19, len(reward_history)),\n",
    "                ma,\n",
    "                label=\"Moving Avg (20 eps)\",\n",
    "                color=\"orange\",\n",
    "                linewidth=2.5\n",
    "            )\n",
    "\n",
    "        plt.title(\"Dueling Double DQN ‚Äî Training Progress\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
