{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175d404a",
   "metadata": {},
   "source": [
    "# üßÆ Laborator 3 ‚Äî Agen»õi Value-Based √Æn Reinforcement Learning\n",
    "\n",
    "**Mediile folosite:** `GridWorld (custom)`, `FrozenLake-v1`, `CartPole-v1`, `MountainCar-v0`, `Bandit 10-Arms`\n",
    "\n",
    "**Algoritmi:** Value Iteration, Monte Carlo, TD(0), Q-learning (Œµ-greedy), Multi-Armed Bandits\n",
    "\n",
    "> Rula»õi sec»õiunile √Æn ordine. Ajusta»õi hiper-parametrii pentru explorare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4ed085",
   "metadata": {},
   "source": [
    "## üîß Setup & utilitare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c9122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5caf950",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£ GridWorld (custom) ‚Äî Value Iteration (model-based)\n",
    "\n",
    "Grid 4√ó4, douƒÉ stƒÉri terminale: `+1` (goal) »ôi `-1` (hole). Fiecare pas non-terminal prime»ôte `-0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29363ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridWorld setup\n",
    "nrow, ncol = 4, 4\n",
    "S, A, gamma = nrow*ncol, 4, 0.99\n",
    "terminal_plus, terminal_minus = 15, 12\n",
    "\n",
    "def to_rc(s): return divmod(s, ncol)\n",
    "def to_s(r,c): return r*ncol + c\n",
    "\n",
    "def step_gw(s, a):\n",
    "    if s in (terminal_plus, terminal_minus):\n",
    "        return s, 0.0, True\n",
    "    r, c = to_rc(s)\n",
    "    if a == 0: r = max(0, r-1)\n",
    "    elif a == 1: c = min(ncol-1, c+1)\n",
    "    elif a == 2: r = min(nrow-1, r+1)\n",
    "    elif a == 3: c = max(0, c-1)\n",
    "    s2 = to_s(r,c)\n",
    "    if s2 == terminal_plus:  return s2, +1.0, True\n",
    "    if s2 == terminal_minus: return s2, -1.0, True\n",
    "    return s2, -0.01, False\n",
    "\n",
    "def value_iteration(theta=1e-5, gamma=0.99):\n",
    "    V = np.zeros(S)\n",
    "    it = 0\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        for s in range(S):\n",
    "            if s in (terminal_plus, terminal_minus): continue\n",
    "            q_vals = []\n",
    "            for a in range(A):\n",
    "                s2, r, done = step_gw(s,a)\n",
    "                q_vals.append(r + (0 if done else gamma*V[s2]))\n",
    "            new_v = max(q_vals)\n",
    "            delta = max(delta, abs(V[s] - new_v))\n",
    "            V[s] = new_v\n",
    "        it += 1\n",
    "        if delta < theta or it > 1000:\n",
    "            break\n",
    "    return V, it\n",
    "\n",
    "V_gw, iters = value_iteration()\n",
    "print(f\"Convergen»õƒÉ √Æn {iters} itera»õii.\")\n",
    "print(V_gw.reshape(nrow, ncol))\n",
    "\n",
    "# Vizualizare\n",
    "plt.figure(figsize=(4,4))\n",
    "grid = V_gw.reshape(nrow, ncol)\n",
    "plt.imshow(grid, cmap=\"viridis\")\n",
    "for r in range(nrow):\n",
    "    for c in range(ncol):\n",
    "        plt.text(c, r, f\"{grid[r,c]:.2f}\", ha=\"center\", va=\"center\", color=\"white\")\n",
    "plt.title(\"Value Iteration ‚Äî V(s)\")\n",
    "plt.axis(\"off\")\n",
    "plt.colorbar(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3e4cb",
   "metadata": {},
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ FrozenLake-v1 ‚Äî Monte Carlo (state-value, episodic, model-free)\n",
    "\n",
    "EstimƒÉm `V(s)` ca media returnurilor observate sub o politicƒÉ aleatoare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f454f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
    "gamma, episodes = 0.99, 5000\n",
    "epsilon, min_eps, decay = 1.0, 0.01, 0.999\n",
    "\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "returns_sum, returns_count = defaultdict(float), defaultdict(int)\n",
    "\n",
    "def epsilon_greedy_policy(state, eps):\n",
    "    if np.random.rand() < eps:\n",
    "        return env.action_space.sample()\n",
    "    return np.argmax(Q[state])\n",
    "\n",
    "reward_history = []\n",
    "\n",
    "for ep in range(1, episodes + 1):\n",
    "    obs, _ = env.reset(seed=ep)\n",
    "    episode, done = [], False\n",
    "\n",
    "    while not done:\n",
    "        a = epsilon_greedy_policy(obs, epsilon)\n",
    "        obs2, reward, terminated, truncated, _ = env.step(a)\n",
    "        episode.append((obs, a, reward))\n",
    "        obs = obs2\n",
    "        done = terminated or truncated\n",
    "\n",
    "    G, visited = 0.0, set()\n",
    "    for s, a, r in reversed(episode):\n",
    "        G = gamma * G + r\n",
    "        if (s, a) not in visited:\n",
    "            returns_sum[(s, a)] += G\n",
    "            returns_count[(s, a)] += 1\n",
    "            Q[s][a] = returns_sum[(s, a)] / returns_count[(s, a)]\n",
    "            visited.add((s, a))\n",
    "\n",
    "    epsilon = max(min_eps, epsilon * decay)\n",
    "    reward_history.append(sum([r for _,_,r in episode]))\n",
    "\n",
    "env.close()\n",
    "\n",
    "# === Politica finalƒÉ ===\n",
    "policy = np.array([np.argmax(Q[s]) for s in range(env.observation_space.n)]).reshape(4,4)\n",
    "symbols = np.array([\"‚Üê\",\"‚Üì\",\"‚Üí\",\"‚Üë\"])\n",
    "policy_grid = np.full((4,4), \"‚Ä¢\", dtype=\"<U1\")\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        idx = i*4 + j\n",
    "        policy_grid[i,j] = symbols[policy[i,j]]\n",
    "\n",
    "plt.imshow(policy.reshape(4,4), cmap=\"Blues\")\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        plt.text(j, i, policy_grid[i,j], ha=\"center\", va=\"center\", color=\"black\", fontsize=14)\n",
    "plt.title(\"Politica finalƒÉ (Monte Carlo Control Œµ-Greedy)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# === Evolu»õia reward-urilor ===\n",
    "plt.plot(np.convolve(reward_history, np.ones(50)/50, mode='valid'))\n",
    "plt.title(\"Evolu»õia reward-ului mediu (MC Control)\")\n",
    "plt.xlabel(\"Episod\"); plt.ylabel(\"Reward mediu\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4261eea",
   "metadata": {},
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ CartPole-v1 ‚Äî Temporal Difference (TD(0))\n",
    "\n",
    "DiscretizƒÉm observa»õiile pentru a aproxima `V(s)` tabular. PoliticƒÉ aleatoare (demonstra»õie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52087faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "alpha, gamma = 0.1, 0.99\n",
    "epsilon, eps_min, eps_decay = 1.0, 0.05, 0.995\n",
    "\n",
    "# discretizare\n",
    "bins = [\n",
    "    np.linspace(-2.4, 2.4, 10),\n",
    "    np.linspace(-3.0, 3.0, 10),\n",
    "    np.linspace(-0.21, 0.21, 10),\n",
    "    np.linspace(-3.5, 3.5, 10)\n",
    "]\n",
    "\n",
    "def discretize(obs):\n",
    "    return tuple(np.digitize(obs[i], bins[i]) for i in range(4))\n",
    "\n",
    "# ini»õializare Q(s,a)\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "def epsilon_greedy(s, eps):\n",
    "    if np.random.rand() < eps:\n",
    "        return env.action_space.sample()\n",
    "    return int(np.argmax(Q[s]))\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for ep in range(500):  # <<< cre»ôte la 1000 pentru convergen»õƒÉ mai bunƒÉ\n",
    "    obs, _ = env.reset(seed=ep)\n",
    "    s = discretize(obs)\n",
    "    a = epsilon_greedy(s, epsilon)\n",
    "    done, total = False, 0\n",
    "\n",
    "    while not done:\n",
    "        obs2, r, terminated, truncated, _ = env.step(a)\n",
    "        s2 = discretize(obs2)\n",
    "        a2 = epsilon_greedy(s2, epsilon)\n",
    "        Q[s][a] += alpha * (r + gamma * Q[s2][a2] - Q[s][a]) \n",
    "        s, a = s2, a2\n",
    "        done = terminated or truncated\n",
    "        total += r\n",
    "\n",
    "    epsilon = max(eps_min, epsilon * eps_decay)\n",
    "    rewards.append(total)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# === Grafic: evolu»õia performan»õei ===\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(np.convolve(rewards, np.ones(20)/20, mode='valid'))\n",
    "plt.title(\"CartPole ‚Äî TD(0)\")\n",
    "plt.xlabel(\"Episod\")\n",
    "plt.ylabel(\"Reward mediu\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2b349",
   "metadata": {},
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ MountainCar-v0 ‚Äî Q-learning cu Œµ-greedy (Œµ decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e25a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# hiperparametri\n",
    "alpha, gamma = 0.2, 0.99\n",
    "epsilon, eps_min, eps_decay = 1.0, 0.01, 0.999  # explorare mai lentƒÉ\n",
    "episodes = 5000\n",
    "\n",
    "# discretizare mai finƒÉ\n",
    "bins = [\n",
    "    np.linspace(-1.2, 0.6, 40),   # pozi»õia ma»ôinii\n",
    "    np.linspace(-0.07, 0.07, 40)  # viteza\n",
    "]\n",
    "\n",
    "def discretize(obs):\n",
    "    return tuple(np.digitize(obs[i], bins[i]) for i in range(2))\n",
    "\n",
    "Q = np.zeros((len(bins[0])+1, len(bins[1])+1, env.action_space.n))\n",
    "returns = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset(seed=ep)\n",
    "    s = discretize(obs)\n",
    "    done = False\n",
    "    total = 0\n",
    "    \n",
    "    while not done:\n",
    "        # ac»õiune Œµ-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = np.random.randint(env.action_space.n)\n",
    "        else:\n",
    "            a = int(np.argmax(Q[s]))\n",
    "        \n",
    "        obs2, reward, term, trunc, _ = env.step(a)\n",
    "        s2 = discretize(obs2)\n",
    "\n",
    "        # recompensƒÉ u»ôor modelatƒÉ (bonus pentru vitezƒÉ »ôi pozi»õie bunƒÉ)\n",
    "        reward += abs(obs2[1]) * 0.5\n",
    "        if obs2[0] > 0.45: reward += 5.0  # bonus mic pentru atingerea dealului\n",
    "        \n",
    "        best_next = np.max(Q[s2])\n",
    "        Q[s][a] += alpha * (reward + gamma * best_next - Q[s][a])\n",
    "        s = s2\n",
    "        total += reward\n",
    "        done = term or trunc\n",
    "\n",
    "    epsilon = max(eps_min, epsilon * eps_decay)\n",
    "    returns.append(total)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# === Vizualizare ===\n",
    "window = 100\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(pd.Series(returns).rolling(window).mean())\n",
    "plt.title(f\"MountainCar-v0 ‚Äî Q-learning\")\n",
    "plt.xlabel(\"Episod\")\n",
    "plt.ylabel(\"Reward mediu\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e47a37",
   "metadata": {},
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ Multi-Armed Bandit ‚Äî Œµ-greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b467d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "arms = 10\n",
    "true_q = np.random.normal(0, 1, arms)\n",
    "Qb = np.zeros(arms)\n",
    "Nb = np.zeros(arms)\n",
    "eps = 0.1\n",
    "T = 1000\n",
    "\n",
    "rewards = []\n",
    "for t in range(T):\n",
    "    if np.random.rand() < eps:\n",
    "        a = np.random.randint(arms)\n",
    "    else:\n",
    "        a = int(np.argmax(Qb))\n",
    "    r = np.random.normal(true_q[a], 1.0)\n",
    "    Nb[a] += 1\n",
    "    Qb[a] += (r - Qb[a]) / Nb[a]\n",
    "    rewards.append(r)\n",
    "\n",
    "plt.plot(pd.Series(rewards).rolling(20).mean())\n",
    "plt.title(\"Bandit 10-Arms ‚Äî reward\")\n",
    "plt.xlabel(\"Pas\"); plt.ylabel(\"Reward\"); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl-lab3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
