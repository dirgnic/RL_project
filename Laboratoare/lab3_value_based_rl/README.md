# ğŸ§® Laborator 3 â€” AgenÈ›i Value-Based Ã®n Reinforcement Learning

## ğŸ¯ Obiective

Ãn acest laborator explorÄƒm algoritmi clasici de RL, de tip **value-based**, prin experimente practice folosind librÄƒria `gymnasium`.  
Scopul este sÄƒ Ã®nÈ›elegem cum pot fi estimate valorile stÄƒrilor È™i acÈ›iunilor, È™i cum pot fi folosite pentru a ghida deciziile unui agent RL.

---

## ğŸ“˜ ConÈ›inutul laboratorului

|  Algoritm / Concept | Mediul folosit | Tip Ã®nvÄƒÈ›are | ObservaÈ›ii |
|-------------------|----------------|---------------|-------------|
| **Value Iteration** | GridWorld (custom) | Model-based | Programare dinamicÄƒ |
| **Monte Carlo** | FrozenLake-v1 | Model-free, episodic | ÃnvaÈ›Äƒ din episoade complete |
| **Q-learning (Îµ-Greedy)** | MountainCar-v0 | Explorare controlatÄƒ | Agent activ cu Îµ-decay |
| **Multi-Armed Bandit** | Simulat (10 braÈ›e) | Simplificat, non-MDP | IlustreazÄƒ trade-off-ul explorare/exploatare |
| **Temporal Difference** | CartPole-v1 | Model-free, online | ÃnvaÈ›Äƒ incremental, fÄƒrÄƒ a aÈ™tepta sfÃ¢rÈ™itul episodului |

---

## ğŸ“Š AnalizÄƒ comparativÄƒ a algoritmilor

| Algoritm | NecesitÄƒ model? | Episodic? | Online? | Puncte forte | LimitÄƒri |
|-----------|----------------|-----------|----------|---------------|-----------|
| **Value Iteration** | âœ… | âŒ | âŒ | ConvergenÈ›Äƒ exactÄƒ, clar teoretic | Scalabilitate redusÄƒ |
| **Monte Carlo** | âŒ | âœ… | âŒ | Bias redus, concept intuitiv | NecesitÄƒ episoade complete |
| **Q-learning** | âŒ | â— | âœ… | Off-policy, general, robust | NecesitÄƒ discretizare È™i fine tuning |
| **Bandit Îµ-Greedy** | âŒ | â€” | âœ… | Exemplu clar de explorare | Nu are noÈ›iunea de stare |
| **Temporal Difference** | âŒ | â— | âœ… | ÃnvaÈ›Äƒ din tranziÈ›ii parÈ›iale, rapid | Bias È™i sensibilitate la Î± |

---

## ğŸ’¬ DiscuÈ›ii È™i concluzii

- Algoritmii **value-based** reprezintÄƒ fundamentul Reinforcement Learning-ului clasic.  
- **Value Iteration** oferÄƒ o soluÈ›ie exactÄƒ, dar doar pentru medii mici È™i complet cunoscute.  
- **Monte Carlo** È™i **TD(0)** sunt primele forme de *learning by experience* â€” agentul Ã®nvaÈ›Äƒ direct din interacÈ›iune.  
- **Q-learning** este cel mai utilizat algoritm tabular, fiind *off-policy* È™i capabil sÄƒ Ã®nveÈ›e comportamente optime prin explorare controlatÄƒ.  
- **Multi-Armed Bandits** oferÄƒ o analogie simplificatÄƒ pentru problema explorareâ€“exploatare, fÄƒrÄƒ complexitatea MDP-urilor.

---

## ğŸ§  ExerciÈ›ii propuse

1. ModificaÈ›i parametrii **Îµ** (*iniÈ›ial*, *decay*, *min*) Ã®n **MountainCar** È™i observaÈ›i efectul asupra traiectoriei.  
2. Ãn **FrozenLake**, setaÈ›i `is_slippery=False` È™i comparaÈ›i convergenÈ›a valorilor obÈ›inute prin algoritmul Monte Carlo.  
3. Ãn **CartPole**, mÄƒriÈ›i **Î±** la `0.5` È™i observaÈ›i dacÄƒ Ã®nvÄƒÈ›area devine instabilÄƒ.  
4. Ãn **Bandit**, implementaÈ›i un **Îµ reward-based decay** â€” reduceÈ›i Îµ mai repede dacÄƒ reward-urile recente cresc.  
5. Ãn **GridWorld**, modificaÈ›i poziÈ›iile stÄƒrilor terminale È™i observaÈ›i schimbarea valorilor `V(s)`.

---

## ğŸ§± ObservaÈ›ii finale

- Laboratorul poate fi extins cu versiuni **policy-based** (ex. *REINFORCE*) sau **actor-critic**, abordate Ã®n cursurile viitoare.  
- Exemplele sunt compatibile cu `gymnasium >= 0.29` È™i `Python 3.10+`.  
- Toate seed-urile sunt controlate prin `rng = np.random.default_rng(42)` pentru rezultate reproductibile.

---

## ğŸ“˜ FiÈ™iere incluse

| FiÈ™ier | Descriere |
|---------|------------|
| **lab3_value_based_agents.ipynb** | Notebook complet cu implementÄƒrile agenÈ›ilor È™i vizualizÄƒri grafice |
| **README.md** | FiÈ™ierul explicativ al laboratorului |
| **environment.yml** | SpecificaÈ›ia mediului Conda necesar pentru rulare |
