{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a91e1b3c",
   "metadata": {},
   "source": [
    "# Q-Learning Quick Start\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load environments\n",
    "2. Apply reward shaping\n",
    "3. Train a Q-Learning agent\n",
    "4. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0776c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from env import load_environment, CustomRewardWrapper\n",
    "from agents.q_learning.q_table import QTable\n",
    "from agents.q_learning.utils import epsilon_greedy, decay_epsilon, evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a7de3",
   "metadata": {},
   "source": [
    "## 1. Explore Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22699dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CartPole environment\n",
    "env = load_environment(\"CartPole-v1\", seed=42)\n",
    "\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Number of actions: {env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5235d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test random agent\n",
    "state, info = env.reset()\n",
    "print(f\"Initial state: {state}\")\n",
    "\n",
    "total_reward = 0\n",
    "for step in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"Step {step+1}: Action={action}, Reward={reward:.2f}, State={state}\")\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52776a16",
   "metadata": {},
   "source": [
    "## 2. Apply Reward Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare default vs modified reward\n",
    "env_default = load_environment(\"CartPole-v1\", seed=42)\n",
    "env_modified = CustomRewardWrapper(\n",
    "    load_environment(\"CartPole-v1\", seed=42),\n",
    "    reward_type=\"bonus_center\",\n",
    "    env_name=\"CartPole-v1\"\n",
    ")\n",
    "\n",
    "# Test one step\n",
    "state_default, _ = env_default.reset()\n",
    "state_modified, _ = env_modified.reset()\n",
    "\n",
    "_, reward_default, _, _, _ = env_default.step(1)\n",
    "_, reward_modified, _, _, _ = env_modified.step(1)\n",
    "\n",
    "print(f\"Default reward: {reward_default}\")\n",
    "print(f\"Modified reward (bonus_center): {reward_modified}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681b2c7",
   "metadata": {},
   "source": [
    "## 3. Train Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bcd057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "env = load_environment(\"CartPole-v1\", seed=42)\n",
    "env = CustomRewardWrapper(env, reward_type=\"bonus_center\", env_name=\"CartPole-v1\")\n",
    "\n",
    "# Initialize Q-Table\n",
    "state_bounds = [(-2.4, 2.4), (-3.0, 3.0), (-0.5, 0.5), (-2.0, 2.0)]\n",
    "q_table = QTable(num_actions=2, num_bins=[10, 10, 10, 10], state_bounds=state_bounds)\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Initial exploration rate\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 200\n",
    "\n",
    "# Training\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "\n",
    "print(\"Training Q-Learning agent...\")\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Select action\n",
    "        action = epsilon_greedy(q_table, state, epsilon, 2)\n",
    "        \n",
    "        # Take step\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Q-Learning update\n",
    "        current_q = q_table.get_q_value(state, action)\n",
    "        if done:\n",
    "            target_q = reward\n",
    "        else:\n",
    "            max_next_q = q_table.get_max_q_value(next_state)\n",
    "            target_q = reward + gamma * max_next_q\n",
    "        \n",
    "        new_q = current_q + alpha * (target_q - current_q)\n",
    "        q_table.update(state, action, new_q)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "    \n",
    "    # Decay epsilon\n",
    "    epsilon = decay_epsilon(epsilon, epsilon_end, epsilon_decay)\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_steps.append(steps)\n",
    "    \n",
    "    if (episode + 1) % 50 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-50:])\n",
    "        print(f\"Episode {episode+1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62820d98",
   "metadata": {},
   "source": [
    "## 4. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Rewards\n",
    "ax1.plot(episode_rewards, alpha=0.3, label='Episode Reward')\n",
    "moving_avg = np.convolve(episode_rewards, np.ones(20)/20, mode='valid')\n",
    "ax1.plot(range(len(moving_avg)), moving_avg, color='red', linewidth=2, label='Moving Average (20)')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "ax1.set_title('Q-Learning: Episode Rewards')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Steps\n",
    "ax2.plot(episode_steps, alpha=0.3, color='green', label='Episode Steps')\n",
    "moving_avg_steps = np.convolve(episode_steps, np.ones(20)/20, mode='valid')\n",
    "ax2.plot(range(len(moving_avg_steps)), moving_avg_steps, color='darkgreen', linewidth=2, label='Moving Average (20)')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Steps')\n",
    "ax2.set_title('Q-Learning: Episode Steps')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"  Mean reward (last 50 episodes): {np.mean(episode_rewards[-50:]):.2f}\")\n",
    "print(f\"  Max reward: {np.max(episode_rewards):.2f}\")\n",
    "print(f\"  Q-Table size: {q_table.get_stats()['num_states']} states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a4fdd",
   "metadata": {},
   "source": [
    "## 5. Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate policy\n",
    "avg_reward, eval_rewards = evaluate_policy(env, q_table, num_episodes=10, render=False, seed=100)\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"  Average reward: {avg_reward:.2f}\")\n",
    "print(f\"  Std deviation: {np.std(eval_rewards):.2f}\")\n",
    "print(f\"  Episode rewards: {[f'{r:.2f}' for r in eval_rewards]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa5856",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Q-Table\n",
    "save_path = project_root / \"results\" / \"q_learning\" / \"models\" / \"notebook_demo_model.pkl\"\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "q_table.save(save_path)\n",
    "\n",
    "print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "# Load and verify\n",
    "loaded_q_table = QTable(num_actions=2, num_bins=[10, 10, 10, 10], state_bounds=state_bounds)\n",
    "loaded_q_table.load(save_path)\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Loaded Q-Table stats: {loaded_q_table.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27126c9",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Try different environments (MountainCar, LunarLander)\n",
    "2. Experiment with different reward types\n",
    "3. Tune hyperparameters (alpha, gamma, epsilon_decay, bins)\n",
    "4. Compare default vs modified rewards\n",
    "5. Train for more episodes\n",
    "6. Use the training script for full experiments: `python agents/q_learning/train.py --config experiments/configs/q_learning_cartpole_default.json`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
